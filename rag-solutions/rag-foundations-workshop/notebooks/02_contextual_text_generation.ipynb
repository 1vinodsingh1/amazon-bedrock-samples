{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with Amazon Bedrock - Why RAG is a Necessary Concept\n",
    "\n",
    "> *PLEASE NOTE: This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "---\n",
    "\n",
    "Question Answering (QA) is an important task that involves extracting answers to factual queries posed in natural language. Typically, a QA system processes a query against a knowledge base containing structured or unstructured data and generates a response with accurate information. Ensuring high accuracy is key to developing a useful, reliable and trustworthy question answering system, especially for enterprise use cases. However, in this notebook, we will highlight a well documented issue with LLMs: LLM's are unable to answer questions outside of their training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27610c0f-7de6-4440-8f76-decf30e3c5ca",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup the `boto3` client connection to Amazon Bedrock\n",
    "\n",
    "Similar to notebook 00, we will create a client side connection to Amazon Bedrock with the `boto3` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2b2a05-78a9-40ca-9b5e-121030f9ede1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "import logging\n",
    "import boto3\n",
    "\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "logging.basicConfig(level=logging.INFO,format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,\n",
    ")\n",
    "claude3 = 'claude3'\n",
    "llama2 = 'llama2'\n",
    "llama3='llama3'\n",
    "mistral='mistral'\n",
    "titan='titan'\n",
    "models_dict = {\n",
    "    claude3 : 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    llama2: 'meta.llama2-13b-chat-v1',\n",
    "    llama3: 'meta.llama3-8b-instruct-v1:0',\n",
    "    mistral: 'mistral.mistral-7b-instruct-v0:2',\n",
    "    titan : 'amazon.olympus-premier-v1:0'\n",
    "}\n",
    "max_tokens_val = 100\n",
    "temperature_val = 0.1\n",
    "dict_add_params = {\n",
    "    llama3: {\"max_gen_len\":max_tokens_val, \"temperature\":temperature_val} , \n",
    "    claude3: {\"top_k\": 200,  \"temperature\": temperature_val, \"max_tokens\": max_tokens_val},\n",
    "    mistral: {\"max_tokens\":max_tokens_val, \"temperature\": temperature_val} , \n",
    "    titan:  {\"topK\": 200,  \"maxTokenCount\": max_tokens_val}\n",
    "}\n",
    "\n",
    "\n",
    "def generate_conversation(bedrock_client,model_id,system_text,input_text):\n",
    "    \"\"\"\n",
    "    Sends a message to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        system_text (JSON) : The system prompt.\n",
    "        input text : The input message.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Generating message with model %s\", model_id)\n",
    "\n",
    "    # Message to send.\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_text}]\n",
    "    }\n",
    "    messages = [message]\n",
    "    system_prompts = [{\"text\" : system_text}]\n",
    "\n",
    "    # Inference parameters to use.\n",
    "    temperature = 0.5\n",
    "    top_k = 200\n",
    "    max_tokens=100\n",
    "\n",
    "    #Base inference parameters to use.\n",
    "    inference_config = {\"temperature\": temperature}\n",
    "    # Additional inference parameters to use.\n",
    "    additional_model_fields = {\"max_gen_len\":100} #{\"top_k\": top_k, \"max_tokens\": max_tokens}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=get_additional_model_fields(model_id)\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "def get_additional_model_fields(modelId):\n",
    "\n",
    "    return dict_add_params.get(modelId)\n",
    "    #{\"top_k\": top_k, \"max_tokens\": max_tokens}}\n",
    "    \n",
    "def get_converse_output(response_obj):\n",
    "    ret_messages=[]\n",
    "    output_message = response['output']['message']\n",
    "    role_out = output_message['role']\n",
    "\n",
    "    for content in output_message['content']:\n",
    "        ret_messages.append(content['text'])\n",
    "        \n",
    "    return ret_messages, role_out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7db128",
   "metadata": {},
   "source": [
    "#### Test an invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3cf9dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating message with model meta.llama3-8b-instruct-v1:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**The Devastating Impact of High Inflation on a Country's GDP**\n",
       "\n",
       "Inflation, the rate at which prices for goods and services are rising, is a crucial economic indicator that can have far-reaching consequences on a country's economy. When inflation rises above a certain threshold, it can lead to a decline in the purchasing power of consumers, reduce savings, and increase uncertainty, ultimately affecting a country's Gross Domestic Product (GDP). In this article, we will explore the impact of high inflation on a country's GDP and examine the data to support our claims.\n",
       "\n",
       "**The Relationship Between Inflation and GDP**\n",
       "\n",
       "Economists have long recognized the inverse relationship between inflation and GDP. When inflation is high, it can lead to a decline in real GDP, as the increased prices reduce the purchasing power of consumers and businesses. This is because high inflation can:\n",
       "\n",
       "1. **Reduce Consumer Spending**: As prices rise, consumers may delay purchases or switch to cheaper alternatives, leading to a decline in aggregate demand and, subsequently, a reduction in GDP.\n",
       "2. **Increase Production Costs**: Higher production costs due to inflation can lead to reduced competitiveness, lower profit margins, and, ultimately, a decline in economic activity.\n",
       "3. **Reduce Savings**: High inflation can erode the value of savings, leading to reduced consumer spending and investment, which can negatively impact GDP.\n",
       "4. **Increase Uncertainty**: High inflation can create uncertainty, leading to reduced investment, lower economic growth, and a decline in GDP.\n",
       "\n",
       "**Data Analysis**\n",
       "\n",
       "To illustrate the impact of high inflation on a country's GDP, let's examine the data from several countries with high inflation rates in recent years.\n",
       "\n",
       "* **Argentina**: In 2020, Argentina experienced an inflation rate of 53.8%, one of the highest in the world. The country's GDP growth rate declined to 2.3% in 2020, down from 3.2% in 2019.\n",
       "* **Venezuela**: In 2020, Venezuela's inflation rate reached 10,000%, one of the highest in the world. The country's GDP contracted by 35.7% in 2020, its worst performance in decades.\n",
       "* **Turkey**: In 2020, Turkey's inflation rate reached 15.3%, prompting the Central Bank to raise interest rates to combat the inflationary pressures. The country's GDP growth rate slowed to 0.6% in 2020, down from 2.6% in 2019.\n",
       "\n",
       "**Conclusion**\n",
       "\n",
       "The data clearly suggests that high inflation can have a devastating impact on a country's GDP. As prices rise, consumers and businesses become less confident, leading to reduced spending, investment, and economic activity. To combat high inflation, central banks and governments must implement policies that reduce inflationary pressures, such as monetary tightening, fiscal discipline, and structural reforms.\n",
       "\n",
       "In conclusion, high inflation is a significant threat to a country's economic stability and growth. It is essential for policymakers to prioritize inflation control and implement policies that promote economic stability and growth. By doing so, countries can mitigate the negative impact of high inflation on their GDP and ensure a more sustainable economic future."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "modelId = models_dict.get(llama3) #claude3) #llama3)\n",
    "system_text = \"You are an economist with access to lots of data.\"\n",
    "input_text = \"Write an article about impact of high inflation to GDP of a country.\"\n",
    "response = generate_conversation(bedrock_runtime, modelId, system_text, input_text)\n",
    "output_message = response['output']['message']\n",
    "\n",
    "\n",
    "display(Markdown(get_converse_output(response)[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6832f0-92fa-44d0-a736-505f83890c7b",
   "metadata": {},
   "source": [
    "---\n",
    "## Highlighting the Contextual Issue\n",
    "\n",
    "We are trying to model a situation where we are asking the model to provide information about Amazon Advertizing Business. We will first ask the model based on the training data to provide us with an answer about pricing of this technoloy. This technique is called `Zero Shot`. Let's take a look at Claude's response to a quick question \"How did Amazon's Advertising business do?\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "29c6c4cd",
   "metadata": {},
   "source": [
    "Alongside our Stores business, Amazon’s Advertising progress remains strong, growing 24% YoY from\n",
    "$38B in 2022 to $47B in 2023, primarily driven by our sponsored ads. We’ve added Sponsored TV to this\n",
    "offering, a self-service solution for brands to create campaigns that can appear on up to 30+ streaming\n",
    "TV services, including Amazon Freevee and Twitch, and have no minimum spend. Recently, we’ve expanded\n",
    "our streaming TV advertising by introducing ads into Prime Video shows and movies, where brands can\n",
    "reach over 200 million monthly viewers in our most popular entertainment offerings, across hit movies and\n",
    "shows, award-winning Amazon MGM Originals, and live sports like Thursday Night Football. Streaming\n",
    "TV advertising is growing quickly and off to a strong start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51227cd9-fd1e-45e0-81ca-d5c9cd69d19a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating message with model anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Unfortunately, I don't have access to Amazon's actual financial results for 2023 since that year is still in the future. As an AI assistant without direct connections to Amazon's internal data, I can only provide information based on what has been publicly reported so far.\n",
       "\n",
       "Amazon does have a large and growing advertising business as part of its overall operations, but specifics on its 2023 performance are not yet known. Major tech companies typically release their quarterly and annual financial reports a few weeks after the end of each period.\n",
       "\n",
       "If you're interested, I can share details on Amazon's advertising revenues and growth rates for past years based on the company's published reports and analyst estimates. However, for 2023 specifically, we'll have to wait until that data is released by Amazon next year. Let me know if you'd like me to provide historical advertising metrics for Amazon instead."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "modelId = models_dict.get(claude3) #claude3) #llama3)\n",
    "system_text = \"You are an economist with access to lots of data.\"\n",
    "input_text = \"How did Amazon's Advertising business do in 2023?\"\n",
    "response = generate_conversation(bedrock_runtime, modelId, system_text, input_text)\n",
    "output_message = response['output']['message']\n",
    "\n",
    "\n",
    "display(Markdown(get_converse_output(response)[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6acd08-afcc-4f25-8f12-b4d146c7e5d1",
   "metadata": {},
   "source": [
    "The answer provided by Llama3 or Claude is actually incorrect based on Andy Jassi's letter to shareholder in 2023. This is not surprising because the letter is fairly new at the time of writing, meaning that there are more likely changes to the correct answer to the question which are not included in Claude's training data.\n",
    "\n",
    "This implies we need to augment the prompt with additional data about the desired technology question and then the model will return us a very factually accurate. We will see how this improves the response in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c92a2-6c28-48d0-9cd1-db93a7f9f1e2",
   "metadata": {},
   "source": [
    "---\n",
    "## Manually Providing Correct Context\n",
    "\n",
    "In order to have Claude correctly answer the question provided, we need to provide the model context which is relevant to the question. Below is a an extract from the letter to shareholders documentation. \n",
    "\n",
    "```\n",
    "Question:\n",
    "\n",
    "How did Amazon's Advertising business do in 2023?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Alongside our Stores business, Amazon’s Advertising progress remains strong, growing 24% YoY from\n",
    "$38B in 2022 to $47B in 2023, primarily driven by our sponsored ads. We’ve added Sponsored TV to this\n",
    "offering, a self-service solution for brands to create campaigns that can appear on up to 30+ streaming\n",
    "TV services, including Amazon Freevee and Twitch, and have no minimum spend. Recently, we’ve expanded\n",
    "our streaming TV advertising by introducing ads into Prime Video shows and movies, where brands can\n",
    "reach over 200 million monthly viewers in our most popular entertainment offerings, across hit movies and\n",
    "shows, award-winning Amazon MGM Originals, and live sports like Thursday Night Football. Streaming\n",
    "TV advertising is growing quickly and off to a strong start.\n",
    "```\n",
    "\n",
    "We can inject this context into the prompt as shown below and ask the LLM to answer our question based on the context provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09ab2ff0-4cce-43a4-bc55-84ab26e31980",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating message with model anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "According to the context provided, Amazon's Advertising business performed strongly in 2023, growing 24% year-over-year from $38 billion in 2022 to $47 billion in 2023. This growth was primarily driven by Amazon's sponsored ads offerings.\n",
       "\n",
       "Some key points about Amazon's Advertising business in 2023:\n",
       "\n",
       "1. Revenue grew from $38 billion in 2022 to $47 billion in 2023, a 24% year-over-year increase.\n",
       "\n",
       "2. The growth was mainly fueled by Amazon's sponsored ads products.\n",
       "\n",
       "3. Amazon introduced a new offering called Sponsored TV, a self-service solution for brands to create ad campaigns across over 30 streaming TV services, including Amazon Freevee and Twitch.\n",
       "\n",
       "4. Amazon expanded streaming TV advertising by introducing ads into Prime Video shows and movies, allowing brands to reach over 200 million monthly viewers on Amazon's popular entertainment offerings.\n",
       "\n",
       "5. The streaming TV advertising segment is described as growing quickly and off to a strong start.\n",
       "\n",
       "Overall, the context suggests that Amazon's Advertising business, particularly its sponsored ads and streaming TV advertising offerings, experienced significant growth and momentum in 2023."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PROMPT = '''Here is some important context which can help inform the questions the Human asks.\n",
    "\n",
    "<context> Amazon's Advertising business in 2023\n",
    "Alongside our Stores business, Amazon’s Advertising progress remains strong, growing 24% YoY from\n",
    "$38B in 2022 to $47B in 2023, primarily driven by our sponsored ads. We’ve added Sponsored TV to this\n",
    "offering, a self-service solution for brands to create campaigns that can appear on up to 30+ streaming\n",
    "TV services, including Amazon Freevee and Twitch, and have no minimum spend. Recently, we’ve expanded\n",
    "our streaming TV advertising by introducing ads into Prime Video shows and movies, where brands can\n",
    "reach over 200 million monthly viewers in our most popular entertainment offerings, across hit movies and\n",
    "shows, award-winning Amazon MGM Originals, and live sports like Thursday Night Football. Streaming\n",
    "TV advertising is growing quickly and off to a strong start.\n",
    "</context>\n",
    "\n",
    "Human: How did Amazon's Advertising business do in 2023?\n",
    "\n",
    "Assistant:\n",
    "'''\n",
    "\n",
    "import json\n",
    "\n",
    "modelId = models_dict.get(claude3) #claude3) #llama3)\n",
    "system_text = \"You are an economist with access to lots of data.\"\n",
    "response = generate_conversation(bedrock_runtime, modelId, system_text, PROMPT)\n",
    "output_message = response['output']['message']\n",
    "\n",
    "\n",
    "display(Markdown(get_converse_output(response)[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79f602-fa67-42ce-b78b-2cde7ee96535",
   "metadata": {},
   "source": [
    "Now you can see that the model answers the question accurately based on the factual context. However, this context had to be added manually to the prompt. In a production setting, we need a way to automate the retrieval of this information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2fbe70",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Note: Long Context Windows\n",
    "\n",
    "One known limitation for RAG based solutions is the need for inclusion of lots of text into a prompt for an LLM. Fortunately, Claude can help this issue by providing an input token limit of 100k tokens. This limit [corresponds to around 75k words](https://www.anthropic.com/index/100k-context-windows) which is an astounding amount of text.\n",
    "\n",
    "Let's take a look at an example of Claude handling this large context size..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d98dc3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Great Gatsby By F. Scott Fitzgerald The Great Gatsby  ...\n",
      "The context contains 52854 words\n"
     ]
    }
   ],
   "source": [
    "book = ''\n",
    "with open('../data/book/book.txt', 'r') as f:\n",
    "    book = f.read()\n",
    "print('Context:', book[0:53], '...')\n",
    "print('The context contains', len(book.split(' ')), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e95a8582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating message with model anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here is a summary of the plot of The Great Gatsby by F. Scott Fitzgerald:\n",
       "\n",
       "The story is narrated by Nick Carraway, who moves to New York to become a bond trader. He rents a house next door to a mysterious millionaire named Jay Gatsby. Nick is drawn into the wealthy Long Island social circle of his cousin Daisy and her husband Tom Buchanan. \n",
       "\n",
       "Nick learns that Gatsby is in love with Daisy, with whom he had a romantic relationship years earlier before going off to war. Gatsby is incredibly wealthy from bootlegging and other shady business dealings. He throws lavish parties every weekend in hopes that Daisy will attend.\n",
       "\n",
       "Nick arranges for Gatsby to reunite with Daisy, and they begin an affair. Tom grows suspicious of his wife's relationship with Gatsby. In a pivotal confrontation, Tom reveals that Gatsby's wealth comes from criminal activities. Daisy ends up leaving Gatsby after a dramatic incident.\n",
       "\n",
       "In the climax, Gatsby's dreams are crushed when Daisy chooses to remain with her husband Tom. Later that night, Gatsby's former love interest Myrtle Wilson is struck and killed by Gatsby's car, which Daisy was driving. George Wilson mistakenly believes Gatsby was the driver and shoots him dead.\n",
       "\n",
       "Nick is deeply disillusioned by the carelessness and lack of compassion shown by the wealthy characters. Only Nick and a few others attend Gatsby's funeral. In the end, Nick returns to the Midwest, leaving the materialistic East behind."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PROMPT =f'''Human: Summarize the plot of this book.\n",
    "\n",
    "<book>\n",
    "{book}\n",
    "</book>\n",
    "\n",
    "Assistant:'''\n",
    "\n",
    "import json\n",
    "\n",
    "modelId = models_dict.get(claude3) #claude3) #llama3)\n",
    "system_text = \"You are a Literary scholar\"\n",
    "response = generate_conversation(bedrock_runtime, modelId, system_text, PROMPT)\n",
    "output_message = response['output']['message']\n",
    "\n",
    "\n",
    "display(Markdown(get_converse_output(response)[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a3d8a",
   "metadata": {},
   "source": [
    "#### Latency\n",
    "\n",
    "However you can see that it has taken close to a minute or more to generate the summary. Let us examine a technique to speed up the response by parsing the context into various sections and then invoke in parallel and then finally condense the response or summary of summary suaully known as prompt decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34601225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:context:doc=1 length characters of the doc=272852 approx tokens = 68213.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# break down into smaller chunks\n",
    "\n",
    "context_doc = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200, separators=\"\\n\\n\", length_function=len).split_text(book) #-  separator=\",\"\n",
    "print(f\"len:context:doc={len(context_doc)}\", f\"length characters of the doc={len(context_doc[0])}\", f\"approx tokens = {len(context_doc[0])/4}\" )\n",
    "\n",
    "context_doc_list = []\n",
    "for each_doc in context_doc:\n",
    "    context_doc_list.append(each_doc)\n",
    "    #print(f\"len:context:doc={len(each_doc)}:: tokens={len(each_doc)/4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f1f6563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating message with model anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m system_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a Literary scholar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m start_time\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 33\u001b[0m map_call_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mgenerate_conversation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbedrock_runtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelId\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_question\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_question\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontext_doc_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m map_call_list_new \u001b[38;5;241m=\u001b[39m parse_map_yes_no(map_call_list)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken for MAP function::\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:826\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(return_exceptions, *coros_or_futures)\u001b[0m\n\u001b[1;32m    824\u001b[0m outer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# bpo-46672\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m coros_or_futures:\n\u001b[0;32m--> 826\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg_to_fut\u001b[49m:\n\u001b[1;32m    827\u001b[0m         fut \u001b[38;5;241m=\u001b[39m _ensure_future(arg, loop\u001b[38;5;241m=\u001b[39mloop)\n\u001b[1;32m    828\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.outputs import LLMResult\n",
    "\n",
    "def create_map_question(context_doc):\n",
    "    prompt_question = \"\"\"\n",
    "    System: Summarize the following portion of the document below in Context in less than 50 words. Do not return anything else\n",
    "\n",
    "    Context:{context}\n",
    "\n",
    "    \"\"\".format(context=context_doc)\n",
    "    return prompt_question\n",
    "\n",
    "def parse_map_yes_no(map_call_list):\n",
    "    ret_list = []\n",
    "    for ai_message_response in map_call_list:\n",
    "        ret_call = get_converse_output(ai_message_response)[0][0]\n",
    "        print(ret_call)\n",
    "        ret_list.append(ret_call) # this is summary so e have to include every response back \n",
    "            \n",
    "    return ret_list\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "modelId = models_dict.get(claude3) #claude3) #llama3)\n",
    "system_text = \"You are a Literary scholar\"\n",
    "start_time=time.time()\n",
    "map_call_list = await asyncio.gather( *[generate_conversation(bedrock_runtime, modelId, system_text, prompt_question) for idx, prompt_question in enumerate(context_doc_list)] )\n",
    "map_call_list_new = parse_map_yes_no(map_call_list)\n",
    "print(f\"Time taken for MAP function::{time.time()-start_time}:seconds\")\n",
    "map_call_list_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48a0e8-147d-4525-a6b2-68a09af1b2c4",
   "metadata": {},
   "source": [
    "---\n",
    "## Next steps\n",
    "\n",
    "Now you have been able to see a concrete example where LLMs can be improved with correct context injected into a prompt, lets move on to notebook 02 to see how we can automate this process."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
