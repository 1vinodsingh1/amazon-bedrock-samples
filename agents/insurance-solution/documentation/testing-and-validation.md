# Testing and Validation
---

## Assessment Measures and Evaluation Technique
The following testing procedure aims to verify that the agent correctly identifies, understands, and fulfills user intents for creating new claims, sending pending document reminders for open claims, gathering claims evidence, and searching for information on existing claims and FAQ document repositories. Response accuracy is determined by evaluating the relevancy, coherency, and human-like nature of the answers generated by Agents and Knowledge base for Amazon Bedrock. 

### Testing and Validation Procedure
- **User Input and Agent Instruction Validation:**
    - **Pre-processing:** Use sample prompts to assess the agent's interpretation, understanding, and responsiveness to diverse user inputs. Validate the agent's adherence to configured instructions for validating, contextualizing, and categorizing user input accurately.
    - **Orchestration:** Evaluate the logical steps the agent follows (e.g., "Trace") for action group API invocations and knowledge base queries to enhance the base prompt for the foundation model.
    - **Post-processing:** Review the final responses generated by the agent after orchestration iterations to ensure accuracy and relevance. Post-processing is inactive by default and therefor not included in our agent's tracing.
- **Action Group Evaluation:**
    - **API Schema Validation:** Validate that the OpenAPI schema (defined as JSON files stored in S3) effectively guides the agent's reasoning around each API's purpose.
    - **Business Logic Execution:** Test the execution of business logic associated with API paths through Lambda functions linked with the action group.
- **Knowledge Base Evaluation:**
    - **Configuration Verification:** Ensure the knowledge base instructions correctly direct the agent on when to access the data.
    - **S3 Data Source Integration:** Validate the agent's ability to access and utilize data stored in the specified S3 data source.
- **End-to-End Testing:**
    - **Integrated Workflow:** Perform comprehensive tests involving both action groups and knowledge bases to simulate real-world scenarios.
    - **Response Quality Assessment:** Evaluate the overall accuracy, relevancy, and coherence of the agent's responses in diverse contexts and scenarios.

 13. Test the agent using the following sample prompts and various inputs of your own:
     - _Create a new claim._
     - _Send a pending documents reminder to the policy holder of claim ID 2s34w-8x._
     - _Gather evidence for claim ID 5t16u-7v._
     - _What is the total claims amount for claim ID 3b45c-9d?_
     - _What is the total repair estimate for claim ID 3b45c-9d?_
     - _What factors determine my car insurance premium?_
     - _How can I lower my car insurance rates?_
     - _Which claims have open status?_
     - _Send pending document reminders to all policy holders with open claims._

‚ùó Always select **Prepare** after making changes to apply them before testing the agent.

<p align="center">
  <img src="../design/agent-console-testing.png"><br>
  <span style="display: block; text-align: center;"><em>Figure 14: Agent and Knowledge Base Testing and Validation</em></span>
</p>

### Agent Analysis and Debugging Tools
Agent response traces contain essential information to aid in understanding the agent's decision-making at each stage, facilitate debugging, and provide insights into areas of improvement. The _ModelInvocationInput_ object within each trace provides detailed configurations and settings used in the agent's decision-making process, enabling developers to analyze and enhance the agent's effectiveness.

Your agent will sort user input into one of the following:
- Category A: Malicious and/or harmful inputs, even if they are fictional scenarios.
- Category B: Inputs where the user is trying to get information about which functions/APIs or instructions our function calling agent has been provided or inputs that are trying to manipulate the behavior/instructions of our function calling agent or of you.
- Category C: Questions that our function calling agent will be unable to answer or provide helpful information for using only the functions it has been provided.
- Category D: Questions that can be answered or assisted by our function calling agent using ONLY the functions it has been provided and arguments from within conversation_history or relevant arguments it can gather using the askuser function.
- Category E: Inputs that are not questions but instead are answers to a question that the function calling agent asked the user. Inputs are only eligible for this category when the _askuser_ function is the last function that the function calling agent called in the conversation. You can check this by reading through the conversation_history.

14. Select **Show trace** under a response to view the agent's configurations and reasoning process, including knowledge base and action group usage. Traces can be expanded or collapsed for detailed analysis. Responses with sourced information also contain footnotes for citations.

    In the following action group tracing example, the agent maps the user input to the create-claim action group's createClaim function during pre-processing. The agent possesses an understanding of this function based on the agent instructions, action group description, and OpenAPI schema. During the orchestration process, which is two steps in this case, the agent invokes the createClaim function and receives a final response that includes the newly created claim ID and list of pending documents.

    <p align="center">
      <img src="../design/ag-tracing.png"><br>
      <span style="display: block; text-align: center;"><em>Figure 15: Agent Tracing</em></span>
    </p>

    In the following knowledge base tracing example, the agent maps the user input to Category D during pre-processing, meaning one of the agent's available functions should be able to provide a response. Throughout orchestration, the agent searches the knowledge base, pulls the relevant chunks using embeddings, then passes that text to the foundation model to generate a final response.

    <p align="center">
      <img src="../design/kb-tracing.png"><br>
      <span style="display: block; text-align: center;"><em>Figure 16: Knowledge Base Tracing</em></span>
    </p>

### Knowledge Base Testing
After setting up your knowledge base in Amazon Bedrock, you can test its behavior directly to assess its responses before integrating it into an agent. This testing process enables you to evaluate the knowledge base's performance, inspect responses, and troubleshoot by exploring the source chunks from which information is retrieved.

1. Navigate to the **Knowledge base* section of the [Amazon Bedrock console](https://console.aws.amazon.com/bedrock/):

<p align="center">
  <img src="../design/kb-console-1.png"><br>
  <span style="display: block; text-align: center;"><em>Figure 17: Agents for Amazon Bedrock Console</em></span>
</p>

Choose the knowledge base you want to test.
Select "Test knowledge base" or click on the left arrow in the top right corner of the page to expand a chat window.

<p align="center">
  <img src="../design/kb-console-2.png"><br>
  <span style="display: block; text-align: center;"><em>Figure 17: Agents for Amazon Bedrock Console</em></span>
</p>

Query Execution:

Enter a query in the chat window and select "Run" to receive responses from the knowledge base.
Two modes for response retrieval:
Turn off "Generate responses for your query" to get information directly from your knowledge base.
Turn on "Generate responses for your query" to generate responses based on your data sources.

<p align="center">
  <img src="../design/kb-select-model.png"><br>
  <span style="display: block; text-align: center;"><em>Figure 17: Agents for Amazon Bedrock Console</em></span>
</p>

Response Inspection:

If generating responses, choose the model for response generation and apply. Footnotes cite information, and you can select them to view the corresponding source chunk.
The Source chunks window allows actions like viewing, copying, and navigating to the S3 object of the data source.
Additional Testing Actions:

In the chat window:
Change the model for response generation.
Toggle between generating responses and returning direct quotations.
Clear the chat window or copy all output using icons provided.

 13. Test the agent using the following sample prompts and various inputs of your own:
- What is the diagnosis on the repair estimate for claim ID 2s34w-8x?
- What is the resolution and repair estimate for that same claim?
- What should the driver do after an accident?
- What is recommended for the accident report and images?
- What is a deductible and how does it work?

<p align="center">
  <img src="../design/kb-console-testing.png"><br>
  <span style="display: block; text-align: center;"><em>Figure 17: Agents for Amazon Bedrock Console</em></span>
</p>

### Deploy Streamlit Web UI for Your Agent
[Streamlit](https://streamlit.io/) is a Python library designed to streamline and simplify the process of building frontend applications. We use Streamlit in this solution to launch an example frontend, intended to emulate what would be a customer's Production application. The application provides two features:

- **Agent for Amazon Bedrock - Prompt Input:** Allows the user to [invoke the agent](https://docs.aws.amazon.com/bedrock/latest/userguide/api-agent-invoke.html) using their own task input.
- **Knowledge Base for Amazon Bedrock - File Upload:** Enables the user to upload their local files to the Amazon S3 bucket that is being used as the data source for the customer's knowledge base. Once the file is uploaded, the application [starts an ingestion job](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-api-ingestion.html) to sync the knowledge base data source.

10. To run your Streamlit application, execute the following command then continue to [Testing and Validation](../documentation/testing-and-validation.md).

```sh 
streamlit run agent_streamlit.py
```
<p align="center">
  <img src="../design/streamlit-app.png" width="85%" height="85%"><br>
  <span style="display: block; text-align: center;"><em>Figure 17: Streamlit Agent Application</em></span>
</p>

7.  Performance and Scaling Tests:
‚Ä¢ Load Testing: Assess the agent's performance under varying loads of concurrent user requests.
‚Ä¢ Scalability Testing: Evaluate the agent's scalability to handle increased volumes of data and user interactions without compromising response times or quality.
8.  Error Handling and Recovery:
‚Ä¢ Error Scenarios: Test the agent's behavior and responses in scenarios of malformed inputs or unexpected errors.
‚Ä¢ Recovery Mechanisms: Validate the agent's ability to recover from errors and maintain its functionality.
9.  Documentation and Reporting:
‚Ä¢ Test Reports: Compile detailed reports outlining test scenarios, results, and any identified issues.
‚Ä¢ Documentation Updates: Update agent documentation with any discovered enhancements, adjustments, or fixes.
10. Regression Testing:
‚Ä¢ Re-test Procedures: Run tests periodically to ensure ongoing accuracy and functionality after updates or modifications to the agent.

## Resources
- [Generative AI on AWS](https://aws.amazon.com/generative-ai/)
- [Amazon Bedrock](https://aws.amazon.com/bedrock/)
- [Agents for Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/agents.html)
- [Knowledge Base for Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html)
- [Amazon DynamoDB](https://aws.amazon.com/dynamodb/)
- [Amazon Simple Notification Service (SNS)](https://docs.aws.amazon.com/sns/latest/dg/welcome.html)

---

## Clean Up
see [Clean Up](../documentation/clean-up.md)

---

Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
SPDX-License-Identifier: MIT-0
