{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and evaluating Q&A Application using Knowledge Bases for Amazon Bedrock using RAG Assessment (RAGAS) framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "In this notebook, we will dive deep into building Q&A application using Retrieve API provide by Knowledge Bases for Amazon Bedrock, along with LangChain and RAGAS for evaluating the responses. Here, we will query the knowledge base to get the desired number of document chunks based on similarity search, prompt the query using Anthropic Claude, and then evaluate the responses effectively using Ragas evaluation metrics, such as faithfulness, answer relevancy, context precision based expectations.\n",
    "\n",
    "### Knowledge Bases for Amazon Bedrock Introduction\n",
    "\n",
    "With knowledge bases, you can securely connect foundation models (FMs) in Amazon Bedrock to your company\n",
    "data for Retrieval Augmented Generation (RAG). Access to additional data helps the model generate more relevant,\n",
    "context-speciﬁc, and accurate responses without continuously retraining the FM. All information retrieved from\n",
    "knowledge bases comes with source attribution to improve transparency and minimize hallucinations. For more information on creating a knowledge base using console, please refer to this [post](!https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html).\n",
    "\n",
    "### Pattern\n",
    "\n",
    "We can implement the solution using Retreival Augmented Generation (RAG) pattern. RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. Here, we are performing RAG effectively on the knowledge base created in the previous notebook or using console. \n",
    "\n",
    "### Pre-requisite\n",
    "\n",
    "Before being able to answer the questions, the documents must be processed and stored in Knowledge Bases for Amazon Bedrock.\n",
    "\n",
    "1. Load the documents into the knowledge base by connecting your s3 bucket (data source). \n",
    "2. Ingestion - Knowledge base will split them into smaller chunks (based on the strategy selected), generate embeddings and store it in the associated vectore store.\n",
    "\n",
    "![data_ingestion.png](./images/data_ingestion.png)\n",
    "\n",
    "\n",
    "#### Notebook Walkthrough\n",
    "\n",
    "\n",
    "\n",
    "For our notebook we will use the `Retreive API` provided by Knowledge Bases for Amazon Bedrock which converts user queries into\n",
    "embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom\n",
    "workﬂows on top of the semantic search results. The output of the `Retrieve API` includes the the `retrieved text chunks`, the `location type` and `URI` of the source data, as well as the relevance `scores` of the retrievals. \n",
    "\n",
    "\n",
    "We will then use the text chunks being generated and augment it with the original prompt and pass it through the `claude-v2.1` model using prompt engineering patterns based for your use case.\n",
    "\n",
    "Finally we will evaluate the generated responses using RAGAS on using metrics such as faithfulness, answer relevancy,and context precision. For evaluation, we will use `Anthropic Claude V2.1 model`.\n",
    "### Ask question\n",
    "\n",
    "\n",
    "![retrieveapi.png](./images/retrieveAPI.png)\n",
    "\n",
    "\n",
    "#### Evaluation\n",
    "1. Utilize Ragas for evaluation on \n",
    "    1. **Faithfulness:** This measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better.\n",
    "    2. **Answer Relevancy:** The evaluation metric, Answer Relevancy, focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information. This metric is computed using the question and the answer, with values ranging between 0 and 1, where higher scores indicate better relevancy.\n",
    "    3. **Context Precision:** Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked higher or not. Ideally all the relevant chunks must appear at the top ranks. This metric is computed using the question and the contexts, with values ranging between 0 and 1, where higher scores indicate better precision.\n",
    "    4. **Context Recall:** Context recall measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. It is computed based on the ground truth and the retrieved context, and the values range between 0 and 1, with higher values indicating better performance.\n",
    "    \n",
    "\n",
    "### USE CASE:\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "In this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on. This data is already ingested into the knowledge base. You will need the `knowledge base id` to run this example.\n",
    "In your specific use case, you can sync different files for different domain topics and query this notebook in the same manner to evaluate model responses using the retrieve API from knowledge bases.\n",
    "\n",
    "\n",
    "### Python 3.10\n",
    "\n",
    "⚠  For this lab we need to run the notebook based on a Python 3.10 runtime. ⚠\n",
    "\n",
    "### Setup\n",
    "\n",
    "To run this notebook you would need to install dependencies, langchain and ragas and the updated boto3, botocore whls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/ayanray/GitProjects/amazon-bedrock-samples/.venv/lib/python3.11/site-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/8a/6a/19e9fe04fca059ccf770861c7d5721ab4c2aebc539889e97c7977528a53b/pip-24.0-py3-none-any.whl.metadata\n",
      "  Downloading pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.2.1\n",
      "    Uninstalling pip-23.2.1:\n",
      "      Successfully uninstalled pip-23.2.1\n",
      "Successfully installed pip-24.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "botocore 1.33.2 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "botocore 1.33.2 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install boto3==1.33.2 --force-reinstall --quiet\n",
    "%pip install botocore==1.33.2 --force-reinstall --quiet\n",
    "%pip install langchain==0.0.342 --force-reinstall --quiet\n",
    "%pip install ragas==0.0.20 --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restart the kernel with the updated packages that are installed through the dependencies above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow the steps below to set up necessary packages\n",
    "\n",
    "1. Import the necessary libraries for creating `bedrock-runtime` for invoking foundation models and `bedrock-agent-runtime` client for using Retrieve API provided by Knowledge Bases for Amazon Bedrock. \n",
    "2. Import Langchain for: \n",
    "   1. Initializing bedrock model  `anthropic.claude-v2.1` as our large language model to perform query completions using the RAG pattern. \n",
    "   2. Initialize Langchain retriever integrated with knowledge bases. \n",
    "   3. Later in the notebook we will wrap the LLM and retriever with `RetrieverQAChain` for building our Q&A application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pprint\n",
    "from botocore.client import Config\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "kb_id = \"<knowledge_base_id>\" # replace it with your Knowledge base id.\n",
    "\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "bedrock_agent_client = boto3.client(\"bedrock-agent-runtime\",\n",
    "                              config=bedrock_config\n",
    "                              )\n",
    "\n",
    "model_kwargs_claude = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_k\": 10,\n",
    "    \"max_tokens_to_sample\": 3000\n",
    "}\n",
    "\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2:1\",\n",
    "              model_kwargs=model_kwargs_claude,\n",
    "              streaming=True,\n",
    "              client = bedrock_client,)\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\",client=bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve API: Process flow \n",
    "\n",
    "Create a `AmazonKnowledgeBasesRetriever` object from LangChain which will call the `Retreive API` provided by Knowledge Bases for Amazon Bedrock which converts user queries into\n",
    "embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom\n",
    "workﬂows on top of the semantic search results. The output of the `Retrieve API` includes the the `retrieved text chunks`, the `location type` and `URI` of the source data, as well as the relevance `scores` of the retrievals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "        knowledge_base_id=kb_id,\n",
    "        retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": 4}},\n",
    "        # endpoint_url=endpoint_url,\n",
    "        # region_name=\"us-east-1\",\n",
    "        # credentials_profile_name=\"<profile_name>\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`score`: You can view the associated score of each of the text chunk that was returned which depicts its correlation to the query in terms of how closely it matches it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt specific to the model to personalize responses \n",
    "\n",
    "Here, we will use the specific prompt below for the model to act as a financial advisor AI system that will provide answers to questions by using fact based and statistical information when possible. We will provide the `Retrieve API` responses from above as a part of the `{context}` in the prompt for the model to refer to, along with the user `query`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "    Human: You are a financial advisor AI system, and provides answers to questions by using fact based and statistical information when possible. \n",
    "    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "    Assistant:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "# Setup RAG pipeline\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Evaluation Data\n",
    "\n",
    "As RAGAS aims to be a reference-free evaluation framework, the required preparations of the evaluation dataset are minimal. You will need to prepare `question` and `ground_truths` pairs from which you can prepare the remaining information through inference as shown below. If you are not interested in the `context_recall` metric, you don’t need to provide the `ground_truths` information. In this case, all you need to prepare are the `questions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayanray/GitProjects/amazon-bedrock-samples/.venv/lib/python3.11/site-packages/langchain_community/llms/bedrock.py:73: UserWarning: Error: Prompt must alternate between '\n",
      "\n",
      "Human:' and '\n",
      "\n",
      "Assistant:'. Received \n",
      "\n",
      "Human: \n",
      "    \n",
      "\n",
      "Human: You are a financial advisor AI system, and provides answers to questions by using fact based and statistical information when possible. \n",
      "    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags. \n",
      "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "    <context>\n",
      "    [Document(page_content='It’s not just about the pay and the benefits. It’s about all the other detailed aspects of the relationship too.   Does your Chair take comfort in the outcome of the recent union vote in Bessemer? No, he doesn’t. I think we need to do a better job for our employees. While the voting results were lopsided and our direct relationship with employees is strong, it’s clear to me that we need a better vision for how we create value for employees – a vision for their success.   If you read some of the news reports, you might think we have no care for employees. In those reports, our employees are sometimes accused of being desperate souls and treated as robots. That’s not accurate. They’re sophisticated and thoughtful people who have options for where to work. When we survey fulfillment center employees, 94% say they would recommend Amazon to a friend as a place to work.   Employees are able to take informal breaks throughout their shifts to stretch, get water, use the rest room, or talk to a manager, all without impacting their performance. These informal work breaks are in addition to the 30-minute lunch and 30-minute break built into their normal schedule.   We don’t set unreasonable performance goals. We set achievable performance goals that take into account tenure and actual employee performance data. Performance is evaluated over a long period of time as we know that a variety of things can impact performance in any given week, day, or hour. If employees are on track to miss a performance target over a period of time, their manager talks with them and provides coaching.   Coaching is also extended to employees who are excelling and in line for increased responsibilities. In fact, 82% of coaching is positive, provided to employees who are meeting or exceeding expectations. We terminate the employment of less than 2.6% of employees due to their inability to perform their jobs (and that number was even lower in 2020 because of operational impacts of COVID-19).   Earth’s Best Employer and Earth’s Safest Place to Work   The fact is, the large team of thousands of people who lead operations at Amazon have always cared deeply for our hourly employees, and we’re proud of the work environment we’ve created. We’re also proud of the fact that Amazon is a company that does more than just create jobs for computer scientists and people with advanced degrees. We create jobs for people who never got that advantage.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2020-Shareholder-Letter.pdf'}}, 'score': 0.004938911}), Document(page_content='We terminate the employment of less than 2.6% of employees due to their inability to perform their jobs (and that number was even lower in 2020 because of operational impacts of COVID-19).   Earth’s Best Employer and Earth’s Safest Place to Work   The fact is, the large team of thousands of people who lead operations at Amazon have always cared deeply for our hourly employees, and we’re proud of the work environment we’ve created. We’re also proud of the fact that Amazon is a company that does more than just create jobs for computer scientists and people with advanced degrees. We create jobs for people who never got that advantage.        Despite what we’ve accomplished, it’s clear to me that we need a better vision for our employees’ success. We have always wanted to be Earth’s Most Customer-Centric Company. We won’t change that. It’s what got us here. But I am committing us to an addition. We are going to be Earth’s Best Employer and Earth’s Safest Place to Work.   In my upcoming role as Executive Chair, I’m going to focus on new initiatives. I’m an inventor. It’s what I enjoy the most and what I do best. It’s where I create the most value. I’m excited to work alongside the large team of passionate people we have in Ops and help invent in this arena of Earth’s Best Employer and Earth’s Safest Place to Work. On the details, we at Amazon are always flexible, but on matters of vision we are stubborn and relentless. We have never failed when we set our minds to something, and we’re not going to fail at this either.   We dive deep into safety issues. For example, about 40% of work-related injuries at Amazon are related to musculoskeletal disorders (MSDs), things like sprains or strains that can be caused by repetitive motions. MSDs are common in the type of work that we do and are more likely to occur during an employee’s first six months. We need to invent solutions to reduce MSDs for new employees, many of whom might be working in a physical role for the first time.   One such program is WorkingWell – which we launched to 859,000 employees at 350 sites across North America and Europe in 2020 – where we coach small groups of employees on body mechanics, proactive wellness, and safety. In addition to reducing workplace injuries, these concepts have a positive impact on regular day-to-day activities outside work.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2020-Shareholder-Letter.pdf'}}, 'score': 0.00468853}), Document(page_content='Typical single-company data centers operate at roughly 18% server utilization. They need that excess capacity to handle large usage spikes. AWS benefits from multi- tenant usage patterns and operates at far higher server utilization rates. In addition, AWS has been successful in increasing the energy efficiency of its facilities and equipment, for instance by using more efficient evaporative cooling in certain data centers instead of traditional air conditioning. A study by 451 Research found that AWS’s infrastructure is 3.6 times more energy efficient than the median U.S. enterprise data center surveyed. Along with our use of renewable energy, these factors enable AWS to do the same tasks as traditional data centers with an 88% lower carbon footprint. And don’t think we’re not going to get those last 12 points—we’ll make AWS 100% carbon free through more investments in renewable energy projects.   Leveraging scale for good   Over the last decade, no company has created more jobs than Amazon. Amazon directly employs 840,000 workers worldwide, including over 590,000 in the U.S., 115,000 in Europe, and 95,000 in Asia. In total, Amazon directly and indirectly supports 2 million jobs in the U.S., including 680,000-plus jobs created by Amazon’s investments in areas like construction, logistics, and professional services, plus another 830,000 jobs created by small and medium-sized businesses selling on Amazon. Globally, we support nearly 4 million jobs. We are especially proud of the fact that many of these are entry-level jobs that give people their first opportunity to participate in the workforce.   And Amazon’s jobs come with an industry-leading $15 minimum wage and comprehensive benefits. More than 40 million Americans—many making the federal minimum wage of $7.25 an hour—earn less than the lowest- paid Amazon associate. When we raised our starting minimum wage to $15 an hour in 2018, it had an immediate and meaningful impact on the hundreds of thousands of people working in our fulfillment centers. We want other big employers to join us by raising their own minimum pay rates, and we continue to lobby for a $15 federal minimum wage.        We want to improve workers’ lives beyond pay. Amazon provides every full-time employee with health insurance, a 401(k) plan, 20 weeks paid maternity leave, and other benefits. These are the same benefits that Amazon’s most senior executives receive.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2019-Shareholder-Letter.pdf'}}, 'score': 0.0044046575}), Document(page_content='MSDs are common in the type of work that we do and are more likely to occur during an employee’s first six months. We need to invent solutions to reduce MSDs for new employees, many of whom might be working in a physical role for the first time.   One such program is WorkingWell – which we launched to 859,000 employees at 350 sites across North America and Europe in 2020 – where we coach small groups of employees on body mechanics, proactive wellness, and safety. In addition to reducing workplace injuries, these concepts have a positive impact on regular day-to-day activities outside work.   We’re developing new automated staffing schedules that use sophisticated algorithms to rotate employees among jobs that use different muscle-tendon groups to decrease repetitive motion and help protect employees from MSD risks. This new technology is central to a job rotation program that we’re rolling out throughout 2021.   Our increased attention to early MSD prevention is already achieving results. From 2019 to 2020, overall MSDs decreased by 32%, and MSDs resulting in time away from work decreased by more than half.   We employ 6,200 safety professionals at Amazon. They use the science of safety to solve complex problems and establish new industry best practices. In 2021, we’ll invest more than $300 million into safety projects, including an initial $66 million to create technology that will help prevent collisions of forklifts and other types of industrial vehicles.   When we lead, others follow. Two and a half years ago, when we set a $15 minimum wage for our hourly employees, we did so because we wanted to lead on wages – not just run with the pack – and because we believed it was the right thing to do. A recent paper by economists at the University of California-Berkeley and Brandeis University analyzed the impact of our decision to raise our minimum starting pay to $15 per hour. Their assessment reflects what we’ve heard from employees, their families, and the communities they live in.   Our increase in starting wage boosted local economies across the country by benefiting not only our own employees but also other workers in the same community. The study showed that our pay raise resulted in a 4.7% increase in the average hourly wage among other employers in the same labor market.   And we’re not done leading. If we want to be Earth’s Best Employer, we shouldn’t settle for 94% of employees saying they would recommend Amazon to a friend as a place to work.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2020-Shareholder-Letter.pdf'}}, 'score': 0.0043721213})]\n",
      "    </context>\n",
      "\n",
      "    <question>\n",
      "    How many days has Amazon asked employees to come to work in office?\n",
      "    </question>\n",
      "\n",
      "    The response should be specific and use statistics or numbers when possible.\n",
      "\n",
      "    \n",
      "\n",
      "Assistant:\n",
      "  warnings.warn(ALTERNATION_ERROR + f\" Received {input_text}\")\n",
      "/Users/ayanray/GitProjects/amazon-bedrock-samples/.venv/lib/python3.11/site-packages/langchain_community/llms/bedrock.py:73: UserWarning: Error: Prompt must alternate between '\n",
      "\n",
      "Human:' and '\n",
      "\n",
      "Assistant:'. Received \n",
      "\n",
      "Human: \n",
      "    \n",
      "\n",
      "Human: You are a financial advisor AI system, and provides answers to questions by using fact based and statistical information when possible. \n",
      "    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags. \n",
      "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "    <context>\n",
      "    [Document(page_content='They estimate that, in 2020, third-party seller profits from selling on Amazon were between $25 billion and $39 billion, and to be conservative here I’ll go with $25 billion.   For customers, we have to break it down into consumer customers and AWS customers.   We’ll do consumers first. We offer low prices, vast selection, and fast delivery, but imagine we ignore all of that for the purpose of this estimate and value only one thing: we save customers time.   Customers complete 28% of purchases on Amazon in three minutes or less, and half of all purchases are finished in less than 15 minutes. Compare that to the typical shopping trip to a physical store – driving, parking, searching store aisles, waiting in the checkout line, finding your car, and driving home. Research suggests the typical physical store trip takes about an hour. If you assume that a typical Amazon purchase takes 15 minutes and that it saves you a couple of trips to a physical store a week, that’s more than 75 hours a year saved. That’s important. We’re all busy in the early 21st century.   So that we can get a dollar figure, let’s value the time savings at $10 per hour, which is conservative. Seventy- five hours multiplied by $10 an hour and subtracting the cost of Prime gives you value creation for each Prime member of about $630. We have 200 million Prime members, for a total in 2020 of $126 billion of value creation.   AWS is challenging to estimate because each customer’s workload is so different, but we’ll do it anyway, acknowledging up front that the error bars are high. Direct cost improvements from operating in the cloud versus on premises vary, but a reasonable estimate is 30%. Across AWS’s entire 2020 revenue of $45 billion, that 30% would imply customer value creation of $19 billion (what would have cost them $64 billion on their own cost $45 billion from AWS). The difficult part of this estimation exercise is that the direct cost reduction is the smallest portion of the customer benefit of moving to the cloud. The bigger benefit is the increased speed of software development – something that can significantly improve the customer’s competitiveness and top line. We have no reasonable way of estimating that portion of customer value except to say that it’s almost certainly larger than the direct cost savings.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2020-Shareholder-Letter.pdf'}}, 'score': 0.0058308467}), Document(page_content='Conversely, our Consumer revenue grew dramatically in 2020. In 2020, Amazon’s North America and International Consumer revenue grew 39% YoY on the very large 2019 revenue base of $245 billion; and, this extraordinary growth extended into 2021 with revenue increasing 43% YoY in Q1 2021. These are astounding numbers. We realized the equivalent of three years’ forecasted growth in about 15 months.   As the world opened up again starting in late Q2 2021, and more people ventured out to eat, shop, and travel, consumer spending returned to being spread over many more entities. We weren’t sure what to expect in 2021, but the fact that we continued to grow at double digit rates (with a two-year Consumer compounded annual growth rate of 29%) was encouraging as customers appreciated the role Amazon played for them during the pandemic, and started using Amazon for a larger amount of their household purchases.   This growth also created short-term logistics and cost challenges. We spent Amazon’s first 25 years building a very large fulfillment network, and then had to double it in the last 24 months to meet customer demand. As we were bringing this new capacity online, the labor market tightened considerably, making it challenging both to receive all of the inventory our vendors and sellers wanted to send us and to place that inventory as close to customers as we typically do. Combined with ocean, air, and trucking capacity becoming scarcer and more expensive, this created extra transportation and productivity costs. Supply chains were disrupted in ways none of us had seen previously. We hoped that the major impact from COVID-19 would recede as 2021 drew to a close, but then omicron reared its head in December, which had worldwide ramifications, including impacting people’s ability to work. And then in late February, with Russia’s invasion of Ukraine, fuel costs and inflation became bigger issues with which to contend.   So, 2021 was a crazy and unpredictable year, continuing a trend from 2020. But, I’m proud of the incredible commitment and effort from our employees all over the world. I’m not sure any of us would have gotten        through the pandemic the same way without the dedication and extraordinary efforts shown by our teams during this period, and I’m eternally grateful.   It’s not normal for a company of any size to be able to respond to something as discontinuous and unpredictable as this pandemic turned out to be. What is it about Amazon that made it possible for us to do so?', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2021-Shareholder-Letter.pdf'}}, 'score': 0.005385009}), Document(page_content='Shorter travel distances mean lower cost to serve, less impact on the environment, and customers getting their orders faster. On the latter, we’re excited about seeing more next day and same-day deliveries, and we’re on track to have our fastest Prime delivery speeds ever in 2023. Overall, we remain confident about our plans to lower costs, reduce delivery times, and build a meaningfully larger retail business with healthy operating margins.   AWS has an $85B annualized revenue run rate, is still early in its adoption curve, but at a juncture where it’s critical to stay focused on what matters most to customers over the long-haul. Despite growing 29% year-over- year (“YoY”) in 2022 on a $62B revenue base, AWS faces short-term headwinds right now as companies are being more cautious in spending given the challenging, current macroeconomic conditions. While some companies might obsess over how they could extract as much money from customers as possible in these tight times, it’s neither what customers want nor best for customers in the long term, so we’re taking a different tack. One of the many advantages of AWS and cloud computing is that when your business grows, you can seamlessly scale up; and conversely, if your business contracts, you can choose to give us back that capacity and cease paying for it. This elasticity is unique to the cloud, and doesn’t exist when you’ve already made expensive capital investments in your own on-premises datacenters, servers, and networking gear. In AWS, like all our businesses, we’re not trying to optimize for any one quarter or year. We’re trying to build customer relationships (and a business) that outlast all of us; and as a result, our AWS sales and support teams are spending much of their time helping customers optimize their AWS spend so they can better weather this uncertain economy. Many of these AWS customers tell us that they’re not cost-cutting as much as cost- optimizing so they can take their resources and apply them to emerging and inventive new customer experiences they’re planning. Customers have appreciated this customer-focused, long-term approach, and we think it’ll bode well for both customers and AWS.   While these short-term headwinds soften our growth rate, we like a lot of the fundamentals that we’re seeing in AWS. Our new customer pipeline is robust, as are our active migrations.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2022-Shareholder-Letter.pdf'}}, 'score': 0.005303363}), Document(page_content='The difficult part of this estimation exercise is that the direct cost reduction is the smallest portion of the customer benefit of moving to the cloud. The bigger benefit is the increased speed of software development – something that can significantly improve the customer’s competitiveness and top line. We have no reasonable way of estimating that portion of customer value except to say that it’s almost certainly larger than the direct cost savings. To be conservative here (and remembering we’re really only trying to get ballpark estimates), I’ll say it’s the same and call AWS customer value creation $38 billion in 2020.   Adding AWS and consumer together gives us total customer value creation in 2020 of $164 billion.        Summarizing: Shareholders $21B Employees $91B 3P Sellers $25B Customers $164B Total $301B   If each group had an income statement representing their interactions with Amazon, the numbers above would be the “bottom lines” from those income statements. These numbers are part of the reason why people work for us, why sellers sell through us, and why customers buy from us. We create value for them. And this value creation is not a zero-sum game. It is not just moving money from one pocket to another. Draw the box big around all of society, and you’ll find that invention is the root of all real value creation. And value created is best thought of as a metric for innovation.   Of course, our relationship with these constituencies and the value we create isn’t exclusively dollars and cents. Money doesn’t tell the whole story. Our relationship with shareholders, for example, is relatively simple. They invest and hold shares for a duration of their choosing. We provide direction to shareowners infrequently on matters such as annual meetings and the right process to vote their shares. And even then they can ignore those directions and just skip voting.   Our relationship with employees is a very different example. We have processes they follow and standards they meet. We require training and various certifications. Employees have to show up at appointed times. Our interactions with employees are many, and they’re fine-grained. It’s not just about the pay and the benefits. It’s about all the other detailed aspects of the relationship too.   Does your Chair take comfort in the outcome of the recent union vote in Bessemer? No, he doesn’t. I think we need to do a better job for our employees.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2020-Shareholder-Letter.pdf'}}, 'score': 0.0046977205})]\n",
      "    </context>\n",
      "\n",
      "    <question>\n",
      "    By what percentage did AWS revenue grow year-over-year in 2022?\n",
      "    </question>\n",
      "\n",
      "    The response should be specific and use statistics or numbers when possible.\n",
      "\n",
      "    \n",
      "\n",
      "Assistant:\n",
      "  warnings.warn(ALTERNATION_ERROR + f\" Received {input_text}\")\n",
      "/Users/ayanray/GitProjects/amazon-bedrock-samples/.venv/lib/python3.11/site-packages/langchain_community/llms/bedrock.py:73: UserWarning: Error: Prompt must alternate between '\n",
      "\n",
      "Human:' and '\n",
      "\n",
      "Assistant:'. Received \n",
      "\n",
      "Human: \n",
      "    \n",
      "\n",
      "Human: You are a financial advisor AI system, and provides answers to questions by using fact based and statistical information when possible. \n",
      "    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags. \n",
      "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "    <context>\n",
      "    [Document(page_content='In the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated commodity. But, there’s a lot more to compute than just a server. Customers want various flavors of compute (e.g. server configurations optimized for storage, memory, high-performance compute, graphics rendering, machine learning), multiple form factors (e.g. fixed instance sizes, portable containers, serverless functions), various sizes and optimizations of persistent storage, and a slew of networking capabilities. Then, there’s the CPU chip that runs in your compute. For many years, the industry had used Intel or AMD x86 processors. We have important partnerships with these companies, but realized that if we wanted to push price and performance further (as customers requested), we’d have to develop our own chips, too. Our first generalized chip was Graviton, which we announced in 2018. This helped a subset of customer workloads run more cost-effectively than prior options. But, it wasn’t until 2020, after taking the learnings from Graviton and innovating on a new chip, that we had something remarkable with our Graviton2 chip, which provides up to 40% better price-performance than the comparable latest generation x86 processors. Think about how much of an impact 40% improvement on compute is. Compute is used for every bit of technology. That’s a huge deal for customers. And, while Graviton2 has been a significant success thus far (48 of the top 50 AWS EC2 customers have already adopted it), the AWS Chips team was already learning from what customers said could be better, and announced Graviton3 this past December (offering a 25% improvement on top of Graviton2’s relative gains). The list of what we’ve invented and delivered for customers in EC2 (and AWS in general) is pretty mind-boggling, and this iterative approach to innovation has not only given customers much more functionality in AWS than they can find anywhere else (which is a significant differentiator), but also allowed us to arrive at the much more game-changing offering that AWS is today.   Devices: Our first foray into devices was the Kindle, released in 2007. It was not the most sophisticated industrial design (it was creamy white in color and the corners were uncomfortable for some people to hold), but revolutionary because it offered customers the ability to download any of over 90,000 books (now millions) in 60 seconds—and we got better and faster at building attractive designs.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2021-Shareholder-Letter.pdf'}}, 'score': 0.0045338534}), Document(page_content='While these short-term headwinds soften our growth rate, we like a lot of the fundamentals that we’re seeing in AWS. Our new customer pipeline is robust, as are our active migrations. Many companies use discontinuous periods like this to step back and determine what they strategically want to change, and we find an increasing number of enterprises opting out of managing their own infrastructure, and preferring to move to AWS to enjoy the agility, innovation, cost-efficiency, and security benefits. And most importantly for customers, AWS continues to deliver new capabilities rapidly (over 3,300 new features and services launched in 2022), and invest in long-term inventions that change what’s possible.   Chip development is a good example. In last year’s letter, I mentioned the investment we were making in our general-purpose CPU processors named Graviton. Graviton2-based compute instances deliver up to 40% better price-performance than the comparable latest generation x86-based instances; and in 2022, we delivered our Graviton3 chips, providing 25% better performance than the Graviton2 processors. Further, as machine learning adoption has continued to accelerate, customers have yearned for lower-cost GPUs (the chips most commonly used for machine learning). AWS started investing years ago in these specialized chips for machine learning training and inference (inferences are the predictions or answers that a machine learning model provides). We delivered our first training chip in 2022 (“Trainium”); and for the most common machine learning models, Trainium-based instances are up to 140% faster than GPU-based instances at up to 70% lower cost. Most companies are still in the training stage, but as they develop models that graduate to large-scale production, they’ll find that most of the cost is in inference because models are trained periodically whereas inferences are happening all the time as their associated application is being exercised. We launched our first inference chips (“Inferentia”) in 2019, and they have saved companies like Amazon over a hundred million dollars in capital expense already. Our Inferentia2 chip, which just launched, offers up to four times higher throughput and ten times lower latency than our first Inferentia processor. With the enormous upcoming growth in machine learning, customers will be able to get a lot more done with AWS’s training and inference chips at a significantly lower cost. We’re not close to being done innovating here, and this long-term investment should prove fruitful for both customers and AWS.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2022-Shareholder-Letter.pdf'}}, 'score': 0.0039107245}), Document(page_content='Our Inferentia2 chip, which just launched, offers up to four times higher throughput and ten times lower latency than our first Inferentia processor. With the enormous upcoming growth in machine learning, customers will be able to get a lot more done with AWS’s training and inference chips at a significantly lower cost. We’re not close to being done innovating here, and this long-term investment should prove fruitful for both customers and AWS. AWS is still in the early stages of its evolution, and has a chance for unusual growth in the next decade.   Similarly high potential, Amazon’s Advertising business is uniquely effective for brands, which is part of why it continues to grow at a brisk clip. Akin to physical retailers’ advertising businesses selling shelf space, end- caps, and placement in their circulars, our sponsored products and brands offerings have been an integral part        of the Amazon shopping experience for more than a decade. However, unlike physical retailers, Amazon can tailor these sponsored products to be relevant to what customers are searching for given what we know about shopping behaviors and our very deep investment in machine learning algorithms. This leads to advertising that’s more useful for customers; and as a result, performs better for brands. This is part of why our Advertising revenue has continued to grow rapidly (23% YoY in Q4 2022, 25% YoY overall for 2022 on a $31B revenue base), even as most large advertising-focused businesses’ growth have slowed over the last several quarters.   We strive to be the best place for advertisers to build their brands. We have near and long-term opportunities that will help us achieve that mission. We’re continuing to make large investments in machine learning to keep honing our advertising selection algorithms. For the past couple of years, we’ve invested in building comprehensive, flexible, and durable planning and measurement solutions, giving marketers greater insight into advertising effectiveness. An example is Amazon Marketing Cloud (“AMC”). AMC is a “clean room” (i.e. secure digital environment) in which advertisers can run custom audience and campaign analytics across a range of first and third-party inputs, in a privacy-safe manner, to generate advertising and business insights to inform their broader marketing and sales strategies.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2022-Shareholder-Letter.pdf'}}, 'score': 0.0036656237}), Document(page_content='However, not surprisingly, with that rate and scale of change, there was a lot of optimization needed to yield the intended productivity. Over the last several months, we’ve scrutinized every process path in our fulfillment centers and transportation network and redesigned scores of processes and mechanisms, resulting in steady productivity gains and cost reductions over the last few quarters. There’s more work to do, but we’re pleased with our trajectory and the meaningful upside in front of us.   We also took this occasion to make larger structural changes that set us up better to deliver lower costs and faster speed for many years to come. A good example was reevaluating how our US fulfillment network was organized. Until recently, Amazon operated one national US fulfillment network that distributed inventory from fulfillment centers spread across the entire country. If a local fulfillment center didn’t have the product a customer ordered, we’d end up shipping it from other parts of the country, costing us more and increasing delivery times. This challenge became more pronounced as our fulfillment network expanded to hundreds of additional nodes over the last few years, distributing inventory across more locations and increasing the complexity of connecting the fulfillment center and delivery station nodes efficiently. Last year, we started rearchitecting our inventory placement strategy and leveraging our larger fulfillment center footprint to move from a national fulfillment network to a regionalized network model. We made significant internal changes (e.g. placement and logistics software, processes, physical operations) to create eight interconnected regions in smaller geographic areas. Each of these regions has broad, relevant selection to operate in a largely self- sufficient way, while still being able to ship nationally when necessary. Some of the most meaningful and hard        work came from optimizing the connections between this large amount of infrastructure. We also continue to improve our advanced machine learning algorithms to better predict what customers in various parts of the country will need so that we have the right inventory in the right regions at the right time. We’ve recently completed this regional roll out and like the early results. Shorter travel distances mean lower cost to serve, less impact on the environment, and customers getting their orders faster. On the latter, we’re excited about seeing more next day and same-day deliveries, and we’re on track to have our fastest Prime delivery speeds ever in 2023. Overall, we remain confident about our plans to lower costs, reduce delivery times, and build a meaningfully larger retail business with healthy operating margins.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2022-Shareholder-Letter.pdf'}}, 'score': 0.0035805341})]\n",
      "    </context>\n",
      "\n",
      "    <question>\n",
      "    Compared to Graviton2 processors, what performance improvement did Graviton3 chips deliver according to the passage?\n",
      "    </question>\n",
      "\n",
      "    The response should be specific and use statistics or numbers when possible.\n",
      "\n",
      "    \n",
      "\n",
      "Assistant:\n",
      "  warnings.warn(ALTERNATION_ERROR + f\" Received {input_text}\")\n",
      "/Users/ayanray/GitProjects/amazon-bedrock-samples/.venv/lib/python3.11/site-packages/langchain_community/llms/bedrock.py:73: UserWarning: Error: Prompt must alternate between '\n",
      "\n",
      "Human:' and '\n",
      "\n",
      "Assistant:'. Received \n",
      "\n",
      "Human: \n",
      "    \n",
      "\n",
      "Human: You are a financial advisor AI system, and provides answers to questions by using fact based and statistical information when possible. \n",
      "    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags. \n",
      "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "    <context>\n",
      "    [Document(page_content='In the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated commodity. But, there’s a lot more to compute than just a server. Customers want various flavors of compute (e.g. server configurations optimized for storage, memory, high-performance compute, graphics rendering, machine learning), multiple form factors (e.g. fixed instance sizes, portable containers, serverless functions), various sizes and optimizations of persistent storage, and a slew of networking capabilities. Then, there’s the CPU chip that runs in your compute. For many years, the industry had used Intel or AMD x86 processors. We have important partnerships with these companies, but realized that if we wanted to push price and performance further (as customers requested), we’d have to develop our own chips, too. Our first generalized chip was Graviton, which we announced in 2018. This helped a subset of customer workloads run more cost-effectively than prior options. But, it wasn’t until 2020, after taking the learnings from Graviton and innovating on a new chip, that we had something remarkable with our Graviton2 chip, which provides up to 40% better price-performance than the comparable latest generation x86 processors. Think about how much of an impact 40% improvement on compute is. Compute is used for every bit of technology. That’s a huge deal for customers. And, while Graviton2 has been a significant success thus far (48 of the top 50 AWS EC2 customers have already adopted it), the AWS Chips team was already learning from what customers said could be better, and announced Graviton3 this past December (offering a 25% improvement on top of Graviton2’s relative gains). The list of what we’ve invented and delivered for customers in EC2 (and AWS in general) is pretty mind-boggling, and this iterative approach to innovation has not only given customers much more functionality in AWS than they can find anywhere else (which is a significant differentiator), but also allowed us to arrive at the much more game-changing offering that AWS is today.   Devices: Our first foray into devices was the Kindle, released in 2007. It was not the most sophisticated industrial design (it was creamy white in color and the corners were uncomfortable for some people to hold), but revolutionary because it offered customers the ability to download any of over 90,000 books (now millions) in 60 seconds—and we got better and faster at building attractive designs.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2021-Shareholder-Letter.pdf'}}, 'score': 0.006097814}), Document(page_content='Our Inferentia2 chip, which just launched, offers up to four times higher throughput and ten times lower latency than our first Inferentia processor. With the enormous upcoming growth in machine learning, customers will be able to get a lot more done with AWS’s training and inference chips at a significantly lower cost. We’re not close to being done innovating here, and this long-term investment should prove fruitful for both customers and AWS. AWS is still in the early stages of its evolution, and has a chance for unusual growth in the next decade.   Similarly high potential, Amazon’s Advertising business is uniquely effective for brands, which is part of why it continues to grow at a brisk clip. Akin to physical retailers’ advertising businesses selling shelf space, end- caps, and placement in their circulars, our sponsored products and brands offerings have been an integral part        of the Amazon shopping experience for more than a decade. However, unlike physical retailers, Amazon can tailor these sponsored products to be relevant to what customers are searching for given what we know about shopping behaviors and our very deep investment in machine learning algorithms. This leads to advertising that’s more useful for customers; and as a result, performs better for brands. This is part of why our Advertising revenue has continued to grow rapidly (23% YoY in Q4 2022, 25% YoY overall for 2022 on a $31B revenue base), even as most large advertising-focused businesses’ growth have slowed over the last several quarters.   We strive to be the best place for advertisers to build their brands. We have near and long-term opportunities that will help us achieve that mission. We’re continuing to make large investments in machine learning to keep honing our advertising selection algorithms. For the past couple of years, we’ve invested in building comprehensive, flexible, and durable planning and measurement solutions, giving marketers greater insight into advertising effectiveness. An example is Amazon Marketing Cloud (“AMC”). AMC is a “clean room” (i.e. secure digital environment) in which advertisers can run custom audience and campaign analytics across a range of first and third-party inputs, in a privacy-safe manner, to generate advertising and business insights to inform their broader marketing and sales strategies.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2022-Shareholder-Letter.pdf'}}, 'score': 0.005747719}), Document(page_content='The list of what we’ve invented and delivered for customers in EC2 (and AWS in general) is pretty mind-boggling, and this iterative approach to innovation has not only given customers much more functionality in AWS than they can find anywhere else (which is a significant differentiator), but also allowed us to arrive at the much more game-changing offering that AWS is today.   Devices: Our first foray into devices was the Kindle, released in 2007. It was not the most sophisticated industrial design (it was creamy white in color and the corners were uncomfortable for some people to hold), but revolutionary because it offered customers the ability to download any of over 90,000 books (now millions) in 60 seconds—and we got better and faster at building attractive designs. Shortly thereafter, we launched a tablet, and then a phone (with the distinguishing feature of having front-facing cameras and a gyroscope to give customers a dynamic perspective along with varied 3D experiences). The phone was unsuccessful, and though we determined we were probably too late to this party and directed these resources elsewhere, we hired some fantastic long-term builders and learned valuable lessons from this failure that have served us well in devices like Echo and FireTV.   When I think of the first Echo device—and what Alexa could do for customers at that point—it was noteworthy, yet so much less capable than what’s possible today. Today, there are hundreds of millions of Alexa-enabled devices out there (in homes, offices, cars, hotel rooms, Amazon Echo devices, and third-party manufacturer devices); you can listen to music—or watch videos now; you can control your lights and home automation; you can create routines like “Start My Day” where Alexa tells you the weather, your estimated commute time based on current traffic, then plays the news; you can easily order retail items on Amazon; you can get general or customized news, updates on sporting events and related stats—and we’re still quite early with respect to what Alexa and Alexa-related devices will do for customers. Our goal is for Alexa to be the world’s most helpful and resourceful personal assistant, who makes people’s lives meaningfully easier and better. We have a lot more inventing and iterating to go, but customers continue to indicate that we’re on the right path. We have several other devices at varying stages of evolution (e.g.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2021-Shareholder-Letter.pdf'}}, 'score': 0.005396666}), Document(page_content='While these short-term headwinds soften our growth rate, we like a lot of the fundamentals that we’re seeing in AWS. Our new customer pipeline is robust, as are our active migrations. Many companies use discontinuous periods like this to step back and determine what they strategically want to change, and we find an increasing number of enterprises opting out of managing their own infrastructure, and preferring to move to AWS to enjoy the agility, innovation, cost-efficiency, and security benefits. And most importantly for customers, AWS continues to deliver new capabilities rapidly (over 3,300 new features and services launched in 2022), and invest in long-term inventions that change what’s possible.   Chip development is a good example. In last year’s letter, I mentioned the investment we were making in our general-purpose CPU processors named Graviton. Graviton2-based compute instances deliver up to 40% better price-performance than the comparable latest generation x86-based instances; and in 2022, we delivered our Graviton3 chips, providing 25% better performance than the Graviton2 processors. Further, as machine learning adoption has continued to accelerate, customers have yearned for lower-cost GPUs (the chips most commonly used for machine learning). AWS started investing years ago in these specialized chips for machine learning training and inference (inferences are the predictions or answers that a machine learning model provides). We delivered our first training chip in 2022 (“Trainium”); and for the most common machine learning models, Trainium-based instances are up to 140% faster than GPU-based instances at up to 70% lower cost. Most companies are still in the training stage, but as they develop models that graduate to large-scale production, they’ll find that most of the cost is in inference because models are trained periodically whereas inferences are happening all the time as their associated application is being exercised. We launched our first inference chips (“Inferentia”) in 2019, and they have saved companies like Amazon over a hundred million dollars in capital expense already. Our Inferentia2 chip, which just launched, offers up to four times higher throughput and ten times lower latency than our first Inferentia processor. With the enormous upcoming growth in machine learning, customers will be able to get a lot more done with AWS’s training and inference chips at a significantly lower cost. We’re not close to being done innovating here, and this long-term investment should prove fruitful for both customers and AWS.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2022-Shareholder-Letter.pdf'}}, 'score': 0.0053731296})]\n",
      "    </context>\n",
      "\n",
      "    <question>\n",
      "    Which was the first inference chip launched by AWS according to the passage?\n",
      "    </question>\n",
      "\n",
      "    The response should be specific and use statistics or numbers when possible.\n",
      "\n",
      "    \n",
      "\n",
      "Assistant:\n",
      "  warnings.warn(ALTERNATION_ERROR + f\" Received {input_text}\")\n",
      "/Users/ayanray/GitProjects/amazon-bedrock-samples/.venv/lib/python3.11/site-packages/langchain_community/llms/bedrock.py:73: UserWarning: Error: Prompt must alternate between '\n",
      "\n",
      "Human:' and '\n",
      "\n",
      "Assistant:'. Received \n",
      "\n",
      "Human: \n",
      "    \n",
      "\n",
      "Human: You are a financial advisor AI system, and provides answers to questions by using fact based and statistical information when possible. \n",
      "    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags. \n",
      "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "    <context>\n",
      "    [Document(page_content='.   • Our cash and investment balances at year-end were $125 million, thanks to our initial public offering in May 1997 and our $75 million loan, affording us substantial strategic flexibility.   Our Employees   The past year’s success is the product of a talented, smart, hard-working group, and I take great pride in being a part of this team. Setting the bar high in our approach to hiring has been, and will continue to be, the single most important element of Amazon.com’s success.   It’s not easy to work here (when I interview people I tell them, “You can work long, hard, or smart, but at Amazon.com you can’t choose two out of three”), but we are working to build something important, something that matters to our customers, something that we can all tell our grandchildren about. Such things aren’t meant to be easy. We are incredibly fortunate to have this group of dedicated employees whose sacrifices and passion build Amazon.com.   Goals for 1998   We are still in the early stages of learning how to bring new value to our customers through Internet commerce and merchandising. Our goal remains to continue to solidify and extend our brand and customer base. This requires sustained investment in systems and infrastructure to support outstanding customer convenience, selection, and service while we grow. We are planning to add music to our product offering, and over time we believe that other products may be prudent investments. We also believe there are significant opportunities to better serve our customers overseas, such as reducing delivery times and better tailoring the customer experience. To be certain, a big part of the challenge for us will lie not in finding new ways to expand our business, but in prioritizing our investments.   We now know vastly more about online commerce than when Amazon.com was founded, but we still have so much to learn. Though we are optimistic, we must remain vigilant and maintain a sense of urgency. The challenges and hurdles we will face to make our long-term vision for Amazon.com a reality are several: aggressive, capable, well-funded competition; considerable growth challenges and execution risk; the risks of product and geographic expansion; and the need for large continuing investments to meet an expanding market opportunity.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2020-Shareholder-Letter.pdf'}}, 'score': 0.008792579}), Document(page_content='• Distribution center capacity grew from 50,000 to 285,000 square feet, including a 70% expansion of our Seattle facilities and the launch of our second distribution center in Delaware in November.   • Inventories rose to over 200,000 titles at year-end, enabling us to improve availability for our customers.   • Our cash and investment balances at year-end were $125 million, thanks to our initial public offering in May 1997 and our $75 million loan, affording us substantial strategic flexibility.   Our Employees   The past year’s success is the product of a talented, smart, hard-working group, and I take great pride in being a part of this team. Setting the bar high in our approach to hiring has been, and will continue to be, the single most important element of Amazon.com’s success.   It’s not easy to work here (when I interview people I tell them, “You can work long, hard, or smart, but at Amazon.com you can’t choose two out of three”), but we are working to build something important, something that matters to our customers, something that we can all tell our grandchildren about. Such things aren’t meant to be easy. We are incredibly fortunate to have this group of dedicated employees whose sacrifices and passion build Amazon.com.   Goals for 1998   We are still in the early stages of learning how to bring new value to our customers through Internet commerce and merchandising. Our goal remains to continue to solidify and extend our brand and customer base. This requires sustained investment in systems and infrastructure to support outstanding customer convenience, selection, and service while we grow. We are planning to add music to our product offering, and over time we believe that other products may be prudent investments. We also believe there are significant opportunities to better serve our customers overseas, such as reducing delivery times and better tailoring the customer experience. To be certain, a big part of the challenge for us will lie not in finding new ways to expand our business, but in prioritizing our investments.   We now know vastly more about online commerce than when Amazon.com was founded, but we still have so much to learn. Though we are optimistic, we must remain vigilant and maintain a sense of urgency.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2019-Shareholder-Letter.pdf'}}, 'score': 0.008441969}), Document(page_content='Having virtually every book at your fingertips in 60 seconds, and then being able to store and retrieve them on a lightweight digital reader was not “a thing” yet when we launched Kindle in 2007, nor was a voice-driven personal assistant like Alexa (launched in 2014) that you could use to access entertainment, control your smart home, shop, and retrieve all sorts of information.   There have also been times when macroeconomic conditions or operating inefficiencies have presented us with new challenges. For instance, in the 2001 dot-com crash, we had to secure letters of credit to buy inventory for the holidays, streamline costs to deliver better profitability for the business, yet still prioritized the long-term customer experience and business we were trying to build (if you remember, we actually lowered prices in most of our categories during that tenuous 2001 period). You saw this sort of balancing again in 2008-2009 as we endured the recession provoked by the mortgage-backed securities financial crisis. We took several actions to manage the cost structure and efficiency of our Stores business, but we also balanced this streamlining with investment in customer experiences that we believed could be substantial future businesses with strong returns for shareholders. In 2008, AWS was still a fairly small, fledgling business. We knew we were on to something, but it still required substantial capital investment. There were voices inside and outside of the company questioning why Amazon (known mostly as an online retailer then) would be investing so much in cloud computing. But, we knew we were inventing something special that could create a lot of value for customers and Amazon in the future. We had a head start on potential competitors; and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision to continue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, with strong profitability, that has transformed how customers from start-ups to multinational companies to public sector organizations manage their technology infrastructure. Amazon would be a different company if we’d slowed investment in AWS during that 2008-2009 period.   Change is always around the corner. Sometimes, you proactively invite it in, and sometimes it just comes a-knocking. But, when you see it’s coming, you have to embrace it. And, the companies that do this well over a long period of time usually succeed.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2022-Shareholder-Letter.pdf'}}, 'score': 0.008427389}), Document(page_content='• The percentage of orders from repeat customers grew from over 46% in the fourth quarter of 1996 to over 58% in the same period in 1997.   • In terms of audience reach, per Media Metrix, our Web site went from a rank of 90th to within the top 20.   • We established long-term relationships with many important strategic partners, including America Online, Yahoo!, Excite, Netscape, GeoCities, AltaVista, @Home, and Prodigy.        Infrastructure During 1997, we worked hard to expand our business infrastructure to support these greatly increased   traffic, sales, and service levels:   • Amazon.com’s employee base grew from 158 to 614, and we significantly strengthened our management team.   • Distribution center capacity grew from 50,000 to 285,000 square feet, including a 70% expansion of our Seattle facilities and the launch of our second distribution center in Delaware in November.   • Inventories rose to over 200,000 titles at year-end, enabling us to improve availability for our customers.   • Our cash and investment balances at year-end were $125 million, thanks to our initial public offering in May 1997 and our $75 million loan, affording us substantial strategic flexibility.   Our Employees The past year’s success is the product of a talented, smart, hard-working group, and I take great pride in   being a part of this team. Setting the bar high in our approach to hiring has been, and will continue to be, the single most important element of Amazon.com’s success.   It’s not easy to work here (when I interview people I tell them, “You can work long, hard, or smart, but at Amazon.com you can’t choose two out of three”), but we are working to build something important, something that matters to our customers, something that we can all tell our grandchildren about. Such things aren’t meant to be easy. We are incredibly fortunate to have this group of dedicated employees whose sacrifices and passion build Amazon.com.   Goals for 1998 We are still in the early stages of learning how to bring new value to our customers through Internet   commerce and merchandising. Our goal remains to continue to solidify and extend our brand and customer base. This requires sustained investment in systems and infrastructure to support outstanding customer convenience, selection, and service while we grow.', metadata={'location': {'type': 'S3', 's3Location': {'uri': 's3://rag-evaluation-kb-2/AMZN-2021-Shareholder-Letter.pdf'}}, 'score': 0.008411144})]\n",
      "    </context>\n",
      "\n",
      "    <question>\n",
      "    According to the context, in what year did Amazon's annual revenue increase from $245B to $434B?\n",
      "    </question>\n",
      "\n",
      "    The response should be specific and use statistics or numbers when possible.\n",
      "\n",
      "    \n",
      "\n",
      "Assistant:\n",
      "  warnings.warn(ALTERNATION_ERROR + f\" Received {input_text}\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = [\"How many days has Amazon asked employees to come to work in office?\", \n",
    "             \"By what percentage did AWS revenue grow year-over-year in 2022?\",\n",
    "             \"Compared to Graviton2 processors, what performance improvement did Graviton3 chips deliver according to the passage?\",\n",
    "             \"Which was the first inference chip launched by AWS according to the passage?\",\n",
    "             \"According to the context, in what year did Amazon's annual revenue increase from $245B to $434B?\"\n",
    "            ]\n",
    "ground_truths = [[\"Amazon has asked corporate employees to come back to office at least three days a week beginning May 2022.\"],\n",
    "                [\"AWS had a 29% year-over-year ('YoY') revenue in 2022 on $62B revenue base.\"],\n",
    "                [\"In 2022, AWS delivered their Graviton3 chips, providing 25% better performance than the Graviton2 processors.\"],\n",
    "                [\"AWS launched their first inference chips (“Inferentia”) in 2019, and they have saved companies like Amazon over a hundred million dollars in capital expense.\"],\n",
    "                [\"Amazon's annual revenue increased from $245B in 2019 to $434B in 2022.\"]]\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for query in questions:\n",
    "  answers.append(rag_chain.invoke(query))\n",
    "  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
    "\n",
    "# To dict\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truths\": ground_truths\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the RAG application\n",
    "First, import all the metrics you want to use from `ragas.metrics`. Then, you can use the `evaluate()` function and simply pass in the relevant metrics and the prepared dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [02:08<00:00, 128.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:23<00:00, 83.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [02:29<00:00, 149.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:07<00:00, 67.19s/it]\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "from ragas.llms import LangchainLLM\n",
    "\n",
    "ragas_bedrock_model = LangchainLLM(llm)\n",
    "\n",
    "#set embeddings model for evaluating answer relevancy metric\n",
    "answer_relevancy.embeddings = bedrock_embeddings\n",
    "\n",
    "#specify the metrics here\n",
    "metrics = [\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_precision,\n",
    "        context_recall\n",
    "    ]\n",
    "\n",
    "#set llm for metric evaluation\n",
    "for m in metrics:\n",
    "    m.__setattr__(\"llm\", ragas_bedrock_model)\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can see the resulting RAGAS scores for the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 800\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Please note the scores above gives a relative idea on the performance of your RAG application and should be used with caution and not as standalone scores. Also note, that we have used only 5 question/answer pairs for evaluation, as best practice, you should use enough data to cover different aspects of your document for evaluating model.\n",
    "\n",
    "Based on the scores, you can review other components of your RAG workflow to further optimize the scores, few recommended options are to review your chunking strategy, prompt instructions, adding more numberOfResults for additional context and so on. "
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
