{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create Knowledge Bases for Amazon Bedrock to decide RIGHT Chuncking Size based on your dataset for optimizing RAG application\n",
    "\n",
    "This notebook provides sample code for building multiple knowledge bases for Amazon Bedrock based on different chunk sizes.\n",
    "\n",
    "Repeat the following steps for each chunk size (you want to evaluate):\n",
    "\n",
    "- Create execution role for Knowledge Bases for Amazon Bedrock with necessary policies for accessing data from S3 and writing embeddings into vector store (OpenSearchServerless).\n",
    "- Create an empty OpenSearch serverless index.\n",
    "- Download documents (or point to your document S3 location)\n",
    "- Create knowledge base for Amazon Bedrock \n",
    "- Create a data source within knowledge base which will connect to Amazon S3\n",
    "- Once the data is available in the Bedrock Knowledge Bases with different chunk size, we'll evaluate the text chunks retreived for refernce QA pairs from these knowledge bases for faithfulness, correctness, and relevancy metrics using LlamaIndex. \n",
    "- Using these metrics we can decide for RIGHT chunk size for our RAG based application. \n",
    "\n",
    "Finally, based on the evaluation results, a question answering application with RIGHT chunk strategy can be built using the Amazon Bedrock APIs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usecase\n",
    "### Dataset\n",
    "In this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "Before running the rest of this notebook, you'll need to run the cells below to (ensure necessary libraries are installed and) connect to Bedrock.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> Please ignore error messages related to pip's dependency resolver.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.50.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "awscli 1.32.49 requires botocore==1.34.49, but you have botocore 1.33.13 which is incompatible.\n",
      "awscli 1.32.49 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.8.2 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires urllib3<2, but you have urllib3 2.0.7 which is incompatible.\n",
      "opensearch-py 2.3.1 requires urllib3<2,>=1.21.1, but you have urllib3 2.0.7 which is incompatible.\n",
      "primeqa 0.15.2 requires click~=8.0.4, but you have click 8.1.7 which is incompatible.\n",
      "primeqa 0.15.2 requires fastapi~=0.85.0, but you have fastapi 0.100.1 which is incompatible.\n",
      "primeqa 0.15.2 requires grpcio~=1.48.1, but you have grpcio 1.53.0 which is incompatible.\n",
      "primeqa 0.15.2 requires grpcio-tools~=1.48.1, but you have grpcio-tools 1.53.0 which is incompatible.\n",
      "primeqa 0.15.2 requires numpy~=1.21.5, but you have numpy 1.26.4 which is incompatible.\n",
      "primeqa 0.15.2 requires openai~=0.27.0, but you have openai 1.13.3 which is incompatible.\n",
      "primeqa 0.15.2 requires packaging~=21.3, but you have packaging 23.2 which is incompatible.\n",
      "primeqa 0.15.2 requires pandas~=1.4.0, but you have pandas 2.2.1 which is incompatible.\n",
      "primeqa 0.15.2 requires protobuf~=3.20.0, but you have protobuf 4.21.12 which is incompatible.\n",
      "primeqa 0.15.2 requires tqdm~=4.64.0, but you have tqdm 4.66.2 which is incompatible.\n",
      "primeqa 0.15.2 requires uvicorn~=0.18.0, but you have uvicorn 0.23.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.50.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "awscli 1.32.49 requires botocore==1.34.49, but you have botocore 1.33.2 which is incompatible.\n",
      "awscli 1.32.49 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.8.2 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires urllib3<2, but you have urllib3 2.0.7 which is incompatible.\n",
      "opensearch-py 2.3.1 requires urllib3<2,>=1.21.1, but you have urllib3 2.0.7 which is incompatible.\n",
      "primeqa 0.15.2 requires click~=8.0.4, but you have click 8.1.7 which is incompatible.\n",
      "primeqa 0.15.2 requires fastapi~=0.85.0, but you have fastapi 0.100.1 which is incompatible.\n",
      "primeqa 0.15.2 requires grpcio~=1.48.1, but you have grpcio 1.53.0 which is incompatible.\n",
      "primeqa 0.15.2 requires grpcio-tools~=1.48.1, but you have grpcio-tools 1.53.0 which is incompatible.\n",
      "primeqa 0.15.2 requires numpy~=1.21.5, but you have numpy 1.26.4 which is incompatible.\n",
      "primeqa 0.15.2 requires openai~=0.27.0, but you have openai 1.13.3 which is incompatible.\n",
      "primeqa 0.15.2 requires packaging~=21.3, but you have packaging 23.2 which is incompatible.\n",
      "primeqa 0.15.2 requires pandas~=1.4.0, but you have pandas 2.2.1 which is incompatible.\n",
      "primeqa 0.15.2 requires protobuf~=3.20.0, but you have protobuf 4.21.12 which is incompatible.\n",
      "primeqa 0.15.2 requires tqdm~=4.64.0, but you have tqdm 4.66.2 which is incompatible.\n",
      "primeqa 0.15.2 requires uvicorn~=0.18.0, but you have uvicorn 0.23.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opensearch-py==2.3.1 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (2.3.1)\n",
      "Collecting urllib3<2,>=1.21.1 (from opensearch-py==2.3.1)\n",
      "  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.4.0 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2.31.0)\n",
      "Requirement already satisfied: six in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from opensearch-py==2.3.1) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from opensearch-py==2.3.1) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.3.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.3.1) (3.6)\n",
      "Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "Successfully installed urllib3-1.26.18\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: retrying==1.3.4 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (1.3.4)\n",
      "Requirement already satisfied: six>=1.7.0 in /Users/snghigf/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages (from retrying==1.3.4) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.50.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "awscli 1.32.49 requires botocore==1.34.49, but you have botocore 1.33.2 which is incompatible.\n",
      "awscli 1.32.49 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.8.2 which is incompatible.\n",
      "botocore 1.33.2 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.1 which is incompatible.\n",
      "chainlit 0.7.700 requires dataclasses_json<0.6.0,>=0.5.7, but you have dataclasses-json 0.6.4 which is incompatible.\n",
      "chainlit 0.7.700 requires httpx<0.25.0,>=0.23.0, but you have httpx 0.27.0 which is incompatible.\n",
      "langchain-community 0.0.25 requires langchain-core<0.2.0,>=0.1.28, but you have langchain-core 0.0.13 which is incompatible.\n",
      "langchain-community 0.0.25 requires langsmith<0.2.0,>=0.1.0, but you have langsmith 0.0.92 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires dataclasses-json<0.6.0,>=0.5.7, but you have dataclasses-json 0.6.4 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires urllib3<2, but you have urllib3 2.2.1 which is incompatible.\n",
      "opensearch-py 2.3.1 requires urllib3<2,>=1.21.1, but you have urllib3 2.2.1 which is incompatible.\n",
      "primeqa 0.15.2 requires click~=8.0.4, but you have click 8.1.7 which is incompatible.\n",
      "primeqa 0.15.2 requires fastapi~=0.85.0, but you have fastapi 0.100.1 which is incompatible.\n",
      "primeqa 0.15.2 requires grpcio~=1.48.1, but you have grpcio 1.53.0 which is incompatible.\n",
      "primeqa 0.15.2 requires grpcio-tools~=1.48.1, but you have grpcio-tools 1.53.0 which is incompatible.\n",
      "primeqa 0.15.2 requires numpy~=1.21.5, but you have numpy 1.26.4 which is incompatible.\n",
      "primeqa 0.15.2 requires openai~=0.27.0, but you have openai 1.13.3 which is incompatible.\n",
      "primeqa 0.15.2 requires packaging~=21.3, but you have packaging 23.2 which is incompatible.\n",
      "primeqa 0.15.2 requires pandas~=1.4.0, but you have pandas 2.2.1 which is incompatible.\n",
      "primeqa 0.15.2 requires protobuf~=3.20.0, but you have protobuf 4.21.12 which is incompatible.\n",
      "primeqa 0.15.2 requires tqdm~=4.64.0, but you have tqdm 4.66.2 which is incompatible.\n",
      "primeqa 0.15.2 requires uvicorn~=0.18.0, but you have uvicorn 0.23.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: pandas 2.2.1 does not provide the extra 'jinja2'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.50.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "asyncer 0.0.2 requires anyio<4.0.0,>=3.4.0, but you have anyio 4.3.0 which is incompatible.\n",
      "chainlit 0.7.700 requires httpx<0.25.0,>=0.23.0, but you have httpx 0.27.0 which is incompatible.\n",
      "langchain 0.0.342 requires anyio<4.0, but you have anyio 4.3.0 which is incompatible.\n",
      "langchain-community 0.0.25 requires langchain-core<0.2.0,>=0.1.28, but you have langchain-core 0.0.13 which is incompatible.\n",
      "langchain-community 0.0.25 requires langsmith<0.2.0,>=0.1.0, but you have langsmith 0.0.92 which is incompatible.\n",
      "primeqa 0.15.2 requires click~=8.0.4, but you have click 8.1.7 which is incompatible.\n",
      "primeqa 0.15.2 requires fastapi~=0.85.0, but you have fastapi 0.100.1 which is incompatible.\n",
      "primeqa 0.15.2 requires grpcio~=1.48.1, but you have grpcio 1.53.0 which is incompatible.\n",
      "primeqa 0.15.2 requires grpcio-tools~=1.48.1, but you have grpcio-tools 1.53.0 which is incompatible.\n",
      "primeqa 0.15.2 requires numpy~=1.21.5, but you have numpy 1.26.4 which is incompatible.\n",
      "primeqa 0.15.2 requires openai~=0.27.0, but you have openai 1.13.3 which is incompatible.\n",
      "primeqa 0.15.2 requires packaging~=21.3, but you have packaging 23.2 which is incompatible.\n",
      "primeqa 0.15.2 requires pandas~=1.4.0, but you have pandas 2.2.1 which is incompatible.\n",
      "primeqa 0.15.2 requires protobuf~=3.20.0, but you have protobuf 4.21.12 which is incompatible.\n",
      "primeqa 0.15.2 requires tqdm~=4.64.0, but you have tqdm 4.66.2 which is incompatible.\n",
      "primeqa 0.15.2 requires uvicorn~=0.18.0, but you have uvicorn 0.23.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install boto3==1.33.2 --force-reinstall --quiet\n",
    "%pip install botocore==1.33.2 --force-reinstall --quiet\n",
    "%pip install -U opensearch-py==2.3.1\n",
    "%pip install -U retrying==1.3.4\n",
    "\n",
    "%pip install langchain==0.0.342 --force-reinstall --quiet\n",
    "%pip install llama-index==0.9.3.post1 --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import random\n",
    "from retrying import retry\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "from utility import create_bedrock_execution_role, create_oss_policy_attach_bedrock_execution_role, create_policies_in_oss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting boto3 clients for required AWS services\n",
    "sts_client = boto3.client('sts')\n",
    "iam_client = boto3.client('iam')\n",
    "s3_client = boto3.client(\"s3\")\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime')\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime')\n",
    "aoss_client = boto3.client('opensearchserverless')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('us-east-1', '017444429555')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = boto3.session.Session()\n",
    "region_name = session.region_name\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region_name, account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate random suffix for unique vector store, vector index, and KB name\n",
    "suffix = random.randrange(200, 900)\n",
    "# Bucket name  and prefix for KB files \n",
    "bucket_name = f\"chunk-evaluation-{region_name}-{account_id}\" # replace it with your bucket name.\n",
    "s3_prefix = 'shareholder-letters'\n",
    "\n",
    "# Create S3 bucket for uploading the data\n",
    "s3bucket = s3_client.create_bucket(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Create a vector store - OpenSearch Serverless index\n",
    "\n",
    "### Create OSS policies and collection\n",
    "First of all we have to create a vector store. In this section we will use *Amazon OpenSerach serverless.*\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your application—without impacting data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_name = f'bedrock-sample-rag-{suffix}'\n",
    "base_index_name = f\"bedrock-sample-rag-index-{suffix}\"\n",
    "\n",
    "# create IAM role\n",
    "bedrock_kb_execution_role = create_bedrock_execution_role(bucket_name=bucket_name)\n",
    "bedrock_kb_execution_role_name = bedrock_kb_execution_role['Role']['RoleName']\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AmazonBedrockExecutionRoleForKnowledgeBase_759'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_kb_execution_role_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create security, network and data access policies within OSS\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(vector_store_name=vector_store_name,\n",
    "                       aoss_client=aoss_client,\n",
    "                       bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'ResponseMetadata': { 'HTTPHeaders': { 'connection': 'keep-alive',\n",
      "                                         'content-length': '312',\n",
      "                                         'content-type': 'application/x-amz-json-1.0',\n",
      "                                         'date': 'Tue, 05 Mar 2024 19:58:17 '\n",
      "                                                 'GMT',\n",
      "                                         'x-amzn-requestid': '12217fe3-10e3-4924-83c3-51769150d1de'},\n",
      "                        'HTTPStatusCode': 200,\n",
      "                        'RequestId': '12217fe3-10e3-4924-83c3-51769150d1de',\n",
      "                        'RetryAttempts': 0},\n",
      "  'createCollectionDetail': { 'arn': 'arn:aws:aoss:us-east-1:017444429555:collection/mfivuhtfc0ib6yl6nil',\n",
      "                              'createdDate': 1709668697679,\n",
      "                              'id': 'mfivuhtfc0ib6yl6nil',\n",
      "                              'kmsKeyArn': 'auto',\n",
      "                              'lastModifiedDate': 1709668697679,\n",
      "                              'name': 'bedrock-sample-rag-699',\n",
      "                              'status': 'CREATING',\n",
      "                              'type': 'VECTORSEARCH'}}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(collection)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfivuhtfc0ib6yl6nil.us-east-1.aoss.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opensearch serverless arn:  arn:aws:iam::017444429555:policy/AmazonBedrockOSSPolicyForKnowledgeBase_759\n"
     ]
    }
   ],
   "source": [
    "# create oss policy and attach it to Bedrock execution role\n",
    "create_oss_policy_attach_bedrock_execution_role(collection_id=collection_id,\n",
    "                                                bedrock_kb_execution_role=bedrock_kb_execution_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 -  Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare dataset\n",
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to s3 in a prefix\n",
    "def uploadDirectory(path,bucket_name):\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            for file in files:\n",
    "                s3_client.upload_file(os.path.join(root,file), bucket_name, s3_prefix + '/' + file)\n",
    "\n",
    "uploadDirectory(data_root, bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Chunking Evaluation Setup\n",
    "\n",
    "### OSS Configurations\n",
    "We'll set-up multiple KBs with various chunk sizes, and create multiple vector indexes for storing Embeddings for corresponding chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\"\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1536\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "# # It can take up to a minute for data access rules to be enforced\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Base Configurations\n",
    "Steps:\n",
    "- initialize Open search serverless configuration which will include collection ARN, index name, vector field, text field and metadata field.\n",
    "- initialize chunking strategy, based on which KB will split the documents into pieces of size equal to the chunk size mentioned in the `chunkingStrategyConfiguration`.\n",
    "- initialize the s3 configuration, which will be used to create the data source object later.\n",
    "- initialize the Titan embeddings model ARN, as this will be used to create the embeddings for each of the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearchServerlessConfiguration = {\n",
    "            \"collectionArn\": collection[\"createCollectionDetail\"]['arn'],\n",
    "            \"vectorIndexName\": base_index_name,\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",\n",
    "                \"textField\": \"text\",\n",
    "                \"metadataField\": \"text-metadata\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "chunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 512,\n",
    "        \"overlapPercentage\": 20\n",
    "    }\n",
    "}\n",
    "\n",
    "s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{bucket_name}\",\n",
    "    \"inclusionPrefixes\":[s3_prefix] # you can use this if you want to create a KB using data within s3 prefixes.\n",
    "}\n",
    "\n",
    "embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\"\n",
    "\n",
    "name = f\"bedrock-sample-knowledge-base-{suffix}\"\n",
    "description = \"Amazon shareholder letter knowledge base.\"\n",
    "roleArn = bedrock_kb_execution_role_arn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use these configurations as input to the `create_e2e_kb` method (defined below), which will set-up the end-to-end Knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 -  Create seperate Knowledge base for each chunk size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create separate Knowledge base for each chunk size. This function will create e2e Knowledge base by following following steps:\n",
    "\n",
    "- Create Knowledge bases and OpenSearchServerless collection/index based on the input configuration\n",
    "- Create Datasource for each Knowledge base\n",
    "- Create Ingestion Job for ingesting data into each Knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to setup end-to-end knowledge base as mentioned above\n",
    "\n",
    "def create_e2e_kb(kb_name, kb_description, bucket_name, chunk_size, overlapPercentage, oss_client, base_index_name):\n",
    "    \n",
    "    index_name = base_index_name + '-' + str(chunk_size)\n",
    "    response = oss_client.indices.create(index=index_name, body=json.dumps(body_json))\n",
    "    time.sleep(20)\n",
    "    \n",
    "    # Set the parameters value\n",
    "    opensearchServerlessConfiguration['vectorIndexName'] = index_name\n",
    "    chunkingStrategyConfiguration['fixedSizeChunkingConfiguration']['maxTokens']= chunk_size\n",
    "    chunkingStrategyConfiguration['fixedSizeChunkingConfiguration']['overlapPercentage']= overlapPercentage\n",
    "    \n",
    "   \n",
    "    kb_name = f\"{kb_name}-{suffix}-{chunk_size}\"\n",
    "    description = f\"{kb_description}-{chunk_size}\"\n",
    "    \n",
    "    # Create a KnowledgeBase\n",
    "    kb_response=''\n",
    "    \n",
    "    try:\n",
    "        # @retry(wait_random_min=1000, wait_random_max=2000,stop_max_attempt_number=7)\n",
    "        create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "                name = kb_name,\n",
    "                description = description,\n",
    "                roleArn = roleArn,\n",
    "                knowledgeBaseConfiguration = {\n",
    "                    \"type\": \"VECTOR\",\n",
    "                    \"vectorKnowledgeBaseConfiguration\": {\n",
    "                        \"embeddingModelArn\": embeddingModelArn\n",
    "                    }\n",
    "                },\n",
    "                storageConfiguration = {\n",
    "                    \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "                    \"opensearchServerlessConfiguration\":opensearchServerlessConfiguration\n",
    "                }\n",
    "            )\n",
    "\n",
    "        kb_response= create_kb_response[\"knowledgeBase\"]\n",
    "        # Get KnowledgeBase \n",
    "        print(f\"KB ID: {kb_response['knowledgeBaseId']} KB Name: {kb_response['name']}\")\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"{err=}, {type(err)=}\")\n",
    "    \n",
    "    # Get KnowledgeBase \n",
    "    get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb_response['knowledgeBaseId'])\n",
    "        \n",
    "    # Create a DataSource in KnowledgeBase \n",
    "    create_ds_response = bedrock_agent_client.create_data_source(\n",
    "        name = kb_name,\n",
    "        description = description,\n",
    "        knowledgeBaseId = kb_response['knowledgeBaseId'],\n",
    "        dataSourceConfiguration = {\n",
    "            \"type\": \"S3\",\n",
    "            \"s3Configuration\":s3Configuration\n",
    "        },\n",
    "        vectorIngestionConfiguration = {\n",
    "            \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "        }\n",
    "    )\n",
    "    ds = create_ds_response[\"dataSource\"]\n",
    "    \n",
    "    # Get DataSource \n",
    "    bedrock_agent_client.get_data_source(knowledgeBaseId = kb_response['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])\n",
    "    \n",
    "    # Start an ingestion job\n",
    "    start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb_response['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])\n",
    "    job = start_job_response[\"ingestionJob\"]\n",
    " \n",
    "    kb_id = kb_response[\"knowledgeBaseId\"]\n",
    "    \n",
    "    return kb_name, kb_id, index_name, job\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB ID: MULPGDRDQZ KB Name: amazon-shareholder-letters-knowledge-base-699-300\n",
      "KB ID: EYKWHFDT9R KB Name: amazon-shareholder-letters-knowledge-base-699-512\n",
      "KB ID: Z3FLLRDBYF KB Name: amazon-shareholder-letters-knowledge-base-699-700\n",
      "CPU times: user 61.5 ms, sys: 6.46 ms, total: 67.9 ms\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define variables for creating KnowledgeBase\n",
    "\n",
    "base_kb_name = f\"amazon-shareholder-letters-knowledge-base\"\n",
    "kb_description = \"Amazon shareholder letter knowledge base.\"\n",
    "overlapPercentage = 20\n",
    "\n",
    "kb_oss_index_dict={}\n",
    "job_list =[]\n",
    "\n",
    "# Define chunksize you want to evaluate\n",
    "chunkSizes = [300, 512, 700]\n",
    "data = []\n",
    "\n",
    "# Create KB for each chunk size\n",
    "for chunk_size in chunkSizes:\n",
    "    kb_name, kb_id, index_name, job = create_e2e_kb(base_kb_name, kb_description, bucket_name, chunk_size, overlapPercentage, oss_client, base_index_name)\n",
    "    kb_oss_index_dict[chunk_size] = {'kb_id':kb_id, 'kb_name':kb_name, 'index_name':index_name}\n",
    "    job_list.append(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'knowledgeBaseId': 'MULPGDRDQZ',\n",
       "  'dataSourceId': 'PQCRNN189I',\n",
       "  'ingestionJobId': '9PULPGWYMN',\n",
       "  'status': 'STARTING',\n",
       "  'statistics': {'numberOfDocumentsScanned': 0,\n",
       "   'numberOfNewDocumentsIndexed': 0,\n",
       "   'numberOfModifiedDocumentsIndexed': 0,\n",
       "   'numberOfDocumentsDeleted': 0,\n",
       "   'numberOfDocumentsFailed': 0},\n",
       "  'startedAt': datetime.datetime(2024, 3, 5, 19, 59, 55, 666655, tzinfo=tzutc()),\n",
       "  'updatedAt': datetime.datetime(2024, 3, 5, 19, 59, 55, 666655, tzinfo=tzutc())},\n",
       " {'knowledgeBaseId': 'EYKWHFDT9R',\n",
       "  'dataSourceId': '1FYVVT31AB',\n",
       "  'ingestionJobId': 'TUWBFKKXRA',\n",
       "  'status': 'STARTING',\n",
       "  'statistics': {'numberOfDocumentsScanned': 0,\n",
       "   'numberOfNewDocumentsIndexed': 0,\n",
       "   'numberOfModifiedDocumentsIndexed': 0,\n",
       "   'numberOfDocumentsDeleted': 0,\n",
       "   'numberOfDocumentsFailed': 0},\n",
       "  'startedAt': datetime.datetime(2024, 3, 5, 20, 0, 17, 338204, tzinfo=tzutc()),\n",
       "  'updatedAt': datetime.datetime(2024, 3, 5, 20, 0, 17, 338204, tzinfo=tzutc())},\n",
       " {'knowledgeBaseId': 'Z3FLLRDBYF',\n",
       "  'dataSourceId': 'XTECZN5DGA',\n",
       "  'ingestionJobId': 'UBESMHDEUY',\n",
       "  'status': 'STARTING',\n",
       "  'statistics': {'numberOfDocumentsScanned': 0,\n",
       "   'numberOfNewDocumentsIndexed': 0,\n",
       "   'numberOfModifiedDocumentsIndexed': 0,\n",
       "   'numberOfDocumentsDeleted': 0,\n",
       "   'numberOfDocumentsFailed': 0},\n",
       "  'startedAt': datetime.datetime(2024, 3, 5, 20, 0, 39, 26322, tzinfo=tzutc()),\n",
       "  'updatedAt': datetime.datetime(2024, 3, 5, 20, 0, 39, 26322, tzinfo=tzutc())}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait while Ingestion job is running\n",
    "# Get job status\n",
    "for job in job_list:\n",
    "    while(job['status']!='COMPLETE' ):\n",
    "        get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "          knowledgeBaseId = job_list[-1]['knowledgeBaseId'],\n",
    "            dataSourceId = job_list[-1][\"dataSourceId\"],\n",
    "            ingestionJobId = job_list[-1][\"ingestionJobId\"]\n",
    "          )\n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "        time.sleep(10)\n",
    "    # pp.pprint(job)\n",
    "    # time.sleep(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 300: { 'index_name': 'bedrock-sample-rag-index-699-300',\n",
      "         'kb_id': 'MULPGDRDQZ',\n",
      "         'kb_name': 'amazon-shareholder-letters-knowledge-base-699-300'},\n",
      "  512: { 'index_name': 'bedrock-sample-rag-index-699-512',\n",
      "         'kb_id': 'EYKWHFDT9R',\n",
      "         'kb_name': 'amazon-shareholder-letters-knowledge-base-699-512'},\n",
      "  700: { 'index_name': 'bedrock-sample-rag-index-699-700',\n",
      "         'kb_id': 'Z3FLLRDBYF',\n",
      "         'kb_name': 'amazon-shareholder-letters-knowledge-base-699-700'}}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(kb_oss_index_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store kb_oss_index_dict, so we can retrieve it while CleanUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'kb_oss_index_dict' (dict)\n"
     ]
    }
   ],
   "source": [
    "%store kb_oss_index_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Chunking evaluation setup\n",
    "\n",
    "    - Define Retrieve function - to get text chunks\n",
    "    - Define Prompt template specific to the model\n",
    "    - Define Refernce QA pair\n",
    "    - Define function to evaluate text chunks for faithfulness, correctness, and relevancy metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve functions that calls Retrieve API\n",
    "\n",
    "Define a retrieve function that calls the `Retreive API` provided by Knowledge bases for Amazon Bedrock which converts user queries into\n",
    "embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom\n",
    "workﬂows on top of the semantic search results. The output of the `Retrieve API` includes the the `retrieved text chunks`, the `location type` and `URI` of the source data, as well as the relevance `scores` of the retrievals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve(query, kbId, numberOfResults=5):\n",
    "    return bedrock_agent_runtime_client.retrieve(\n",
    "        retrievalQuery= {\n",
    "            'text': query\n",
    "        },\n",
    "        knowledgeBaseId=kbId,\n",
    "        retrievalConfiguration= {\n",
    "            'vectorSearchConfiguration': {\n",
    "                'numberOfResults': numberOfResults\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt template - specific to the model to personalize responses \n",
    "\n",
    "Here, we will use the specific prompt below for the model to act as a financial advisor AI system that will provide answers to questions by using fact based and statistical information when possible. We will provide the `Retrieve API` responses from above as a part of the `{context_str}` in the prompt for the model to refer to, along with the user `query`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a financial advisor AI system, and provides answers to questions by using fact based and statistical information when possible. \n",
    "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{context_str}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{query_str}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\"\"\"\n",
    "titan_prompt = PromptTemplate(template=PROMPT_TEMPLATE, \n",
    "                               input_variables=[\"context_str\",\"query_str\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fetch the text chunks from the RetrieveAPI response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fetch context from the response\n",
    "def get_contexts(retrievalResults):\n",
    "    contexts = []\n",
    "    for retrievedResult in retrievalResults: \n",
    "        contexts.append(retrievedResult['content']['text'])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference QA pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_question_answer_pair = [(\"How many days has Amazon asked employees to come to work in office?\",\n",
    "                          \"Amazon has asked corporate employees to come back to office at least three days a week beginning May 2022.\"),\n",
    "                         (\"By what percentage did AWS revenue grow year-over-year in 2022?\",\n",
    "                          \"AWS had a 29% year-over-year ('YoY') revenue in 2022 on $62B revenue base.\"),\n",
    "                         (\"Compared to Graviton2 processors, what performance improvement did Graviton3 chips deliver according to the passage?\",\n",
    "                          \"In 2022, AWS delivered their Graviton3 chips, providing 25% better performance than the Graviton2 processors.\"),\n",
    "                         (\"Which was the first inference chip launched by AWS according to the passage?\",\n",
    "                          \"AWS launched their first inference chips (“Inferentia”) in 2019, and they have saved companies like Amazon over a hundred million dollars in capital expense.\"),\n",
    "                         (\"According to the context, in what year did Amazon's annual revenue increase from $245B to $434B?\",\n",
    "                          \"Amazon's annual revenue increased from $245B in 2019 to $434B in 2022.\"\n",
    "                          )\n",
    "                          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Evaluate Chunk size for RAG workflow\n",
    "\n",
    "For our evaluation, we will use the Retreive API provided by Knowledge bases for Amazon Bedrock which converts user queries into embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom workﬂows on top of the semantic search results. The output of the Retrieve API includes the the retrieved text chunks, the location type and URI of the source data, as well as the scores of the retrievals.\n",
    "\n",
    "Next, we'll evaluate the relevancy of text chunks corresponding to the original prompt using LLaMaIndex relevancy metrics, and compare it against a threshold value.\n",
    "\n",
    "- if relevancy score for retreived text chunks doesn't pass the threshold, then original prompt won't be passed to large language model (LLM) to generate the response.\n",
    "    \n",
    "- otherwise, we will use the text chunks being generated and augment it with the original prompt and pass it through the LLM using prompt engineering patterns.\n",
    "\n",
    "        - Finally we will evaluate the generated responses using metrics such as faithfulness, correctness, and relevancy metrics. For evaluation, we will use Anthropic Claude v2 model, and for response generation we'll use amazon.titan-text-express-v1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llms import LangChainLLM\n",
    "from langchain.llms import Bedrock\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from llama_index.evaluation import RelevancyEvaluator, FaithfulnessEvaluator, CorrectnessEvaluator\n",
    "from llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, set_global_service_context\n",
    "\n",
    "def evaluate_chunk_size_kb(chunk_size, qa_pairs, kb_id, vector_index, relevancy_threshold):\n",
    "    \"\"\"\n",
    "    Evaluate the average response time, faithfulness, and relevancy of responses generated by amazon.titan-text-lite-v1 for a given chunk size.\n",
    "    We'll use 'amazon.titan-embed-text-v1' for embedding and 'anthropic.claude-v2' to evaluate the response\n",
    "    \n",
    "    Parameters:\n",
    "    chunk_size (int): The size of data chunks being processed.\n",
    "    qa_pairs (list): List of QA tuples\n",
    "    kb_id (str) : KnowledgeBase ID under evaluation\n",
    "    vector_index (str): vector index associated with KnowledgeBase\n",
    "    relevancy_threshold (float) : Threshold value for Relevancy\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
    "    \"\"\"\n",
    "    bedrock_client = boto3.client('bedrock-runtime')\n",
    "    \n",
    "    # Configurations\n",
    "    llm_parameters = {\n",
    "    \"maxTokenCount\":2000,\n",
    "    \"stopSequences\":[],\n",
    "    \"temperature\":0,\n",
    "    \"topP\":0.9\n",
    "    }\n",
    "\n",
    "    model_kwargs_claude = {\n",
    "        \"temperature\": 0,\n",
    "        \"top_k\": 10,\n",
    "        \"max_tokens_to_sample\": 3000\n",
    "    }\n",
    "\n",
    "   \n",
    "    # 1. define bedrock model to generate and evaluate the responses\n",
    "    llm = Bedrock(model_id = \"amazon.titan-text-express-v1\",\n",
    "              model_kwargs=llm_parameters,\n",
    "              client = bedrock_runtime_client,)\n",
    "    \n",
    "    # 2. define bedrock model to evaluate the responses\n",
    "    llm_claude = Bedrock(model_id = \"anthropic.claude-v2\",\n",
    "              model_kwargs=model_kwargs_claude,\n",
    "              client = bedrock_runtime_client,)\n",
    "    \n",
    "    # 3. Define the embed model to be used - \n",
    "    embed_model = BedrockEmbeddings(model_id='amazon.titan-embed-text-v1')\n",
    "    \n",
    "    # Finally, get vector index\n",
    "    vector_index = vector_index\n",
    "    kb_id = kb_id\n",
    "    \n",
    "    # Establishing evaluators - Pass two parameters llm and embed model in the service context\n",
    "    serviceContextLLM = ServiceContext.from_defaults(llm = llm_claude, embed_model=embed_model)\n",
    "    set_global_service_context(service_context)\n",
    "    faithfulnessLLM = FaithfulnessEvaluator(service_context=serviceContextLLM)\n",
    "    relevancyLLM = RelevancyEvaluator(service_context=serviceContextLLM)\n",
    "    CorrectnessLLM = CorrectnessEvaluator(service_context=serviceContextLLM)\n",
    "    \n",
    "    \n",
    "    # Define variables\n",
    "    relevancy_threshold = relevancy_threshold # Relevancy threshold\n",
    "    \n",
    "    total_response_time = 0\n",
    "    total_faithfulness = 0\n",
    "    total_relevancy = 0\n",
    "    total_correctness = []\n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    average_response_time = 0\n",
    "    average_faithfulness = 0\n",
    "    average_relevancy = 0\n",
    "    avereage_correctness = 0\n",
    "    \n",
    "    \n",
    "    # Check for the avg relevancy score\n",
    "    '''\n",
    "    If avg relevency score is under 'relevancy_threshold', we won't pass the context to LLM to\n",
    "    generate the response. This will avoid unneccessary cost for LLM inference.\n",
    "    '''\n",
    "    relevancy_score = 0\n",
    "    avg_relevancy_score = 0\n",
    "    \n",
    "    for question, reference_answer in qa_pairs:\n",
    "        result = retrieve(question, kb_id, 5)\n",
    "        retrievalResults = result['retrievalResults']\n",
    "        # get context\n",
    "        contexts = get_contexts(retrievalResults=retrievalResults)\n",
    "        relevancy_result = relevancyLLM.evaluate(query=question,\n",
    "                                                 response=reference_answer, \n",
    "                                                  contexts=contexts)\n",
    "        relevancy_score += relevancy_result.score\n",
    "     \n",
    "    avg_relevancy_score = relevancy_score/len(qa_pairs)\n",
    "      \n",
    "    eval_qa_num = 0\n",
    "   \n",
    "    # if 'avg_relevancy_score' passes the threshold -  then calculate the metrics\n",
    "    if avg_relevancy_score > relevancy_threshold:\n",
    "       \n",
    "        # Iterate over each question in eval_question_answer_pair to compute metrics.\n",
    "        # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n",
    "        for question, reference_answer in qa_pairs:\n",
    "            print(\"\\t Question:\", question)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            # retrieve matching documents\n",
    "            result = retrieve(question, kb_id, 5)\n",
    "            retrievalResults = result['retrievalResults']\n",
    "            contexts = get_contexts(retrievalResults=retrievalResults)\n",
    "            #call LLM with updated context and question.\n",
    "            prompt = titan_prompt.format(context_str=contexts, \n",
    "                                     query_str=question)\n",
    "            response_vector = llm(prompt)\n",
    "            # generated_answer = str(response_vector)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                \n",
    "                \n",
    "                faithfulness_result = faithfulnessLLM.evaluate(query=question,\n",
    "                                                              response=response_vector, \n",
    "                                                              contexts=contexts)\n",
    "\n",
    "                relevancy_result = relevancyLLM.evaluate(query=question,\n",
    "                                                              response=response_vector, \n",
    "                                                              contexts=contexts)\n",
    "            \n",
    "                correctness_result = CorrectnessLLM.evaluate(\n",
    "                                                            query=question,\n",
    "                                                            response=response_vector,\n",
    "                                                            reference=reference_answer)\n",
    "                \n",
    "                print(f\"\\t Eval Result -> elapsed_time: {elapsed_time}, faithfulness: {faithfulness_result.passing}, relevancy:{relevancy_result.score}, correctness: {correctness_result.score}\" )\n",
    "                print(\"\\t Response:\", response_vector)\n",
    "                total_response_time += elapsed_time\n",
    "                total_faithfulness += faithfulness_result.passing\n",
    "                total_relevancy += relevancy_result.score\n",
    "                total_correctness.append(correctness_result.score)\n",
    "                eval_qa_num += 1      \n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"\\tAn error occured while evaluating the question!!!\")\n",
    "                print(e)\n",
    "                total_response_time += elapsed_time\n",
    "                total_faithfulness = 0\n",
    "                total_relevancy = 0\n",
    "                total_correctness.append(0.0)\n",
    "                print(f\"\\t Eval Result -> elapsed_time: {elapsed_time}, faithfulness: err, relevancy: err, correctness: err\" )\n",
    "\n",
    "        # Calculate average metrics\n",
    "        average_response_time = total_response_time / eval_qa_num\n",
    "        average_faithfulness = total_faithfulness / eval_qa_num\n",
    "        average_relevancy = total_relevancy / eval_qa_num\n",
    "        avereage_correctness = sum(total_correctness)/ eval_qa_num\n",
    "                      \n",
    "    # if 'avg_relevancy_score' doesn't passe the threshold -  then don't send to LLM\n",
    "    else:\n",
    "        print(\"\\t Avg relevancy score was below threshold, so Context not sent to the LLM\")\n",
    "        average_response_time = 0\n",
    "        average_faithfulness = 0\n",
    "        average_relevancy = relevancy_score\n",
    "        avereage_correctness = 0\n",
    "        \n",
    "    \n",
    "    # metrics_score ={}\n",
    "    metrics_score = {\n",
    "                    'average_relevancy' : f'{average_relevancy:.2f}',\n",
    "                    'avg_response_time' : f'{average_response_time:.2f}',\n",
    "                    'average_faithfulness' : f'{average_faithfulness:.2f}',\n",
    "                    'avereage_correctness' : f'{avereage_correctness:.2f}',\n",
    "                    # 'detail_eval_df': evals_df\n",
    "                    } \n",
    "    print(\"\\t Evaluation result: \", metrics_score)\n",
    "\n",
    "    return  metrics_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Chunk Size - 300 evaluation : Initiating...\n",
      "--------------------------------------------------------\n",
      "In progress....\n",
      "\t Question: How many days has Amazon asked employees to come to work in office?\n",
      "\t Eval Result -> elapsed_time: 1.323112964630127, faithfulness: False, relevancy:0.0, correctness: 2.0\n",
      "\t Response: \n",
      "Two days a week.\n",
      "\t Question: By what percentage did AWS revenue grow year-over-year in 2022?\n",
      "\t Eval Result -> elapsed_time: 1.3365061283111572, faithfulness: False, relevancy:1.0, correctness: 3.0\n",
      "\t Response: 29%\n",
      "\t Question: Compared to Graviton2 processors, what performance improvement did Graviton3 chips deliver according to the passage?\n",
      "\t Eval Result -> elapsed_time: 7.602873802185059, faithfulness: False, relevancy:1.0, correctness: 3.0\n",
      "\t Response: Graviton3 chips, which were introduced in 2020 and offer a 25% improvement on top of Graviton2's relative gains, provide up to 40% better price-performance than the comparable latest generation x86 processors.\n",
      "\t Question: Which was the first inference chip launched by AWS according to the passage?\n",
      "\t Eval Result -> elapsed_time: 1.3834359645843506, faithfulness: False, relevancy:1.0, correctness: 3.0\n",
      "\t Response: Inferentia\n",
      "\t Question: According to the context, in what year did Amazon's annual revenue increase from $245B to $434B?\n",
      "\t Eval Result -> elapsed_time: 3.3670618534088135, faithfulness: True, relevancy:0.0, correctness: 2.0\n",
      "\t Response: In 2020, Amazon's North America and International Consumer revenue grew 39% YoY on the very large 2019 revenue base of $245 billion; and, this extraordinary growth extended into 2021 with revenue increasing 43% YoY in Q1 2021.\n",
      "\t Evaluation result:  {'average_relevancy': '0.60', 'avg_response_time': '3.00', 'average_faithfulness': '0.20', 'avereage_correctness': '2.60'}\n",
      "Chunk size 300 - Average Relevancy: 0.60, Average Response time: 3.00, Average Faithfulness: 0.20,  Average Correctness: 2.60\n",
      "Done!!\n",
      "\n",
      "--------------------------------------------------------\n",
      "Chunk Size - 512 evaluation : Initiating...\n",
      "--------------------------------------------------------\n",
      "In progress....\n",
      "\t Question: How many days has Amazon asked employees to come to work in office?\n",
      "\t Eval Result -> elapsed_time: 1.7180612087249756, faithfulness: False, relevancy:0.0, correctness: 2.0\n",
      "\t Response: \n",
      "Two days a week.\n",
      "\t Question: By what percentage did AWS revenue grow year-over-year in 2022?\n",
      "\t Eval Result -> elapsed_time: 1.6526360511779785, faithfulness: False, relevancy:1.0, correctness: 3.0\n",
      "\t Response: 29%\n",
      "\t Question: Compared to Graviton2 processors, what performance improvement did Graviton3 chips deliver according to the passage?\n",
      "\t Eval Result -> elapsed_time: 6.393612861633301, faithfulness: False, relevancy:1.0, correctness: 4.0\n",
      "\t Response: Graviton3 chips, which were introduced in 2022, delivered a 25% improvement in performance compared to Graviton2 processors.\n",
      "\t Question: Which was the first inference chip launched by AWS according to the passage?\n",
      "\t Eval Result -> elapsed_time: 1.8829200267791748, faithfulness: False, relevancy:1.0, correctness: 4.0\n",
      "\t Response: Inferentia is the first inference chip launched by AWS.\n",
      "\t Question: According to the context, in what year did Amazon's annual revenue increase from $245B to $434B?\n",
      "\t Eval Result -> elapsed_time: 1.845829963684082, faithfulness: False, relevancy:0.0, correctness: 2.0\n",
      "\t Response: 2021\n",
      "\t Evaluation result:  {'average_relevancy': '0.60', 'avg_response_time': '2.70', 'average_faithfulness': '0.00', 'avereage_correctness': '3.00'}\n",
      "Chunk size 512 - Average Relevancy: 0.60, Average Response time: 2.70, Average Faithfulness: 0.00,  Average Correctness: 3.00\n",
      "Done!!\n",
      "\n",
      "--------------------------------------------------------\n",
      "Chunk Size - 700 evaluation : Initiating...\n",
      "--------------------------------------------------------\n",
      "In progress....\n",
      "\t Question: How many days has Amazon asked employees to come to work in office?\n",
      "\t Eval Result -> elapsed_time: 2.213372230529785, faithfulness: False, relevancy:0.0, correctness: 1.0\n",
      "\t Response: \n",
      "Amazon has not asked employees to come to work in office.\n",
      "\t Question: By what percentage did AWS revenue grow year-over-year in 2022?\n",
      "\t Eval Result -> elapsed_time: 1.8575201034545898, faithfulness: False, relevancy:1.0, correctness: 3.0\n",
      "\t Response: 29%\n",
      "\t Question: Compared to Graviton2 processors, what performance improvement did Graviton3 chips deliver according to the passage?\n",
      "\t Eval Result -> elapsed_time: 3.444913864135742, faithfulness: False, relevancy:1.0, correctness: 3.0\n",
      "\t Response: Graviton3 chips, which were introduced in December 2020, offer a 25% improvement on top of Graviton2's relative gains.\n",
      "\t Question: Which was the first inference chip launched by AWS according to the passage?\n",
      "\t Eval Result -> elapsed_time: 1.8939430713653564, faithfulness: True, relevancy:1.0, correctness: 3.0\n",
      "\t Response: Inferentia\n",
      "\t Question: According to the context, in what year did Amazon's annual revenue increase from $245B to $434B?\n",
      "\t Eval Result -> elapsed_time: 2.0411930084228516, faithfulness: False, relevancy:0.0, correctness: 2.0\n",
      "\t Response: 2021\n",
      "\t Evaluation result:  {'average_relevancy': '0.60', 'avg_response_time': '2.29', 'average_faithfulness': '0.20', 'avereage_correctness': '2.40'}\n",
      "Chunk size 700 - Average Relevancy: 0.60, Average Response time: 2.29, Average Faithfulness: 0.20,  Average Correctness: 2.40\n",
      "Done!!\n",
      "\n",
      "CPU times: user 2.32 s, sys: 116 ms, total: 2.43 s\n",
      "Wall time: 9min 12s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kb_id</th>\n",
       "      <th>vector_index</th>\n",
       "      <th>Chunk Size</th>\n",
       "      <th>Average Relevancy</th>\n",
       "      <th>Average Response Time</th>\n",
       "      <th>Average Faithfulness</th>\n",
       "      <th>Average Correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MULPGDRDQZ</td>\n",
       "      <td>bedrock-sample-rag-index-699-300</td>\n",
       "      <td>300</td>\n",
       "      <td>0.60</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EYKWHFDT9R</td>\n",
       "      <td>bedrock-sample-rag-index-699-512</td>\n",
       "      <td>512</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z3FLLRDBYF</td>\n",
       "      <td>bedrock-sample-rag-index-699-700</td>\n",
       "      <td>700</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        kb_id                      vector_index  Chunk Size Average Relevancy  \\\n",
       "0  MULPGDRDQZ  bedrock-sample-rag-index-699-300         300              0.60   \n",
       "1  EYKWHFDT9R  bedrock-sample-rag-index-699-512         512              0.60   \n",
       "2  Z3FLLRDBYF  bedrock-sample-rag-index-699-700         700              0.60   \n",
       "\n",
       "  Average Response Time Average Faithfulness Average Correctness  \n",
       "0                  3.00                 0.20                2.60  \n",
       "1                  2.70                 0.00                3.00  \n",
       "2                  2.29                 0.20                2.40  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chunk_sizes = list(kb_oss_index_dict.keys())\n",
    "relevancy_threshold = 0.6\n",
    "data = []\n",
    "\n",
    "base_kb_name = f\"amazon-shareholder-letters-knowledge-base\"\n",
    "base_index_name = f\"bedrock-sample-index\"\n",
    "\n",
    "for chunk_size in chunk_sizes[0:3]:\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(f\"Chunk Size - {chunk_size} evaluation : Initiating...\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"In progress....\")\n",
    "    kb_id = kb_oss_index_dict[chunk_size]['kb_id']\n",
    "    vector_index = kb_oss_index_dict[chunk_size]['index_name']\n",
    "    \n",
    "    metrics_score= evaluate_chunk_size_kb(chunk_size, eval_question_answer_pair, kb_id, vector_index, relevancy_threshold)\n",
    "    print(f\"Chunk size {chunk_size} - Average Relevancy: {metrics_score['average_relevancy']}, Average Response time: {metrics_score['avg_response_time']}, Average Faithfulness: {metrics_score['average_faithfulness']},  Average Correctness: {metrics_score['avereage_correctness']}\")\n",
    "    data.append({\n",
    "            'kb_id':kb_id, \n",
    "            'vector_index': vector_index,  \n",
    "            'Chunk Size': chunk_size, \n",
    "            'Average Relevancy': metrics_score['average_relevancy'],\n",
    "            'Average Response Time': metrics_score['avg_response_time'],\n",
    "            'Average Faithfulness': metrics_score['average_faithfulness'],\n",
    "            'Average Correctness': metrics_score['avereage_correctness']} )\n",
    "    print(\"Done!!\\n\")\n",
    "    # detail_eval_df = metrics_score['detail_eval_df']\n",
    "    time.sleep(20)\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9a03dcf8e0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8wAAAIZCAYAAABzrLZGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABux0lEQVR4nO3de3zP9f//8fsYs9nJZAcztprNadiENn0QY0PLVAgZckihkHwoIj5Mnxw/X+VQQz5ZlBzKnEZGWGEOOZfjxGb5xLTJYdv794ef96f3x2sYtvdwu14u78ul1+v1fL6ej9e7Xm33PV8HG5PJZBIAAAAAALBQwtoFAAAAAABQHBGYAQAAAAAwQGAGAAAAAMAAgRkAAAAAAAMEZgAAAAAADBCYAQAAAAAwQGAGAAAAAMCArbULuBN5eXk6c+aMnJycZGNjY+1yAAAAgHtmMpn0xx9/qGLFiipRgnksoDh6IALzmTNn5OPjY+0yAAAAgPvu1KlTqlSpkrXLAGDggQjMTk5Okq7/z8TZ2dnK1QAAAAD37uLFi/Lx8TH/rgug+HkgAvONy7CdnZ0JzAAAAHiocMshUHxxswQAAAAAAAYIzAAAAAAAGCAwAwAAAABg4IG4hxkAAAB4VOXm5uratWvWLgN4aJQqVUolS5a8o7YEZgAAAKAYMplMSk9P14ULF6xdCvDQcXV1laen520fukdgBgAAAIqhG2HZ3d1dDg4OPE0buA9MJpMuXbqkjIwMSZKXl9ct2xOYAQAAgGImNzfXHJbLly9v7XKAh4q9vb0kKSMjQ+7u7re8PJuHfgEAgIfShAkTZGNjo4EDB96y3VdffaVq1aqpTJkyCgoK0sqVKy22d+/eXTY2NhafyMjIQqwckPmeZQcHBytXAjycbpxbt3s+AIEZAAA8dLZv365Zs2apdu3at2y3detWderUST179tSuXbsUHR2t6Oho7du3z6JdZGSk0tLSzJ8vvviiMMsHzLgMGygcd3puEZgBAMBDJSsrS126dNEnn3yicuXK3bLttGnTFBkZqbffflvVq1fX2LFjFRISounTp1u0s7Ozk6enp/lzu/0CAB4OBGYAAPBQ6devn9q0aaPw8PDbtk1OTr6pXUREhJKTky3WJSUlyd3dXYGBgXrttdf0n//8577WDACPqqSkJNnY2BTbp8Hz0C8AAPDQWLhwoXbu3Knt27ffUfv09HR5eHhYrPPw8FB6erp5OTIyUs8//7z8/Px09OhRvfPOO2rVqpWSk5Pv+D2ewP3kOyyhSMc7MaHNXfVLTk7W008/rcjISCUkFG3N1vDXS3ydnJwUGBioESNGqG3btlasyrq6d++uzz77LN/tVapU0c8//6y0tDS5uLgUYWV3jhlmAADwUDh16pTefPNNLViwQGXKlLlv+33ppZf03HPPKSgoSNHR0VqxYoW2b9+upKSk+zYG8DCKi4vTgAEDtGnTJp05c6ZQxzKZTMrJySnUMe7E3LlzlZaWph07dqhRo0Z68cUXtXfvXmuXZTXTpk2zeP6D9N/vKC0tTdu3b1fp0qXv6H3I1kJgBgAAD4WUlBRlZGQoJCREtra2srW11caNG/Wvf/1Ltra2ys3NvamPp6enzp49a7Hu7Nmz8vT0zHecxx9/XI899piOHDly348BeFhkZWVp0aJFeu2119SmTRvNmzfPvK1z587q2LGjRftr167pscce0/z58yVJeXl5io2NlZ+fn+zt7VWnTh0tXrzY3P7GZbyrVq1SvXr1ZGdnp82bN+vo0aNq27atPDw85OjoqPr162vdunUWY6WlpalNmzayt7eXn5+f4uPj5evrq6lTp5rbXLhwQb169VKFChXk7OysZs2aac+ePbc9bldXV3l6eiogIEBjx45VTk6ONmzYYN5+6tQpdejQQa6urnJzc1Pbtm114sQJi+Nq0KCBypYtK1dXVzVq1EgnT56UJI0ePVp169bVrFmz5OPjIwcHB3Xo0EGZmZnm/nl5eRozZowqVaokOzs71a1bV6tXrzZvP3HihGxsbLRkyRI988wzcnBwUJ06dSxuQzl58qSioqJUrlw5lS1bVjVr1rR4e8C+ffvUqlUrOTo6ysPDQ127dtW5c+cMvw8XFxeL5z/89Tvy9PRUhQoVbroke968eXJ1ddWKFSsUGBgoBwcHvfjii7p06ZI+++wz+fr6qly5cnrjjTcs/r9+5coVDRkyRN7e3ipbtqwaNmx4X/6wSWAGAAAPhebNm2vv3r3avXu3+fPkk0+qS5cu2r17t+Hl06GhoVq/fr3FusTERIWGhuY7zq+//qr//Oc/8vLyuu/HADwsvvzyS1WrVk2BgYF6+eWXNWfOHJlMJklSly5d9O233yorK8vcfs2aNbp06ZLatWsnSYqNjdX8+fM1c+ZM7d+/X4MGDdLLL7+sjRs3WowzbNgwTZgwQQcPHlTt2rWVlZWl1q1ba/369dq1a5ciIyMVFRWl1NRUc5+YmBidOXNGSUlJ+vrrrzV79mxlZGRY7Ld9+/bKyMjQqlWrlJKSopCQEDVv3ly///77HR1/Tk6O4uLiJEmlS5eWdP2PAhEREXJyctL333+vLVu2yNHRUZGRkbp69apycnIUHR2tJk2a6KefflJycrL69OljMfN65MgRffnll/r222+1evVq7dq1S6+//rp5+7Rp0zRp0iRNnDhRP/30kyIiIvTcc8/pl19+sajv3Xff1ZAhQ7R7924FBASoU6dO5hn6fv366cqVK9q0aZP27t2rDz74QI6OjpKu/yGhWbNmCg4O1o4dO7R69WqdPXtWHTp0uKPv5U5dunRJ//rXv7Rw4UKtXr1aSUlJateunVauXKmVK1fq3//+t2bNmmXxR5T+/fsrOTlZCxcu1E8//aT27dsrMjLypmMvMNMDIDMz0yTJlJmZae1SAADAA6RJkyamN99807zctWtX07Bhw8zLW7ZsMdna2pomTpxoOnjwoGnUqFGmUqVKmfbu3WsymUymP/74wzRkyBBTcnKy6fjx46Z169aZQkJCTFWrVjVdvny5qA8HD5lb/Y77559/mg4cOGD6888/b9pW5e8rivRzN8LCwkxTp041mUwm07Vr10yPPfaYacOGDRbL8+fPN7fv1KmTqWPHjiaTyWS6fPmyycHBwbR161aLffbs2dPUqVMnk8lkMm3YsMEkybRs2bLb1lKzZk3T//3f/5lMJpPp4MGDJkmm7du3m7f/8ssvJkmmKVOmmEwmk+n77783OTs733SOP/HEE6ZZs2blO44kU5kyZUxly5Y1lShRwiTJ5Ovra/rPf/5jMplMpn//+9+mwMBAU15enrnPlStXTPb29qY1a9aY/vOf/5gkmZKSkgz3P2rUKFPJkiVNv/76q3ndqlWrTCVKlDClpaWZTCaTqWLFiqZx48ZZ9Ktfv77p9ddfN5lMJtPx48dNkkyffvqpefv+/ftNkkwHDx40mUwmU1BQkGn06NGGNYwdO9bUsmVLi3WnTp0ySTIdPnw43+/mr9/R0qVLLdbd+Hd5/vx5k8lkMs2dO9ckyXTkyBFzm1dffdXk4OBg+uOPP8zrIiIiTK+++qrJZDKZTp48aSpZsqTp9OnTFvtu3ry5afjw4Ya13Ooc+ytmmAEAwCMjNTXVfB+dJIWFhSk+Pl6zZ882X/K5bNky1apVS5JUsmRJ/fTTT3ruuecUEBCgnj17ql69evr+++9lZ2dnrcMAirXDhw9r27Zt6tSpkyTJ1tZWHTt2NM+42traqkOHDlqwYIEkKTs7W8uXL1eXLl0kXZ9FvXTpklq0aCFHR0fzZ/78+Tp69KjFWE8++aTFclZWloYMGaLq1avL1dVVjo6OOnjwoHmG+fDhw7K1tVVISIi5j7+/v8Wr4vbs2aOsrCyVL1/eYvzjx4/fNP7/mjJlinbv3q1Vq1apRo0a+vTTT+Xm5mbe75EjR+Tk5GTep5ubmy5fvqyjR4/Kzc1N3bt3V0REhKKiosz3//5V5cqV5e3tbV4ODQ1VXl6eDh8+rIsXL+rMmTNq1KiRRZ9GjRrp4MGDFuv++o76G1fL3Jhlf+ONN/SPf/xDjRo10qhRo/TTTz9ZfDcbNmyw+F6qVasmSbf9bgrCwcFBTzzxhHnZw8NDvr6+5pnuG+tu1Lx3717l5uYqICDAoraNGzfec108JRsAADy0/vf+NaP72dq3b6/27dsb9re3t9eaNWsKoTLg4RUXF6ecnBxVrFjRvM5kMsnOzk7Tp0+Xi4uLunTpoiZNmigjI0OJiYmyt7dXZGSkJJkv1U5ISLAIh5Ju+kNV2bJlLZaHDBmixMRETZw4Uf7+/rK3t9eLL76oq1ev3nH9WVlZ8vLyMvz/haur6y37enp6yt/fX/7+/po7d65at26tAwcOyN3dXVlZWapXr575DwV/VaFCBUnXH4j1xhtvaPXq1Vq0aJFGjBihxMREPfXUU3dc/50oVaqU+Z9vXPKdl5cnSerVq5ciIiKUkJCgtWvXKjY2VpMmTdKAAQOUlZWlqKgoffDBBzft837epvLX+m7UaLTuRs1ZWVkqWbKkUlJSbrr95q8h+24QmAEAAADcFzk5OZo/f74mTZqkli1bWmyLjo7WF198ob59+yosLEw+Pj5atGiRVq1apfbt25sDUY0aNWRnZ6fU1FQ1adKkQONv2bJF3bt3N98LnZWVZfFQrcDAQOXk5GjXrl2qV6+epOsz2ufPnze3CQkJUXp6umxtbeXr63sX38J1DRo0UL169TRu3DhNmzZNISEhWrRokdzd3eXs7Jxvv+DgYAUHB2v48OEKDQ1VfHy8OTCnpqbqzJkz5j9G/PDDDypRooQCAwPl7OysihUrasuWLRbf25YtW9SgQYMC1e7j46O+ffuqb9++Gj58uD755BMNGDBAISEh+vrrr+Xr6ytb2+ITJYODg5Wbm6uMjAz97W9/u6/7Lj5HCQAAcA+K+t200t2/nxZ4WK1YsULnz59Xz549b3qv7gsvvKC4uDj17dtX0vWnZc+cOVM///yzxZOknZycNGTIEA0aNEh5eXl6+umnlZmZqS1btsjZ2VndunXLd/yqVatqyZIlioqKko2NjUaOHGmehZSkatWqKTw8XH369NGMGTNUqlQpvfXWW7K3tzfPtIaHhys0NFTR0dH65z//qYCAAJ05c0YJCQlq167dTZeB38rAgQPVrl07DR06VF26dNGHH36otm3bmp9kffLkSS1ZskRDhw7VtWvXNHv2bD333HOqWLGiDh8+rF9++UUxMTHm/ZUpU0bdunXTxIkTdfHiRb3xxhvq0KGD+QnUb7/9tkaNGqUnnnhCdevW1dy5c7V7927DWe1b1dyqVSsFBATo/Pnz2rBhg6pXry7p+gPBPvnkE3Xq1ElDhw6Vm5ubjhw5ooULF+rTTz+12rvpAwIC1KVLF8XExGjSpEkKDg7Wb7/9pvXr16t27dpq0+bu/1/NPcwAAAAA7ou4uDiFh4ffFJal64F5x44d5ntiu3TpogMHDsjb2/um+27Hjh2rkSNHKjY2VtWrV1dkZKQSEhLk5+d3y/EnT56scuXKKSwsTFFRUYqIiLC4X1mS5s+fLw8PDzVu3Fjt2rVT79695eTkZH5/u42NjVauXKnGjRurR48eCggI0EsvvaSTJ0/Kw8OjQN9HZGSk/Pz8NG7cODk4OGjTpk2qXLmynn/+eVWvXl09e/bU5cuX5ezsLAcHBx06dEgvvPCCAgIC1KdPH/Xr10+vvvqqeX/+/v56/vnn1bp1a7Vs2VK1a9fWxx9/bN7+xhtvaPDgwXrrrbcUFBSk1atX65tvvlHVqlXvuObc3Fz169fP/L0HBASYx7gxg52bm6uWLVsqKChIAwcOlKurq0qUsG60nDt3rmJiYvTWW28pMDBQ0dHR2r59uypXrnxP+7X5/08ruysTJkzQ8OHD9eabb1q8t+x/ffXVVxo5cqROnDihqlWr6oMPPlDr1q3veJyLFy/KxcVFmZmZt7x8AQAAPLqYYcaD5la/416+fFnHjx+Xn5+fOcihcPz666/y8fHRunXr1Lx5c2uXk6/Ro0dr2bJl2r17t7VLeSjc6Tl2138G2L59u2bNmmXxhDUjW7duVadOndSzZ0/t2rVL0dHRio6O1r59++52aAAAAAC4K999952++eYbHT9+XFu3btVLL70kX19fNW7c2NqloRi6q8CclZWlLl266JNPPrF4BLuRadOmKTIyUm+//baqV6+usWPHKiQkRNOnT7+rggEAAADgbl27dk3vvPOOatasqXbt2qlChQpKSkq66SnMgHSXgblfv35q06aNwsPDb9s2OTn5pnYRERFKTk7Ot8+VK1d08eJFiw8AAAAA3KuIiAjt27dPly5d0tmzZ7V06VJVqVLF2mXd1ujRo7kc2woK/JTshQsXaufOndq+ffsdtU9PT7/p5ngPDw+lp6fn2yc2Nlbvv/9+QUsDAAAAAOC+KdAM86lTp/Tmm29qwYIFhfrwgeHDhyszM9P8OXXqVKGNBQAAAACAkQLNMKekpCgjI8Pi0ey5ubnatGmTpk+fritXrtz07i1PT0+dPXvWYt3Zs2fN7wozYmdnJzs7u4KUBgAAAADAfVWgGebmzZtr79692r17t/nz5JNPqkuXLtq9e7fhi6pDQ0O1fv16i3WJiYkKDQ29t8oBAAAAAChEBZphdnJyUq1atSzWlS1bVuXLlzevj4mJkbe3t2JjYyVJb775ppo0aaJJkyapTZs2WrhwoXbs2KHZs2ffp0MAAAAAAOD+u+v3MOcnNTVVaWlp5uWwsDDFx8dr9uzZqlOnjhYvXqxly5bdFLwBAAAAAChO7jkwJyUlaerUqRbL8+bNs2jTvn17HT58WFeuXNG+ffvUunXrex0WAAAAAB4KTZs21cCBA2/ZZt68eXJ1db3tvkaPHi0PDw/Z2Nho2bJldzS+r6+vRabDfxX4tVIAAAAArGi0SxGPl3lX3ZKTk/X0008rMjJSCQkJ97mo4sfGxuamdY0aNdLmzZtv23fJkiUqVaqUednX11cDBw68bYj+XwcPHtT777+vpUuX6qmnnlK5cuUK1B83IzADAAAAuO/i4uI0YMAAxcXF6cyZM6pYsWKhjWUymZSbmytbW+vGm7lz5yoyMtK8XLp06Tvq5+bmdl/GP3r0qCSpbdu2hgEeBXff72EGAAAA8GjLysrSokWL9Nprr6lNmzYWt2x27txZHTt2tGh/7do1PfbYY5o/f74kKS8vT7GxsfLz85O9vb35WUg3JCUlycbGRqtWrVK9evVkZ2enzZs36+jRo2rbtq08PDzk6Oio+vXra926dRZjpaWlqU2bNrK3t5efn5/i4+NvuiT5woUL6tWrlypUqCBnZ2c1a9ZMe/bsue1xu7q6ytPT0/xxc3PTf/7zH3Xq1Ene3t5ycHBQUFCQvvjiC4t+f70ku2nTpjp58qQGDRokGxubm4LvmjVrVL16dTk6OioyMtL8/KjRo0crKipKklSiRAlzP6PLvaOjo9W9e/d8j8PGxkaffvqp2rVrJwcHB1WtWlXffPONRZt9+/apVatWcnR0lIeHh7p27apz586Zty9evFhBQUGyt7dX+fLlFR4eruzsbEnX//01aNBAZcuWlaurqxo1aqSTJ0/e9vu1BgIzAAAAgPvqyy+/VLVq1RQYGKiXX35Zc+bMkclkkiR16dJF3377rbKysszt16xZo0uXLqldu3aSpNjYWM2fP18zZ87U/v37NWjQIL388svauHGjxTjDhg3ThAkTdPDgQdWuXVtZWVlq3bq11q9fr127dikyMlJRUVFKTU0194mJidGZM2eUlJSkr7/+WrNnz1ZGRobFftu3b6+MjAytWrVKKSkpCgkJUfPmzfX7778X+Lu4fPmy6tWrp4SEBO3bt099+vRR165dtW3bNsP2S5YsUaVKlTRmzBilpaVZPFD50qVLmjhxov79739r06ZNSk1N1ZAhQyRJQ4YM0dy5cyXppn534/3331eHDh30008/qXXr1urSpYv5+C9cuKBmzZopODhYO3bs0OrVq3X27Fl16NDBPH6nTp30yiuv6ODBg0pKStLzzz8vk8mknJwcRUdHq0mTJvrpp5+UnJysPn36FNsZcS7JBgAAAHBfxcXF6eWXX5YkRUZGKjMzUxs3blTTpk0VERGhsmXLaunSperataskKT4+Xs8995ycnJx05coVjR8/XuvWrVNoaKgk6fHHH9fmzZs1a9YsNWnSxDzOmDFj1KJFC/Oym5ub6tSpY14eO3asli5dqm+++Ub9+/fXoUOHtG7dOm3fvl1PPvmkJOnTTz9V1apVzX02b96sbdu2KSMjQ3Z2dpKkiRMnatmyZVq8eLH69OmT73F36tRJJUuWNC9//vnnio6ONodaSRowYIDWrFmjL7/8Ug0aNLhpH25ubipZsqScnJzk6elpse3atWuaOXOmnnjiCUlS//79NWbMGEmSo6Oj+aFg/9vvbnTv3l2dOnWSJI0fP17/+te/tG3bNkVGRmr69OkKDg7W+PHjze3nzJkjHx8f/fzzz8rKylJOTo6ef/55ValSRZIUFBQkSfr999+VmZmpZ5991nwc1atXv+d6CwuBGQAAAMB9c/jwYW3btk1Lly6VJNna2qpjx46Ki4tT06ZNZWtrqw4dOmjBggXq2rWrsrOztXz5ci1cuFCSdOTIEV26dMkiCEvS1atXFRwcbLHuRui9ISsrS6NHj1ZCQoLS0tKUk5OjP//80zzDfPjwYdna2iokJMTcx9/f3+LhWHv27FFWVpbKly9vse8///zTfI9wfqZMmaLw8HDzspeXl3JzczV+/Hh9+eWXOn36tK5evaorV67IwcHhlvsy4uDgYA6ZN/b/v7Pj90vt2rXN/1y2bFk5Ozubx9qzZ482bNggR0fHm/odPXpULVu2VPPmzRUUFKSIiAi1bNlSL774osqVKyc3Nzd1795dERERatGihcLDw9WhQwd5eXkVynHcKwIzAAAAgPsmLi5OOTk5Fg/5MplMsrOz0/Tp0+Xi4qIuXbqoSZMmysjIUGJiouzt7c0Py7pxqXZCQoK8vb0t9n1jxveGsmXLWiwPGTJEiYmJmjhxovz9/WVvb68XX3xRV69eveP6s7Ky5OXlpaSkpJu23e61Tp6envL397dYN2HCBE2bNk1Tp05VUFCQypYtq4EDBxaophv++iRt6fq9xjcudc9PiRIlbmpz7dq1uxorLy9P0vXvKCoqSh988MFN/by8vFSyZEklJiZq69atWrt2rf7v//5P7777rn788Uf5+flp7ty5euONN7R69WotWrRII0aMUGJiop566qnb1lXUCMwAAAAA7oucnBzNnz9fkyZNUsuWLS22RUdH64svvlDfvn0VFhYmHx8fLVq0SKtWrVL79u3NAa1GjRqys7NTamqqxeXXd2LLli3q3r27+V7orKwsnThxwrw9MDBQOTk52rVrl+rVqyfp+oz2+fPnzW1CQkKUnp4uW1tb+fr63sW3cHNNbdu2NV+inpeXp59//lk1atTIt0/p0qWVm5t7z2NLUoUKFSzuZ87NzdW+ffv0zDPP3PU+Q0JC9PXXX8vX1zffJ5Pb2NioUaNGatSokd577z1VqVJFS5cu1eDBgyVJwcHBCg4O1vDhwxUaGqr4+PhiGZh56BcAAACA+2LFihU6f/68evbsqVq1all8XnjhBcXFxZnbdu7cWTNnzlRiYqK6dOliXu/k5KQhQ4Zo0KBB+uyzz3T06FHt3LlT//d//6fPPvvsluNXrVpVS5Ys0e7du7Vnzx517tzZPCsqSdWqVVN4eLj69Omjbdu2adeuXerTp4/s7e3ND50KDw9XaGiooqOjtXbtWp04cUJbt27Vu+++qx07dhT4O6latap5tvXgwYN69dVXdfbs2Vv28fX11aZNm3T69GmLJ0/fjWbNmikhIUEJCQk6dOiQXnvtNV24cOGe9tmvXz/9/vvv6tSpk7Zv366jR49qzZo16tGjh3Jzc/Xjjz9q/Pjx2rFjh1JTU7VkyRL99ttvql69uo4fP67hw4crOTlZJ0+e1Nq1a/XLL78U2/uYCcwAAAAA7ou4uDiFh4fLxcXlpm0vvPCCduzYoZ9++knS9adlHzhwQN7e3mrUqJFF27Fjx2rkyJGKjY1V9erVFRkZqYSEBPn5+d1y/MmTJ6tcuXIKCwtTVFSUIiIiLO5XlqT58+fLw8NDjRs3Vrt27dS7d285OTmpTJkykq7PjK5cuVKNGzdWjx49FBAQoJdeekknT56Uh4dHgb+TESNGKCQkRBEREWratKk8PT0VHR19yz5jxozRiRMn9MQTT6hChQoFHvOvXnnlFXXr1k0xMTFq0qSJHn/88XuaXZakihUrasuWLcrNzVXLli0VFBSkgQMHytXVVSVKlJCzs7M2bdqk1q1bKyAgQCNGjNCkSZPUqlUrOTg46NChQ3rhhRcUEBCgPn36qF+/fnr11VfvqabCYmO63UXvxcDFixfl4uKizMxMOTs7W7scAABQDPkOSyjyMU9MaFPkY+LhcavfcS9fvqzjx4/Lz8/PHORQOH799Vf5+Pho3bp1at68ubXLQRG503OMe5gBAAAAPDK+++47ZWVlKSgoSGlpaRo6dKh8fX3VuHFja5eGYojADAAAAOCRce3aNb3zzjs6duyYnJycFBYWpgULFtz0VGhAIjADAAAAeIREREQoIiLC2mXgAcFDvwAAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAIqB7t27Kzo62tpl4C94DzMAAADwAAn6LKhIx9vbbe9d9UtOTtbTTz+tyMhIJSQk3Oeqih8bGxvzPzs5OSkwMFAjRoxQ27ZtrVgV7hUzzAAAAADuu7i4OA0YMECbNm3SmTNnCnUsk8mknJycQh3jTsydO1dpaWnasWOHGjVqpBdffFF7997dHxxQPBCYAQAAANxXWVlZWrRokV577TW1adNG8+bNM2/r3LmzOnbsaNH+2rVreuyxxzR//nxJUl5enmJjY+Xn5yd7e3vVqVNHixcvNrdPSkqSjY2NVq1apXr16snOzk6bN2/W0aNH1bZtW3l4eMjR0VH169fXunXrLMZKS0tTmzZtZG9vLz8/P8XHx8vX11dTp041t7lw4YJ69eqlChUqyNnZWc2aNdOePXtue9yurq7y9PRUQECAxo4dq5ycHG3YsMG8/dSpU+rQoYNcXV3l5uamtm3b6sSJE/nu71bfQ15enipVqqQZM2ZY9Nm1a5dKlCihkydPSpImT56soKAglS1bVj4+Pnr99deVlZVlbj9v3jy5urpqzZo1ql69uhwdHRUZGam0tDSL/c6ZM0c1a9aUnZ2dvLy81L9/f0nSK6+8omeffdai7bVr1+Tu7q64uLjbfmfFHYEZAAAAwH315Zdfqlq1agoMDNTLL7+sOXPmyGQySZK6dOmib7/91iK0rVmzRpcuXVK7du0kSbGxsZo/f75mzpyp/fv3a9CgQXr55Ze1ceNGi3GGDRumCRMm6ODBg6pdu7aysrLUunVrrV+/Xrt27VJkZKSioqKUmppq7hMTE6MzZ84oKSlJX3/9tWbPnq2MjAyL/bZv314ZGRlatWqVUlJSFBISoubNm+v333+/o+PPyckxh8XSpUtLuh4iIyIi5OTkpO+//15btmwxh9OrV68a7udW30OJEiXUqVMnxcfHW/RZsGCBGjVqpCpVqkiSSpQooX/961/av3+/PvvsM3333XcaOnSoRZ9Lly5p4sSJ+ve//61NmzYpNTVVQ4YMMW+fMWOG+vXrpz59+mjv3r365ptv5O/vL0nq1auXVq9ebRGwV6xYoUuXLt30h5EHkY3pxn+5xdjFixfl4uKizMxMOTs7W7scAABQDPkOK/p7JE9MaFPkY+LhcavfcS9fvqzjx4/Lz89PZcqUsdj2INzD3KhRI3Xo0EFvvvmmcnJy5OXlpa+++kpNmzY1L0+ePFldu3aVdH3WOS8vTwsXLtSVK1fk5uamdevWKTQ01LzPXr166dKlS4qPj1dSUpKeeeYZLVu27Lb3CNeqVUt9+/ZV//79dejQIVWvXl3bt2/Xk08+KUk6cuSIqlatqilTpmjgwIHavHmz2rRpo4yMDNnZ2Zn34+/vr6FDh6pPnz6G49jY2KhMmTIqWbKk/vzzT+Xl5cnX11cpKSlyc3PT559/rn/84x86ePCg+X7nq1evytXVVcuWLVPLli3VvXt3XbhwQcuWLbuj72H37t0KCQnRiRMnVLlyZeXl5aly5coaMWKE+vbta1jn4sWL1bdvX507d07S9RnmHj166MiRI3riiSckSR9//LHGjBmj9PR0SZK3t7d69Oihf/zjH4b7rFmzprp162YO4s8995zKly+vuXPn3vLfjTXd6hz7K2aYAQAAANw3hw8f1rZt29SpUydJkq2trTp27GiecbW1tVWHDh20YMECSVJ2draWL1+uLl26SLoeYC9duqQWLVrI0dHR/Jk/f76OHj1qMdaN0HtDVlaWhgwZourVq8vV1VWOjo46ePCgeYb58OHDsrW1VUhIiLmPv7+/ypUrZ17es2ePsrKyVL58eYvxjx8/ftP4/2vKlCnavXu3Vq1apRo1aujTTz+Vm5ubeb9HjhyRk5OTeZ9ubm66fPmy4X7v5HuoW7euqlevbp5l3rhxozIyMtS+fXvzftatW6fmzZvL29tbTk5O6tq1q/7zn//o0qVL5jYODg7msCxJXl5e5ln3jIwMnTlzRs2bN8/3uHv16mUOx2fPntWqVav0yiuv3PK7elDwlGwAAAAA901cXJxycnJUsWJF8zqTySQ7OztNnz5dLi4u6tKli5o0aaKMjAwlJibK3t5ekZGRkmS+VDshIUHe3t4W+/7rjK8klS1b1mJ5yJAhSkxM1MSJE+Xv7y97e3u9+OKL+V7ybCQrK0teXl5KSkq6aZurq+st+3p6esrf31/+/v6aO3euWrdurQMHDsjd3V1ZWVmqV6+e+Q8Ff1WhQgXDOqTbfw9dunRRfHy8hg0bpvj4eEVGRqp8+fKSpBMnTujZZ5/Va6+9pnHjxsnNzU2bN29Wz549dfXqVTk4OEiSSpUqZbF/Gxsb8yX09vb2tzxm6fpl7sOGDVNycrK2bt0qPz8//e1vf7ttvwcBgRkAAADAfZGTk6P58+dr0qRJatmypcW26OhoffHFF+rbt6/CwsLk4+OjRYsWadWqVWrfvr05tNWoUUN2dnZKTU1VkyZNCjT+li1b1L17d/O90FlZWRYP1QoMDFROTo527dqlevXqSbo+k3v+/Hlzm5CQEKWnp8vW1la+vr538S1c16BBA9WrV0/jxo3TtGnTFBISokWLFsnd3f2ObjO90++hc+fOGjFihFJSUrR48WLNnDnTvC0lJUV5eXmaNGmSSpS4fnHxl19+WaDjcHJykq+vr9avX69nnnnGsE358uUVHR2tuXPnKjk5WT169CjQGMUZgRkAAADAfbFixQqdP39ePXv2lIuLi8W2F154QXFxceZ7azt37qyZM2fq559/tniStJOTk4YMGaJBgwYpLy9PTz/9tDIzM7VlyxY5OzurW7du+Y5ftWpVLVmyRFFRUbKxsdHIkSOVl5dn3l6tWjWFh4erT58+mjFjhkqVKqW33npL9vb25vuKw8PDFRoaqujoaP3zn/9UQECAzpw5o4SEBLVr1+6my8BvZeDAgWrXrp2GDh2qLl266MMPP1Tbtm01ZswYVapUSSdPntSSJUs0dOhQVapUyaLvnX4Pvr6+CgsLU8+ePZWbm6vnnnvOvA9/f39du3ZN//d//6eoqCht2bLFIlDfqdGjR6tv375yd3dXq1at9Mcff2jLli0aMGCAuU2vXr307LPPKjc395b/jh403MMMAAAA4L6Ii4tTeHj4TWFZuh6Yd+zYoZ9++knS9UuJDxw4IG9vbzVq1Mii7dixYzVy5EjFxsaqevXqioyMVEJCgvz8/G45/uTJk1WuXDmFhYUpKipKERERFvcrS9L8+fPl4eGhxo0bq127durdu7ecnJzMD36ysbHRypUr1bhxY/Xo0UMBAQF66aWXdPLkSXl4eBTo+4iMjJSfn5/GjRsnBwcHbdq0SZUrV9bzzz+v6tWrq2fPnrp8+XK+M853+j106dJFe/bsUbt27Swuoa5Tp44mT56sDz74QLVq1dKCBQsUGxtboGOQpG7dumnq1Kn6+OOPVbNmTT377LP65ZdfLNqEh4fLy8tLERERFpfjP+h4SjYAAHgo8JRsPGju9inZuL9+/fVX+fj4mB+OhbuTlZUlb29vzZ07V88//7y1y7mtOz3HuCQbAAAAwCPju+++U1ZWloKCgpSWlqahQ4fK19dXjRs3tnZpD6S8vDydO3dOkyZNkqurq8Ul4Q8DAjMAAACAR8a1a9f0zjvv6NixY3JyclJYWJgWLFhw05OicWdSU1Pl5+enSpUqad68ebK1fbgi5sN1NAAAAABwCxEREYqIiLB2GQ8NX19fPQB3+d41HvoFAAAAAIABAjMAAAAAAAYIzAAAAAAAGCAwAwAAAABggMAMAAAAAIABAjMAAAAAAAYIzAAAAAAAGOA9zAAAAMAD5GC16kU6XvVDB++qX3Jysp5++mlFRkYqISHhPldVPG3YsEEffvihfvzxR/3555/y9fVVq1atNHjwYHl7e1u7vJs0bdpUdevW1dSpU61dSrFVoBnmGTNmqHbt2nJ2dpazs7NCQ0O1atWqfNvPmzdPNjY2Fp8yZcrcc9EAAAAAire4uDgNGDBAmzZt0pkzZwp1LJPJpJycnEId43ZmzZql8PBweXp66uuvv9aBAwc0c+ZMZWZmatKkSXe936tXr960Ljc3V3l5efdSLu5QgQJzpUqVNGHCBKWkpGjHjh1q1qyZ2rZtq/379+fbx9nZWWlpaebPyZMn77loAAAAAMVXVlaWFi1apNdee01t2rTRvHnzzNs6d+6sjh07WrS/du2aHnvsMc2fP1+SlJeXp9jYWPn5+cne3l516tTR4sWLze2TkpJkY2OjVatWqV69erKzs9PmzZt19OhRtW3bVh4eHnJ0dFT9+vW1bt06i7HS0tLUpk0b2dvby8/PT/Hx8fL19bWYZb1w4YJ69eqlChUqyNnZWc2aNdOePXvyPd5ff/1Vb7zxht544w3NmTNHTZs2la+vrxo3bqxPP/1U7733nrnt119/rZo1a8rOzk6+vr43hWlfX1+NHTtWMTExcnZ2Vp8+fTRv3jy5urrqm2++UY0aNWRnZ6fU1FRduXJFQ4YMkbe3t8qWLauGDRsqKSnJYn9btmxR06ZN5eDgoHLlyikiIkLnz59X9+7dtXHjRk2bNs08uXnixAnzd7t+/Xo9+eSTcnBwUFhYmA4fPmyx3+XLlyskJERlypTR448/rvfff9/8RwuTyaTRo0ercuXKsrOzU8WKFfXGG2+Y+3788ceqWrWqypQpIw8PD7344ov5frfWVqDAHBUVpdatW6tq1aoKCAjQuHHj5OjoqB9++CHfPjY2NvL09DR/PDw87rloAAAAAMXXl19+qWrVqikwMFAvv/yy5syZI5PJJEnq0qWLvv32W2VlZZnbr1mzRpcuXVK7du0kSbGxsZo/f75mzpyp/fv3a9CgQXr55Ze1ceNGi3GGDRumCRMm6ODBg6pdu7aysrLUunVrrV+/Xrt27VJkZKSioqKUmppq7hMTE6MzZ84oKSlJX3/9tWbPnq2MjAyL/bZv314ZGRlatWqVUlJSFBISoubNm+v33383PN6vvvpKV69e1dChQw23u7q6SpJSUlLUoUMHvfTSS9q7d69Gjx6tkSNHWvxBQZImTpyoOnXqaNeuXRo5cqQk6dKlS/rggw/06aefav/+/XJ3d1f//v2VnJyshQsX6qefflL79u0VGRmpX375RZK0e/duNW/eXDVq1FBycrI2b96sqKgo5ebmatq0aQoNDVXv3r3Nk5s+Pj7mGt59911NmjRJO3bskK2trV555RXztu+//14xMTF68803deDAAc2aNUvz5s3TuHHjJF3/o8CUKVM0a9Ys/fLLL1q2bJmCgoIkSTt27NAbb7yhMWPG6PDhw1q9erUaN25s+L0VB3d9D3Nubq6++uorZWdnKzQ0NN92WVlZqlKlivLy8hQSEqLx48erZs2at9z3lStXdOXKFfPyxYsX77ZMAAAAAEUsLi5OL7/8siQpMjJSmZmZ2rhxo5o2baqIiAiVLVtWS5cuVdeuXSVJ8fHxeu655+Tk5KQrV65o/PjxWrdunTlnPP7449q8ebNmzZqlJk2amMcZM2aMWrRoYV52c3NTnTp1zMtjx47V0qVL9c0336h///46dOiQ1q1bp+3bt+vJJ5+UJH366aeqWrWquc/mzZu1bds2ZWRkyM7OTtL1ALts2TItXrxYffr0uel4f/nlFzk7O8vLy+uW38vkyZPVvHlzcwgOCAjQgQMH9OGHH6p79+7mds2aNdNbb71lXv7+++917do1ffzxx+bjS01N1dy5c5WamqqKFStKkoYMGaLVq1dr7ty5Gj9+vP75z3/qySef1Mcff2ze11+zWOnSpeXg4CBPT8+bah03bpz5ux42bJjatGmjy5cvq0yZMnr//fc1bNgwdevWTdL1fz9jx47V0KFDNWrUKKWmpsrT01Ph4eEqVaqUKleurAYNGpjrLlu2rJ599lk5OTmpSpUqCg4OvuX3Zk0Ffkr23r175ejoKDs7O/Xt21dLly5VjRo1DNsGBgZqzpw5Wr58uT7//HPl5eUpLCxMv/766y3HiI2NlYuLi/nz1790AAAAACi+Dh8+rG3btqlTp06SJFtbW3Xs2FFxcXHm5Q4dOmjBggWSpOzsbC1fvlxdunSRJB05ckSXLl1SixYt5OjoaP7Mnz9fR48etRjrRui9ISsrS0OGDFH16tXl6uoqR0dHHTx40DzDfPjwYdna2iokJMTcx9/fX+XKlTMv79mzR1lZWSpfvrzF+MePH79p/BtMJpNsbGxu+90cPHhQjRo1sljXqFEj/fLLL8rNzc33uKTr4bZ27drm5b179yo3N1cBAQEWdW7cuNFc540Z5rvx17Fu/CHgxkz8nj17NGbMGItxb8xUX7p0Se3bt9eff/6pxx9/XL1799bSpUvNl2u3aNFCVapU0eOPP66uXbtqwYIFunTp0l3VWBQKPMMcGBio3bt3KzMzU4sXL1a3bt20ceNGw9AcGhpqMfscFham6tWra9asWRo7dmy+YwwfPlyDBw82L1+8eJHQDAAAADwA4uLilJOTY571lK4HSjs7O02fPl0uLi7q0qWLmjRpooyMDCUmJsre3l6RkZGSZL5UOyEh4aYnS9+Y8b2hbNmyFstDhgxRYmKiJk6cKH9/f9nb2+vFF180fHBWfrKysuTl5XXTvcDSfy+t/l8BAQHKzMxUWlrabWeZ78T/Hpck2dvbW4TyrKwslSxZUikpKSpZsqRFW0dHR3Ofu1WqVCnzP98Y98aDxrKysvT+++/r+eefv6lfmTJl5OPjo8OHD2vdunVKTEzU66+/rg8//FAbN26Uk5OTdu7cqaSkJK1du1bvvfeeRo8ere3bt+f7/VpTgQNz6dKl5e/vL0mqV6+etm/frmnTpmnWrFm37VuqVCkFBwfryJEjt2xnZ2d308kAAAAAoHjLycnR/PnzNWnSJLVs2dJiW3R0tL744gv17dtXYWFh8vHx0aJFi7Rq1Sq1b9/eHND++lCrv15+fSe2bNmi7t27m++FzsrK0okTJ8zbAwMDlZOTo127dqlevXqSrs9onz9/3twmJCRE6enpsrW1la+v7x2N++KLL2rYsGH65z//qSlTpty0/cKFC3J1dVX16tW1ZcuWm2oOCAi4KfTeTnBwsHJzc5WRkaG//e1vhm1q166t9evX6/333zfcXrp0aYuZ7TsVEhKiw4cPm3OhEXt7e0VFRSkqKkr9+vVTtWrVtHfvXoWEhMjW1lbh4eEKDw/XqFGj5Orqqu+++84wgFvbPb+HOS8vz+J+41vJzc3V3r171bp163sdFgAAAEAxs2LFCp0/f149e/aUi4uLxbYXXnhBcXFx6tu3r6TrT8ueOXOmfv75Z23YsMHczsnJSUOGDNGgQYOUl5enp59+WpmZmdqyZYucnZ3N980aqVq1qpYsWaKoqCjZ2Nho5MiRFq9fqlatmsLDw9WnTx/NmDFDpUqV0ltvvWUxexseHq7Q0FBFR0frn//8pwICAnTmzBklJCSoXbt2hpdL+/j4aMqUKerfv78uXryomJgY+fr66tdff9X8+fPl6OioSZMm6a233lL9+vU1duxYdezYUcnJyZo+fbrFPcZ3KiAgQF26dFFMTIwmTZqk4OBg/fbbb1q/fr1q166tNm3aaPjw4QoKCtLrr7+uvn37qnTp0tqwYYPat2+vxx57TL6+vvrxxx914sQJOTo6ys3N7Y7Gfu+99/Tss8+qcuXKevHFF1WiRAnt2bNH+/bt0z/+8Q/NmzdPubm5atiwoRwcHPT555/L3t5eVapU0YoVK3Ts2DE1btxY5cqV08qVK5WXl6fAwMACfwdFoUD3MA8fPlybNm3SiRMntHfvXg0fPlxJSUnm+w1iYmI0fPhwc/sxY8Zo7dq1OnbsmHbu3KmXX35ZJ0+eVK9eve7vUQAAAACwuri4OIWHh98UlqXrgXnHjh366aefJF1/WvaBAwfk7e190329Y8eO1ciRIxUbG6vq1asrMjJSCQkJ8vPzu+X4kydPVrly5RQWFqaoqChFRERY3K8sSfPnz5eHh4caN26sdu3aqXfv3nJyclKZMmUkXb/8eOXKlWrcuLF69OihgIAAvfTSSzp58uQt3/jz+uuva+3atTp9+rTatWunatWqqVevXnJ2dtaQIUMkXZ+Z/fLLL7Vw4ULVqlVL7733nsaMGWPxwK+CmDt3rmJiYvTWW28pMDBQ0dHR2r59uypXrizpeqheu3at9uzZowYNGig0NFTLly+Xre31edMhQ4aoZMmSqlGjhipUqGDxNPFbiYiI0IoVK7R27VrVr19fTz31lKZMmaIqVapIun7p+ieffKJGjRqpdu3aWrdunb799luVL19erq6uWrJkiZo1a6bq1atr5syZ+uKLL277YGhrsTHdeL77HejZs6fWr1+vtLQ0ubi4qHbt2vr73/9ufjLdjfeN3Xgs+qBBg7RkyRKlp6erXLlyqlevnv7xj38U+CloFy9elIuLizIzM+Xs7FygvgAA4NHgOyyhyMc8MaFNkY+Jh8etfse9fPmyjh8/Lj8/P3OQQ+H49ddf5ePjo3Xr1t31A7Lw4LnTc6xAl2TfeLJdfv73xvgpU6YYXsMPAAAAANbw3XffKSsrS0FBQUpLS9PQoUPl6+tbrN8FDOu553uYAQAAAOBBce3aNb3zzjs6duyYnJycFBYWpgULFlg8FRq4gcAMAAAA4JERERGhiIgIa5eBB0SBHvoFAAAAAMCjgsAMAAAAFFMFeD4vgAK403OLwAwAAAAUMzfup7106ZKVKwEeTjfOrdvdu849zAAAAEAxU7JkSbm6uiojI0OS5ODgIBsbGytXBTz4TCaTLl26pIyMDLm6uqpkyZK3bE9gBgAAAIohT09PSTKHZgD3j6urq/kcuxUCMwAAAFAM2djYyMvLS+7u7rp27Zq1ywEeGqVKlbrtzPINBGYAAACgGCtZsuQd/3IP4P7ioV8AAAAAABggMANAAcyYMUO1a9eWs7OznJ2dFRoaqlWrVuXb/pNPPtHf/vY3lStXTuXKlVN4eLi2bdtm0Wb06NGqVq2aypYta27z448/FvahAAAA4DYIzABQAJUqVdKECROUkpKiHTt2qFmzZmrbtq32799v2D4pKUmdOnXShg0blJycLB8fH7Vs2VKnT582twkICND06dO1d+9ebd68Wb6+vmrZsqV+++23ojosAAAAGLAxPQBvQ7948aJcXFyUmZkpZ2dna5cDABbc3Nz04YcfqmfPnrdtm5ubq3Llymn69OmKiYkxbHPj/3nr1q1T8+bN73e5wEPLd1hCkY95YkKbIh8TDw9+xwWKPx76BQB3KTc3V1999ZWys7MVGhp6R30uXbqka9euyc3NzXD71atXNXv2bLm4uKhOnTr3s1wAAAAUEIEZAApo7969Cg0N1eXLl+Xo6KilS5eqRo0ad9T373//uypWrKjw8HCL9StWrNBLL72kS5cuycvLS4mJiXrssccKo3wAAADcIe5hBoACCgwM1O7du/Xjjz/qtddeU7du3XTgwIHb9pswYYIWLlyopUuXqkyZMhbbnnnmGe3evVtbt25VZGSkOnTooIyMjMI6BAAAANwBAjMAFFDp0qXl7++vevXqKTY2VnXq1NG0adNu2WfixImaMGGC1q5dq9q1a9+0vWzZsvL399dTTz2luLg42draKi4urrAOAQAAAHeAS7IB4B7l5eXpypUr+W7/5z//qXHjxmnNmjV68skn78s+AQAAUPgIzABQAMOHD1erVq1UuXJl/fHHH4qPj1dSUpLWrFkjSYqJiZG3t7diY2MlSR988IHee+89xcfHy9fXV+np6ZIkR0dHOTo6Kjs7W+PGjdNzzz0nLy8vnTt3Th999JFOnz6t9u3bW+04AQAAQGAGgALJyMhQTEyM0tLS5OLiotq1a2vNmjVq0aKFJCk1NVUlSvz3bpcZM2bo6tWrevHFFy32M2rUKI0ePVolS5bUoUOH9Nlnn+ncuXMqX7686tevr++//141a9Ys0mMDAACAJd7DDAAAHgq8hxkPGn7HBYo/HvoFAAAAAIABLskGgDvE7BUAAMCjhRlmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmwMCMGTNUu3ZtOTs7y9nZWaGhoVq1apW1ywLw/3GOAgCAokBgBgxUqlRJEyZMUEpKinbs2KFmzZqpbdu22r9/v7VLAyDOUQAAUDRsrV0AUBxFRUVZLI8bN04zZszQDz/8oJo1a1qpKgA3cI4CAICiQGAGbiM3N1dfffWVsrOzFRoaau1yAPwPzlEAAFBYCMxAPvbu3avQ0FBdvnxZjo6OWrp0qWrUqGHtsgD8f5yjAACgsHEPM5CPwMBA7d69Wz/++KNee+01devWTQcOHLB2WQD+P85RAABQ2JhhBvJRunRp+fv7S5Lq1aun7du3a9q0aZo1a5aVKwMgcY4CAIDCxwwzcIfy8vJ05coVa5cBIB+cowAA4H5jhhkwMHz4cLVq1UqVK1fWH3/8ofj4eCUlJWnNmjXWLg2AOEcBAEDRKNAM84wZM1S7dm05OzvL2dlZoaGhWrVq1S37fPXVV6pWrZrKlCmjoKAgrVy58p4KBopCRkaGYmJiFBgYqObNm2v79u1as2aNWrRoYe3SAIhzFAAAFI0CzTBXqlRJEyZMUNWqVWUymfTZZ5+pbdu22rVrl+F7L7du3apOnTopNjZWzz77rOLj4xUdHa2dO3eqVq1a9+0ggPstLi7O2iUAuAXOUQAAUBRsTCaT6V524Obmpg8//FA9e/a8aVvHjh2VnZ2tFStWmNc99dRTqlu3rmbOnJnvPq9cuWJxH9rFixfl4+OjzMxMOTs730u5AHDXfIclFPmYJya0KfIxgQcV5ygeNBcvXpSLiwu/4wLF2F3fw5ybm6uvvvpK2dnZCg0NNWyTnJyswYMHW6yLiIjQsmXLbrnv2NhYvf/++3dbGmDoYLXqRT5m9UMHi3xM4EHFOQoAAIqbAj8le+/evXJ0dJSdnZ369u2rpUuXqkaNGoZt09PT5eHhYbHOw8ND6enptxxj+PDhyszMNH9OnTpV0DIBAAAAALgnBZ5hDgwM1O7du5WZmanFixerW7du2rhxY76h+W7Y2dnJzs7uvu0PAAAAAICCKnBgLl26tPz9/SVJ9erV0/bt2zVt2jTNmjXrpraenp46e/asxbqzZ8/K09PzLssFAAAAAKBoFPiS7P+Vl5dn8YCuvwoNDdX69est1iUmJuZ7zzMAAAAAAMVFgWaYhw8frlatWqly5cr6448/FB8fr6SkJK1Zs0aSFBMTI29vb8XGxkqS3nzzTTVp0kSTJk1SmzZttHDhQu3YsUOzZ8++/0cCAAAAAMB9VKDAnJGRoZiYGKWlpcnFxUW1a9fWmjVr1KJFC0lSamqqSpT476R1WFiY4uPjNWLECL3zzjuqWrWqli1bxjuYAQAAAADFXoECc1xc3C23JyUl3bSuffv2at++fYGKAgAAAADA2u75HmYAAAAAAB5GBGYAAAAAAAwQmAEAAFBgsbGxql+/vpycnOTu7q7o6GgdPnz4tv2mTp2qwMBA2dvby8fHR4MGDdLly5eLoGIAKDgCMwAAAAps48aN6tevn3744QclJibq2rVratmypbKzs/PtEx8fr2HDhmnUqFE6ePCg4uLitGjRIr3zzjtFWDkA3LkCPfQLAAAAkKTVq1dbLM+bN0/u7u5KSUlR48aNDfts3bpVjRo1UufOnSVJvr6+6tSpk3788cdCrxcA7gYzzAAAALhnmZmZkiQ3N7d824SFhSklJUXbtm2TJB07dkwrV65U69ati6RGACgoZpgBAABwT/Ly8jRw4EA1atRItWrVyrdd586dde7cOT399NMymUzKyclR3759uSQbQLHFDDMAAADuSb9+/bRv3z4tXLjwlu2SkpI0fvx4ffzxx9q5c6eWLFmihIQEjR07togqBYCCYYYZAAAAd61///5asWKFNm3apEqVKt2y7ciRI9W1a1f16tVLkhQUFKTs7Gz16dNH7777rkqUYC4HQPFCYAYAAECBmUwmDRgwQEuXLlVSUpL8/Pxu2+fSpUs3heKSJUua9wcAxQ2BGQAAAAXWr18/xcfHa/ny5XJyclJ6erokycXFRfb29pKkmJgYeXt7KzY2VpIUFRWlyZMnKzg4WA0bNtSRI0c0cuRIRUVFmYMzABQnBGYAAAAU2IwZMyRJTZs2tVg/d+5cde/eXZKUmppqMaM8YsQI2djYaMSIETp9+rQqVKigqKgojRs3rqjKBoACITADAACgwO7kEuqkpCSLZVtbW40aNUqjRo0qpKoA4P7iyQoAAAAAABhghhkAAAC3dLBa9SIfs/qhg0U+JgD8L2aYAQAAAAAwQGAGAAAAAMAAgRkAAAAAAAMEZgAAAAAADBCYAQAAAAAwQGAGAAAAAMAAgRkAAAAAAAMEZgAAAAAADBCYAQAAAAAwQGAGAAAAAMAAgRkAAAAAAAMEZgAAAAAADBCYAQAAAAAwQGAGAAAAAMAAgRkAAAAAAAMEZgAAAAAADBCYi5nY2FjVr19fTk5Ocnd3V3R0tA4fPnzLPvPmzZONjY3Fp0yZMhZtRo8erWrVqqls2bIqV66cwsPD9eOPPxbmoQAAAADAA43AXMxs3LhR/fr10w8//KDExERdu3ZNLVu2VHZ29i37OTs7Ky0tzfw5efKkxfaAgABNnz5de/fu1ebNm+Xr66uWLVvqt99+K8zDAQAAAIAHlq21C4Cl1atXWyzPmzdP7u7uSklJUePGjfPtZ2NjI09Pz3y3d+7c2WJ58uTJiouL008//aTmzZvfW9EAAAAA8BBihrmYy8zMlCS5ubndsl1WVpaqVKkiHx8ftW3bVvv378+37dWrVzV79my5uLioTp0697VeAAAAAHhYEJiLsby8PA0cOFCNGjVSrVq18m0XGBioOXPmaPny5fr888+Vl5ensLAw/frrrxbtVqxYIUdHR5UpU0ZTpkxRYmKiHnvsscI+DAAAAAB4IHFJdjHWr18/7du3T5s3b75lu9DQUIWGhpqXw8LCVL16dc2aNUtjx441r3/mmWe0e/dunTt3Tp988ok6dOigH3/8Ue7u7oV2DAAAAADwoGKGuZjq37+/VqxYoQ0bNqhSpUoF6luqVCkFBwfryJEjFuvLli0rf39/PfXUU4qLi5Otra3i4uLuZ9kAAAAA8NAgMBczJpNJ/fv319KlS/Xdd9/Jz8+vwPvIzc3V3r175eXldct2eXl5unLlyt2WCgAAAAAPNS7JLmb69eun+Ph4LV++XE5OTkpPT5ckubi4yN7eXpIUExMjb29vxcbGSpLGjBmjp556Sv7+/rpw4YI+/PBDnTx5Ur169ZIkZWdna9y4cXruuefk5eWlc+fO6aOPPtLp06fVvn176xwoAAAAABRzBOZiZsaMGZKkpk2bWqyfO3euunfvLklKTU1ViRL/vTjg/Pnz6t27t9LT01WuXDnVq1dPW7duVY0aNSRJJUuW1KFDh/TZZ5/p3LlzKl++vOrXr6/vv/9eNWvWLJLjAgAAAIAHDYG5mDGZTLdtk5SUZLE8ZcoUTZkyJd/2ZcqU0ZIlS+61NAAAAAB4pHAPMwAAAAAABgo0wxwbG6slS5bo0KFDsre3V1hYmD744AMFBgbm22fevHnq0aOHxTo7Oztdvnz57ip+yPkOSyjyMU9MaFPkYwIAAABAcVegGeaNGzeqX79++uGHH5SYmKhr166pZcuWys7OvmU/Z2dnpaWlmT8nT568p6IBAAAAAChsBZphXr16tcXyvHnz5O7urpSUFDVu3DjffjY2NvL09Ly7CgEAAAAAsIJ7uoc5MzNTkuTm5nbLdllZWapSpYp8fHzUtm1b7d+//5btr1y5oosXL1p8AAAAAAAoSncdmPPy8jRw4EA1atRItWrVyrddYGCg5syZo+XLl+vzzz9XXl6ewsLC9Ouvv+bbJzY2Vi4uLuaPj4/P3ZYJAAAAAMBduevA3K9fP+3bt08LFy68ZbvQ0FDFxMSobt26atKkiZYsWaIKFSpo1qxZ+fYZPny4MjMzzZ9Tp07dbZkAAAAAANyVu3oPc//+/bVixQpt2rRJlSpVKlDfUqVKKTg4WEeOHMm3jZ2dnezs7O6mNAAAAAAA7osCzTCbTCb1799fS5cu1XfffSc/P78CD5ibm6u9e/fKy8urwH0BAAAAACgqBZph7tevn+Lj47V8+XI5OTkpPT1dkuTi4iJ7e3tJUkxMjLy9vRUbGytJGjNmjJ566in5+/vrwoUL+vDDD3Xy5En16tXrPh8KAAAAAAD3T4EC84wZMyRJTZs2tVg/d+5cde/eXZKUmpqqEiX+O3F9/vx59e7dW+np6SpXrpzq1aunrVu3qkaNGvdWOQAAAAAAhahAgdlkMt22TVJSksXylClTNGXKlAIVBQAAAACAtd3Te5gBAAAAAHhYEZgBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwECBAnNsbKzq168vJycnubu7Kzo6WocPH75tv6+++krVqlVTmTJlFBQUpJUrV951wQAAAAAAFIUCBeaNGzeqX79++uGHH5SYmKhr166pZcuWys7OzrfP1q1b1alTJ/Xs2VO7du1SdHS0oqOjtW/fvnsuHgAAAACAwmJbkMarV6+2WJ43b57c3d2VkpKixo0bG/aZNm2aIiMj9fbbb0uSxo4dq8TERE2fPl0zZ868y7IBAAAAAChc93QPc2ZmpiTJzc0t3zbJyckKDw+3WBcREaHk5OR8+1y5ckUXL160+AAAAAAAUJTuOjDn5eVp4MCBatSokWrVqpVvu/T0dHl4eFis8/DwUHp6er59YmNj5eLiYv74+PjcbZkAAAAAANyVuw7M/fr10759+7Rw4cL7WY8kafjw4crMzDR/Tp06dd/HAAAAAADgVgp0D/MN/fv314oVK7Rp0yZVqlTplm09PT119uxZi3Vnz56Vp6dnvn3s7OxkZ2d3N6UBAAAAAHBfFGiG2WQyqX///lq6dKm+++47+fn53bZPaGio1q9fb7EuMTFRoaGhBasUAAAAAIAiVKAZ5n79+ik+Pl7Lly+Xk5OT+T5kFxcX2dvbS5JiYmLk7e2t2NhYSdKbb76pJk2aaNKkSWrTpo0WLlyoHTt2aPbs2ff5UAAAAAAAuH8KNMM8Y8YMZWZmqmnTpvLy8jJ/Fi1aZG6TmpqqtLQ083JYWJji4+M1e/Zs1alTR4sXL9ayZctu+aAwAAAAAACsrUAzzCaT6bZtkpKSblrXvn17tW/fviBDAQAAAABgVff0HmYAAAAAAB5WBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAQIED86ZNmxQVFaWKFSvKxsZGy5Ytu2X7pKQk2djY3PRJT0+/25oBAAAAACh0BQ7M2dnZqlOnjj766KMC9Tt8+LDS0tLMH3d394IODQAAAABAkbEtaIdWrVqpVatWBR7I3d1drq6ud9T2ypUrunLlinn54sWLBR4PAAAAAIB7UWT3MNetW1deXl5q0aKFtmzZcsu2sbGxcnFxMX98fHyKqEoAAAAAAK4r9MDs5eWlmTNn6uuvv9bXX38tHx8fNW3aVDt37sy3z/Dhw5WZmWn+nDp1qrDLBAAAAADAQoEvyS6owMBABQYGmpfDwsJ09OhRTZkyRf/+978N+9jZ2cnOzq6wSwMAAAAAIF9Wea1UgwYNdOTIEWsMDQAAAADAHbFKYN69e7e8vLysMTQAAAAAAHekwJdkZ2VlWcwOHz9+XLt375abm5sqV66s4cOH6/Tp05o/f74kaerUqfLz81PNmjV1+fJlffrpp/ruu++0du3a+3cUAAAAAADcZwUOzDt27NAzzzxjXh48eLAkqVu3bpo3b57S0tKUmppq3n716lW99dZbOn36tBwcHFS7dm2tW7fOYh8AAAAAABQ3BQ7MTZs2lclkynf7vHnzLJaHDh2qoUOHFrgwAAAAAACsySr3MAMAAAAAUNwRmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAAIEZAAAAAAADBGYAAAAAAAwQmAEAAAAAMEBgBgAAAADAQIED86ZNmxQVFaWKFSvKxsZGy5Ytu22fpKQkhYSEyM7OTv7+/po3b95dlAoAAAAAQNEpcGDOzs5WnTp19NFHH91R++PHj6tNmzZ65plntHv3bg0cOFC9evXSmjVrClwsAAAAAABFxbagHVq1aqVWrVrdcfuZM2fKz89PkyZNkiRVr15dmzdv1pQpUxQREVHQ4QEAAAAAKBKFfg9zcnKywsPDLdZFREQoOTk53z5XrlzRxYsXLT4AAAAAABSlQg/M6enp8vDwsFjn4eGhixcv6s8//zTsExsbKxcXF/PHx8ensMsEAAAAAMBCsXxK9vDhw5WZmWn+nDp1ytolAQAAAAAeMQW+h7mgPD09dfbsWYt1Z8+elbOzs+zt7Q372NnZyc7OrrBLAwAAAAAgX4U+wxwaGqr169dbrEtMTFRoaGhhDw0AAAAAwF0rcGDOysrS7t27tXv3bknXXxu1e/dupaamSrp+OXVMTIy5fd++fXXs2DENHTpUhw4d0scff6wvv/xSgwYNuj9HAAAAAABAIShwYN6xY4eCg4MVHBwsSRo8eLCCg4P13nvvSZLS0tLM4VmS/Pz8lJCQoMTERNWpU0eTJk3Sp59+yiulAAAAAADFWoHvYW7atKlMJlO+2+fNm2fYZ9euXQUdCgAAAAAAqymWT8kGAAAAAMDaCMwAAAAAABggMKNY+eijj+Tr66syZcqoYcOG2rZt2y3bT506VYGBgbK3t5ePj48GDRqky5cvF1G1wKOHcxQAADxKCMwoNhYtWqTBgwdr1KhR2rlzp+rUqaOIiAhlZGQYto+Pj9ewYcM0atQoHTx4UHFxcVq0aJHeeeedIq4ceDRwjgIAgEcNgRnFxuTJk9W7d2/16NFDNWrU0MyZM+Xg4KA5c+YYtt+6dasaNWqkzp07y9fXVy1btlSnTp1uO+MF4O5wjgIAgEcNgRnFwtWrV5WSkqLw8HDzuhIlSig8PFzJycmGfcLCwpSSkmL+5fvYsWNauXKlWrduXSQ1A48SzlEAAPAoKvBrpYDCcO7cOeXm5srDw8NivYeHhw4dOmTYp3Pnzjp37pyefvppmUwm5eTkqG/fvlzuCRQCzlEAAPAoYoYZD6ykpCSNHz9eH3/8sXbu3KklS5YoISFBY8eOtXZpAMQ5CgAAHnzMMKNYeOyxx1SyZEmdPXvWYv3Zs2fl6elp2GfkyJHq2rWrevXqJUkKCgpSdna2+vTpo3fffVclSvD3IOB+4RwFAACPIn5bQbFQunRp1atXT+vXrzevy8vL0/r16xUaGmrY59KlSzf9wl2yZElJkslkKrxigUcQ5ygAAHgUMcOMYmPw4MHq1q2bnnzySTVo0EBTp05Vdna2evToIUmKiYmRt7e3YmNjJUlRUVGaPHmygoOD1bBhQx05ckQjR45UVFSU+ZdyAPcP5ygAAHjUEJhRbHTs2FG//fab3nvvPaWnp6tu3bpavXq1+SFDqampFrNVI0aMkI2NjUaMGKHTp0+rQoUKioqK0rhx46x1CMBDjXMUAAA8amxMD8B1cRcvXpSLi4syMzPl7Oxs7XIKle+whCIf88SENkU+pjUcrFa9yMesfuhgkY+JwsP5Wbg4R3GvOEcLD+dn4XiUfscFHlTcwwwAAAAAgAEuyYZVBH0WVORjflnkIwIPJmucnxLnKAAAKH6YYQYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAA4CHw0UcfydfXV2XKlFHDhg21bdu2W7a/cOGC+vXrJy8vL9nZ2SkgIEArV64somoB4MFga+0CAAAAcG8WLVqkwYMHa+bMmWrYsKGmTp2qiIgIHT58WO7u7je1v3r1qlq0aCF3d3ctXrxY3t7eOnnypFxdXYu+eAAoxgjMAAAAD7jJkyerd+/e6tGjhyRp5syZSkhI0Jw5czRs2LCb2s+ZM0e///67tm7dqlKlSkmSfH19i7JkAHggcEk2AADAA+zq1atKSUlReHi4eV2JEiUUHh6u5ORkwz7ffPONQkND1a9fP3l4eKhWrVoaP368cnNzi6psAHggMMMMAADwADt37pxyc3Pl4eFhsd7Dw0OHDh0y7HPs2DF999136tKli1auXKkjR47o9ddf17Vr1zRq1KiiKBsAHggEZgAAgEdMXl6e3N3dNXv2bJUsWVL16tXT6dOn9eGHHxKYAeAvCMwAAAAPsMcee0wlS5bU2bNnLdafPXtWnp6ehn28vLxUqlQplSxZ0ryuevXqSk9P19WrV1W6dOlCrRkAHhTcwwwAAPAAK126tOrVq6f169eb1+Xl5Wn9+vUKDQ017NOoUSMdOXJEeXl55nU///yzvLy8CMsA8BcEZgAAgAfc4MGD9cknn+izzz7TwYMH9dprryk7O9v81OyYmBgNHz7c3P61117T77//rjfffFM///yzEhISNH78ePXr189ahwAAxRKXZAMAADzgOnbsqN9++03vvfee0tPTVbduXa1evdr8ILDU1FSVKPHfeRIfHx+tWbNGgwYNUu3ateXt7a0333xTf//73611CABQLBGYAQAAHgL9+/dX//79DbclJSXdtC40NFQ//PBDIVcFAA82LskGAAAAAMAAM8wAAAAPkKDPgop8zC+LfEQAKB6YYQYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAAAAAAA3cVmD/66CP5+vqqTJkyatiwobZt25Zv23nz5snGxsbiU6ZMmbsuGAAAAACAolDgwLxo0SINHjxYo0aN0s6dO1WnTh1FREQoIyMj3z7Ozs5KS0szf06ePHlPRQMAAAAAUNgKHJgnT56s3r17q0ePHqpRo4ZmzpwpBwcHzZkzJ98+NjY28vT0NH88PDzuqWgAAAAAAApbgQLz1atXlZKSovDw8P/uoEQJhYeHKzk5Od9+WVlZqlKlinx8fNS2bVvt37//luNcuXJFFy9etPgAAAAAAFCUChSYz507p9zc3JtmiD08PJSenm7YJzAwUHPmzNHy5cv1+eefKy8vT2FhYfr111/zHSc2NlYuLi7mj4+PT0HKBAAAAADgnhX6U7JDQ0MVExOjunXrqkmTJlqyZIkqVKigWbNm5dtn+PDhyszMNH9OnTpV2GUCAAAAAGDBtiCNH3vsMZUsWVJnz561WH/27Fl5enre0T5KlSql4OBgHTlyJN82dnZ2srOzK0hpAAAAAADcVwWaYS5durTq1aun9evXm9fl5eVp/fr1Cg0NvaN95Obmau/evfLy8ipYpQAAAAAAFKECzTBL0uDBg9WtWzc9+eSTatCggaZOnars7Gz16NFDkhQTEyNvb2/FxsZKksaMGaOnnnpK/v7+unDhgj788EOdPHlSvXr1ur9HAgAAAADAfVTgwNyxY0f99ttveu+995Senq66detq9erV5geBpaamqkSJ/05cnz9/Xr1791Z6errKlSunevXqaevWrapRo8b9OwoAAAAAAO6zAgdmSerfv7/69+9vuC0pKcliecqUKZoyZcrdDAMAAAAAgNUU+lOyAQAAAAB4EBGYAQAAAAAwQGAGAAAAAMAAgRkAAAAAAAMEZgAAAAAADBCYAQAAAAAwQGAGAAAAAMAAgRkAAAAAAAMEZgAAAAAADBCYAQAAAAAwQGAGAAAAAMAAgRkAAAAAAAMEZgAAAAAADBCYAQAAAAAwQGAGAAAAAMAAgRkAAAAAAAMEZgAAAAAADBCYAQAAAAAwQGAGAAAAAMAAgRkAAAAAAAMEZgAAAAAADBCYAQAAAAAwQGAGAAAAAMAAgRkAAAAAAAMEZgAAAAAADBCYAQAAAAAwQGAGAAAAAMAAgRkAAAAAAAMEZgAAAAAADBCYAQAAAAAwQGAGAAAAAMAAgRkAAAAAAAMEZgAAAAAADBCYAQAAAAAwQGAGAAAAAMAAgRl35aOPPpKvr6/KlCmjhg0batu2bdYuCcD/x/kJFG+cowDw4CAwo8AWLVqkwYMHa9SoUdq5c6fq1KmjiIgIZWRkWLs04JHH+QkUb5yjAPBgITCjwCZPnqzevXurR48eqlGjhmbOnCkHBwfNmTPH2qUBjzzOT6B44xwFgAcLgRkFcvXqVaWkpCg8PNy8rkSJEgoPD1dycrIVKwPA+QkUb5yjAPDgITCjQM6dO6fc3Fx5eHhYrPfw8FB6erqVqgIgcX4CxR3nKAA8eAjMAAAAAAAYIDCjQB577DGVLFlSZ8+etVh/9uxZeXp6WqkqABLnJ1DccY4CwIOHwIwCKV26tOrVq6f169eb1+Xl5Wn9+vUKDQ21YmUAOD+B4o1zFAAePHcVmAv6/sCvvvpK1apVU5kyZRQUFKSVK1feVbEoHgYPHqxPPvlEn332mQ4ePKjXXntN2dnZ6tGjh7VLAx55nJ9A8cY5CgAPFtuCdrjx/sCZM2eqYcOGmjp1qiIiInT48GG5u7vf1H7r1q3q1KmTYmNj9eyzzyo+Pl7R0dHauXOnatWqdV8OAkWrY8eO+u233/Tee+8pPT1ddevW1erVq296iAmAosf5CRRvnKMA8GCxMZlMpoJ0aNiwoerXr6/p06dLun4pkY+PjwYMGKBhw4bd1L5jx47Kzs7WihUrzOueeuop1a1bVzNnzryjMS9evCgXFxdlZmbK2dm5IOU+cHyHJRT5mCcmtCnyMYM+CyryMb+MzSnyMasfOljkY6LwcH4WLs5R3CvO0cLD+Vk4HqXfcYEHVYFmmG+8P3D48OHmdbd7f2BycrIGDx5ssS4iIkLLli3Ld5wrV67oypUr5uXMzExJ1/+n8rDLu3KpyMe0xvea+2dukY+ZlVv0Yz4K/80+Sjg/CxfnKO4V52jh4fwsHDeOsYDzVwCKUIEC863eH3jo0CHDPunp6QV+32BsbKzef//9m9b7+PgUpFzcIZep1q6gaDSwxqAuLtYYFQ+RR+X8lDhH8WB6VM5Rzs/C9ccff8jlETpe4EFS4HuYi8Lw4cMtZqXz8vL0+++/q3z58rKxsbFiZbgfLl68KB8fH506dYrLj4BihvMTKN44Rx8uJpNJf/zxhypWrGjtUgDko0CB+W7eH+jp6Vng9w3a2dnJzs7OYp2rq2tBSsUDwNnZmR/2QDHF+QkUb5yjDw9mloHirUCvlbqb9weGhoZatJekxMRE3jcIAAAAACjWCnxJ9uDBg9WtWzc9+eSTatCggaZOnWrx/sCYmBh5e3srNjZWkvTmm2+qSZMmmjRpktq0aaOFCxdqx44dmj179v09EgAAAAAA7qMCB+bbvT8wNTVVJUr8d+I6LCxM8fHxGjFihN555x1VrVpVy5Yt4x3MjzA7OzuNGjXqpsvuAVgf5ydQvHGOAkDRKvB7mAEAAAAAeBQU6B5mAAAAAAAeFQRmAAAAAAAMEJgBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgBAAAAADBga+0C8GjIycnR/v37lZ6eLkny9PRUjRo1VKpUKStXBgBA8cbPUACwHmaYUajy8vI0YsQIVahQQcHBwWrVqpVatWql4OBgubu7a+TIkcrLy7N2mcAj69q1axo6dKj8/f3VoEEDzZkzx2L72bNnVbJkSStVBzza+BkKANZHYEahGjZsmGbPnq0JEybo2LFjys7OVnZ2to4dO6YPPvhAs2fP1vDhw61dJvDIGjdunObPn6++ffuqZcuWGjx4sF599VWLNiaTyUrVAY82foYCgPXZmPhNCIXI09NTn332mSIiIgy3r1mzRjExMTp79mwRVwZAkqpWraopU6bo2WeflSQdOXJErVq10tNPP605c+YoIyNDFStWVG5urpUrBR49/AwFAOtjhhmF6o8//lDFihXz3e7l5aXs7OwirAjAX50+fVq1atUyL/v7+yspKUlbt25V165dCcqAFfEzFACsj8CMQtW0aVMNGTJE586du2nbuXPn9Pe//11NmzYt+sIASLo+g3X06FGLdd7e3tqwYYO2b9+u7t27W6cwAPwMBYBigEuyUahOnTql1q1b69ChQwoKCpKHh4ek6w8S2rt3r2rUqKEVK1bIx8fHypUCj6ZevXrJZDIpLi7upm2nT59W06ZNdezYMWaaASvgZygAWB+BGYUuLy9Pa9as0Q8//GDxSozQ0FC1bNlSJUpwoQNgLSdPntShQ4fyvUfyzJkzSkxMVLdu3Yq4MgASP0MBwNoIzAAAAAAAGLC1dgF4NGzbtk3JyckWfx0PCwtT/fr1rVwZgFs5f/68vv32W8XExFi7FOCRxc9QALAeZphRqDIyMvTCCy9oy5Ytqly5ssX9V6mpqWrUqJG+/vprubu7W7lSAEb27NmjkJAQ7mEGrICfoQBgfcwwo1C9/vrrys3N1cGDBxUYGGix7fDhw3rllVfUr18/ffXVV1aqEHi0Xbx48Zbb//jjjyKqBMD/4mcoAFgfM8woVE5OTtq0aZOCg4MNt6ekpKhp06b8Ug5YSYkSJWRjY5PvdpPJJBsbG2aYASvgZygAWB8zzChUdnZ2t5zB+uOPP2RnZ1eEFQH4KycnJ7377rtq2LCh4fZffvlFr776ahFXBUDiZygAFAcEZhSqjh07qlu3bpoyZYqaN28uZ2dnSdcvA12/fr0GDx6sTp06WblK4NEVEhIiSWrSpInhdldXV3EhEmAd/AwFAOsjMKNQTZ48WXl5eXrppZeUk5Oj0qVLS5KuXr0qW1tb9ezZUxMnTrRylcCjq3Pnzvrzzz/z3e7p6alRo0YVYUUAbuBnKABYH/cwo0hcvHhRKSkpFq/EqFevnvmv5QAAwNjFixe1Y8cOnT17VhI/QwGgKBGYUegOHjyoH374QaGhoapWrZoOHTqkadOm6cqVK3r55ZfVrFkza5cI4P/Lzs7Wl19+qSNHjsjLy0udOnVS+fLlrV0W8EgaMGCAOnTooL/97W/WLgUAHlkEZhSq1atXq23btnJ0dNSlS5e0dOlSxcTEqE6dOsrLy9PGjRu1du1aQjNgJTVq1NDmzZvl5uamU6dOqXHjxjp//rwCAgJ09OhR2dra6ocffpCfn5+1SwUeOTeeYv/EE0+oZ8+e6tatmzw9Pa1dFgA8UkpYuwA83MaMGaO3335b//nPfzR37lx17txZvXv3VmJiotavX6+3335bEyZMsHaZwCPr0KFDysnJkSQNHz5cFStW1MmTJ7Vt2zadPHlStWvX1rvvvmvlKoFH19q1a9W6dWtNnDhRlStXVtu2bbVixQrl5eVZuzQAeCQww4xC5eLiopSUFPn7+ysvL092dnbatm2b+Z2S+/btU3h4uPneZgBFq0SJEkpPT5e7u7ueeOIJzZw5Uy1atDBv37p1q1566SWlpqZasUrg0fTX8/PatWtaunSp5syZo3Xr1snDw0Pdu3dXjx495O/vb+1SAeChxQwzCp2NjY2k6z/4y5QpIxcXF/M2JycnZWZmWqs0APrvOXr58mV5eXlZbPP29tZvv/1mjbIA/EWpUqXUoUMHrV69WseOHVPv3r21YMECBQYGWrs0AHioEZhRqHx9ffXLL7+Yl5OTk1W5cmXzcmpq6k2/oAMoWs2bN1dISIguXryow4cPW2w7efIkD/0CipnKlStr9OjROn78uFavXm3tcgDgocZ7mFGoXnvtNeXm5pqXa9WqZbF91apVPPALsKL/fceyo6OjxfK3337LE3oBK6lSpYpKliyZ73YbGxuLWygAAPcf9zADAAAAAGCAS7IBAAAAADBAYAYAAAAAwACBGQAAAAAAAwRmAAAAAAAMEJgB4AFgY2OjZcuWFfo4TZs21cCBA+/rPkePHq26deve130CAAAUBQIzAFhZenq6BgwYoMcff1x2dnby8fFRVFSU1q9fb+3S7sjSpUv11FNPycXFRU5OTqpZs6ZF6B4yZMgDcywAAAB/xXuYAcCKTpw4oUaNGsnV1VUffvihgoKCdO3aNa1Zs0b9+vXToUOHrF3iLa1fv14dO3bUuHHj9Nxzz8nGxkYHDhxQYmKiuY2jo+NN73cGAAB4EDDDDABW9Prrr8vGxkbbtm3TCy+8oICAANWsWVODBw/WDz/8YNH23LlzateunRwcHFS1alV988035m3z5s2Tq6urRftly5bJxsbGvHzj0uh///vf8vX1lYuLi1566SX98ccf+daXkJAgFxcXLViwwHD7t99+q0aNGuntt99WYGCgAgICFB0drY8++uimcW+wsbG56ePr62vevm/fPrVq1UqOjo7y8PBQ165dde7cuVt9jQAAAIWCwAwAVvL7779r9erV6tevn8qWLXvT9v8NwO+//746dOign376Sa1bt1aXLl30+++/F2jMo0ePatmyZVqxYoVWrFihjRs3asKECYZt4+Pj1alTJy1YsEBdunQxbOPp6an9+/dr3759d1xDWlqa+XPkyBH5+/urcePGkqQLFy6oWbNmCg4O1o4dO7R69WqdPXtWHTp0KNBxAgAA3A8EZgCwkiNHjshkMqlatWp31L579+7q1KmT/P39NX78eGVlZWnbtm0FGjMvL0/z5s1TrVq19Le//U1du3Y1vL/4o48+0uuvv65vv/1Wzz77bL77GzBggOrXr6+goCD5+vrqpZde0pw5c3TlypV8+3h6esrT01MeHh56++235eLiolmzZkmSpk+fruDgYI0fP17VqlVTcHCw5syZow0bNujnn38u0LECAADcK+5hBgArMZlMBWpfu3Zt8z+XLVtWzs7OysjIKNA+fH195eTkZF728vK6aR+LFy9WRkaGtmzZovr1699yf2XLllVCQoKOHj2qDRs26IcfftBbb72ladOmKTk5WQ4ODvn2feedd5ScnKwdO3bI3t5ekrRnzx5t2LDB8J7no0ePKiAgoCCHCwAAcE+YYQYAK6latapsbGzu+MFepUqVsli2sbFRXl6eJKlEiRI3BfBr164VaB83BAcHq0KFCpozZ84dh/onnnhCvXr10qeffqqdO3fqwIEDWrRoUb7tP//8c02ZMkVLly6Vt7e3eX1WVpaioqK0e/dui88vv/xivmwbAACgqBCYAcBK3NzcFBERoY8++kjZ2dk3bb9w4cId76tChQr6448/LPaze/fuu6rriSee0IYNG7R8+XINGDCgwP19fX3l4OBgeEySlJycrF69emnWrFl66qmnLLaFhIRo//798vX1lb+/v8XH6D5vAACAwkRgBgAr+uijj5Sbm6sGDRro66+/1i+//KKDBw/qX//6l0JDQ+94Pw0bNpSDg4PeeecdHT16VPHx8Zo3b95d1xUQEKANGzbo66+/tnin8v8aPXq0hg4dqqSkJB0/fly7du3SK6+8omvXrqlFixY3tU9PT1e7du300ksvKSIiQunp6UpPT9dvv/0mSerXr59+//13derUSdu3b9fRo0e1Zs0a9ejRQ7m5uXd9PAAAAHeDwAwAVvT4449r586deuaZZ/TWW2+pVq1aatGihdavX68ZM2bc8X7c3Nz0+eefa+XKlQoKCtIXX3yh0aNH31NtgYGB+u677/TFF1/orbfeMmzTpEkTHTt2TDExMapWrZpatWql9PR0rV27VoGBgTe1P3TokM6ePavPPvtMXl5e5s+Ne6UrVqyoLVu2KDc3Vy1btlRQUJAGDhwoV1dXlSjBjywAAFC0bEwFfeoMAAAAAACPAP5cDwAAAACAAQIzAAAAAAAGCMwAAAAAABggMAMAAAAAYIDADAAAAACAAQIzAAAAAAAGCMwAAAAAABggMAMAAAAAYIDADAAAAACAAQIzAAAAAAAGCMwAAAAAABj4f8YBTIyZcNviAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert columns to numeric type\n",
    "col_to_convert =['Chunk Size', 'Average Response Time','Average Faithfulness', 'Average Relevancy', 'Average Correctness']\n",
    "df[col_to_convert] = df[col_to_convert].apply(pd.to_numeric)\n",
    "\n",
    "#Plot metrics\n",
    "ax = df.plot(\n",
    "        x='Chunk Size', \n",
    "        y=['Average Response Time','Average Faithfulness', 'Average Relevancy', 'Average Correctness'], \n",
    "        kind='bar', \n",
    "        figsize=(9,6))\n",
    "\n",
    "\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax.bar_label(ax.containers[1])\n",
    "ax.bar_label(ax.containers[2])\n",
    "ax.bar_label(ax.containers[3])\n",
    "\n",
    "ax.legend( bbox_to_anchor =(1 ,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test example\n",
    "\n",
    "Let's evaluate the response for each chunk size to for a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kb_id = kb_oss_index_dict[300]['kb_id']\n",
    "\n",
    "query = \"What is Amazon's doing in the field of generative AI?\"\n",
    "\n",
    "response = retrieve(query, kb_id, 5)\n",
    "retrievalResults = response['retrievalResults']\n",
    "# pp.pprint(retrievalResults)\n",
    "contexts = get_contexts(retrievalResults)\n",
    "\n",
    "prompt = titan_prompt.format(context_str=contexts, query_str=query)\n",
    "\n",
    "# Configurations\n",
    "parameters = {\n",
    "\"maxTokenCount\":2000,\n",
    "\"stopSequences\":[],\n",
    "\"temperature\":0,\n",
    "\"topP\":0.9\n",
    "}\n",
    "\n",
    "llm = Bedrock(model_id = \"amazon.titan-text-lite-v1\", model_kwargs=parameters, client = bedrock_runtime_client)\n",
    "response = llm(prompt)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kb_id = kb_oss_index_dict[512]['kb_id']\n",
    "\n",
    "query = \"What is Amazon's doing in the field of generative AI?\"\n",
    "\n",
    "response = retrieve(query, kb_id, 5)\n",
    "retrievalResults = response['retrievalResults']\n",
    "# pp.pprint(retrievalResults)\n",
    "contexts = get_contexts(retrievalResults)\n",
    "\n",
    "prompt = titan_prompt.format(context_str=contexts, query_str=query)\n",
    "\n",
    "# Configurations\n",
    "parameters = {\n",
    "\"maxTokenCount\":2000,\n",
    "\"stopSequences\":[],\n",
    "\"temperature\":0,\n",
    "\"topP\":0.9\n",
    "}\n",
    "\n",
    "llm = Bedrock(model_id = \"amazon.titan-text-lite-v1\", model_kwargs=parameters, client = bedrock_runtime_client)\n",
    "response = llm(prompt)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kb_id = kb_oss_index_dict[700]['kb_id']\n",
    "\n",
    "query = \"What is Amazon's doing in the field of generative AI?\"\n",
    "\n",
    "response = retrieve(query, kb_id, 5)\n",
    "retrievalResults = response['retrievalResults']\n",
    "# pp.pprint(retrievalResults)\n",
    "contexts = get_contexts(retrievalResults)\n",
    "\n",
    "prompt = titan_prompt.format(context_str=contexts, query_str=query)\n",
    "\n",
    "# Configurations\n",
    "parameters = {\n",
    "\"maxTokenCount\":2000,\n",
    "\"stopSequences\":[],\n",
    "\"temperature\":0,\n",
    "\"topP\":0.9\n",
    "}\n",
    "\n",
    "llm = Bedrock(model_id = \"amazon.titan-text-lite-v1\", model_kwargs=parameters, client = bedrock_runtime_client)\n",
    "response = llm(prompt)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Please make sure to delete all the resources that were created as you will be incurred cost for storing documents in OSS index and S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty and delete S3 Bucket\n",
    "\n",
    "objects = s3_client.list_objects(Bucket=bucket_name)  \n",
    "if 'Contents' in objects:\n",
    "    for obj in objects['Contents']:\n",
    "        s3_client.delete_object(Bucket=bucket_name, Key=obj['Key']) \n",
    "s3_client.delete_bucket(Bucket=bucket_name)\n",
    "\n",
    "# Delete all KnowledgeBases\n",
    "\n",
    "for item in job_list:\n",
    "    bedrock_agent_client.delete_data_source(dataSourceId = item[\"dataSourceId\"], knowledgeBaseId=item['knowledgeBaseId'])\n",
    "    bedrock_agent_client.delete_knowledge_base(knowledgeBaseId=item['knowledgeBaseId'])\n",
    "    \n",
    "# delete all indexes\n",
    "for key in kb_oss_index_dict:\n",
    "    oss_client.indices.delete(index=kb_oss_index_dict[key]['index_name'])\n",
    "\n",
    "\n",
    "# delete collection\n",
    "collection_id = collection['createCollectionDetail']['id']\n",
    "aoss_client.delete_collection(id=collection_id)\n",
    "\n",
    "# delete data, network, and encryption access ploicies\n",
    "aoss_client.delete_access_policy(type=\"data\", name=access_policy['accessPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"network\", name=network_policy['securityPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"encryption\", name=encryption_policy['securityPolicyDetail']['name'])\n",
    "\n",
    "## DELETE ROLE\n",
    "try: \n",
    "    # Get the list of attached policies\n",
    "    attached_policies = iam_client.list_attached_role_policies(RoleName=bedrock_kb_execution_role_name)\n",
    "\n",
    "    # Detach each policy\n",
    "    for policy in attached_policies['AttachedPolicies']:\n",
    "        iam_client.detach_role_policy(RoleName=bedrock_kb_execution_role_name, PolicyArn=policy['PolicyArn'])\n",
    "        iam_client.delete_policy( PolicyArn=policy['PolicyArn'])\n",
    "\n",
    "    # Delete role\n",
    "    iam_client.delete_role(RoleName=bedrock_kb_execution_role_name)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"couldn't delete role\", bedrock_kb_execution_role_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "bedrock_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
