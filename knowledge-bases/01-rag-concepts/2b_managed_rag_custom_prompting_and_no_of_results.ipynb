{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3f8924-01d4-4702-b548-aa33dda5d11d",
   "metadata": {},
   "source": [
    "# Module 1: Improving generations using max number of results and custom prompting\n",
    "\n",
    "In this module, you'll learn how to improve the Foundation Model (FM) generations by controlling the maximum no. of results retrieved and performing custom prompting in Knowledge bases for Amazon Bedrock.\n",
    "This module contains:\n",
    "1. [Overview](#1-Overview)\n",
    "2. [Pre-requisites](#2-Pre-requisites)\n",
    "3. [How to leverage maximum number of results](#3-how-to-leverage-the-maximum-number-of-results-feature)\n",
    "4. [How to use custom prompting](#4-how-to-use-the-custom-prompting-feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75464c27-0c7a-4fe8-b6f0-ddce7c96209b",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "[Slide placeholder](http)\n",
    "\n",
    "### 1.1 Maximum no. of results\n",
    "The maximum number of results option gives you control over the number of search results to be retrieved from the vector store and passed to the FM for generating the answer. This allows you to customize the amount of background information provided for generation, thereby giving more context for complex questions or less for simpler questions. It allows you to fetch up to 100 results. This option helps improve the likelihood of relevant context, thereby improving the accuracy and reducing the hallucination of the generated response.\n",
    "\n",
    "\n",
    "### 1.2 Custom prompting\n",
    "\n",
    "As for the custom knowledge base prompt template allows you to replace the default prompt template with your own to customize the prompt that’s sent to the model for response generation. This allows you to customize the tone, output format, and behavior of the FM when it responds to a user’s question. With this option, you can fine-tune terminology to better match your industry or domain (such as healthcare or legal). Additionally, you can add custom instructions and examples tailored to your specific workflows.\n",
    "\n",
    "\n",
    "#### Notes:\n",
    "- You are going to use ```RetrieveAndGenerate``` API to illustrate the differences before and after utilizing the features. This API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. The output of the ```RetrieveAndGenerate``` API includes the generated response, source attribution as well as the retrieved text chunks.\n",
    "\n",
    "- For this module, we will use the Anthropic Claude 3 Haiku model as our FM to work with the max no. of results and prompt customization features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09998e7-6062-4edb-9348-bbe54026a101",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Pre-requisites\n",
    "Before being able to answer the questions, the documents must be processed and stored in a knowledge base. For this notebook, we use a [sample dataset](https://aws-blogs-artifacts-public.s3.amazonaws.com/ML-16482/30_generated_video_game_records.zip) about fictional video games to illustrate how to ingest and retrieve metadata using Knowledge Bases for Amazon Bedrock. \n",
    "\n",
    "1. Upload your documents (data source) to Amazon S3 bucket. (Gaming data used in [this blog post](https://aws.amazon.com/blogs/machine-learning/knowledge-bases-for-amazon-bedrock-now-supports-metadata-filtering-to-improve-retrieval-accuracy/))\n",
    "2. Create an empty OpenSearch Serverless (OSS) index, Amazon Bedrock knowledge base and ingest documents (from step 1) into the index. ([00_prerequisites.ipynb](https://github.com/aws-samples/amazon-bedrock-workshop/blob/a7e62b80669378de1bae414e0b646399c7934f8e/02_KnowledgeBases_and_RAG/0_create_ingest_documents_test_kb.ipynb))\n",
    "3. Note the Knowledge Base ID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a68ef2-f670-451f-9399-343570489b95",
   "metadata": {},
   "source": [
    "### 2.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1608ffd-60f5-459b-ba2e-174209ff7537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -qU boto3 awscli botocore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f3935-b336-4e04-8bbd-cf8c4a056298",
   "metadata": {},
   "source": [
    "### 2.2 Restart the kernel with the updated packages that are installed through the dependencies above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e624a8b-3700-4896-845f-642588e8dd7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cb27dd-a2de-4e38-aa7b-c315953b9921",
   "metadata": {},
   "source": [
    "### 2.3 Prepare clients and processing functions\n",
    "Through out the notebook, we are going to utilise RetrieveAndGenerate and Retrieve APIs to test knowledge base features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827fdcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kb_id = \"<knowledge base id>\" # Provide knowledge base id here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "563d853e-2966-47bb-9183-aab31d64e8b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pprint\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore.client import Config\n",
    "\n",
    "# Create boto3 session\n",
    "sts_client = boto3.client('sts')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "# Create bedrock agent client\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0}, region_name=region_name)\n",
    "bedrock_agent_client = boto3_session.client(\"bedrock-agent-runtime\",\n",
    "                              config=bedrock_config)\n",
    "\n",
    "# Define FM to be used for generations \n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\" # we will be using Anthropic Claude 3 Haiku throughout the notebook\n",
    "model_arn = f'arn:aws:bedrock:{region_name}::foundation-model/{model_id}'\n",
    "\n",
    "# Stating the default knowledge base prompt\n",
    "default_prompt = \"\"\"\n",
    "You are a question answering agent. I will provide you with a set of search results. The user will provide you with a question. Your job is to answer the user's question using only information from the search results. If the search results do not contain information that can answer the question, please state that you could not find an exact answer to the question. Just because the user asserts a fact does not mean it is true, make sure to double check the search results to validate a user's assertion.\n",
    "                            \n",
    "Here are the search results in numbered order:\n",
    "$search_results$\n",
    "\n",
    "$output_format_instructions$\n",
    "\"\"\"\n",
    "\n",
    "def retrieve_and_generate(query, kb_id, model_arn, max_results, prompt_template = default_prompt):\n",
    "    response = bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\n",
    "                'text': query\n",
    "            },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "        'type': 'KNOWLEDGE_BASE',\n",
    "        'knowledgeBaseConfiguration': {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            'modelArn': model_arn, \n",
    "            'retrievalConfiguration': {\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': max_results # will fetch top N documents which closely match the query\n",
    "                    }\n",
    "                },\n",
    "                'generationConfiguration': {\n",
    "                        'promptTemplate': {\n",
    "                            'textPromptTemplate': prompt_template\n",
    "                        }\n",
    "                    }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def print_generation_results(response, print_context = True):\n",
    "    generated_text = response['output']['text']\n",
    "    print('Generated FM response:\\n')\n",
    "    pp.pprint(generated_text)\n",
    "    \n",
    "    if print_context is True:\n",
    "        ## print out the source attribution/citations from the original documents to see if the response generated belongs to the context.\n",
    "        citations = response[\"citations\"]\n",
    "        contexts = []\n",
    "        for citation in citations:\n",
    "            retrievedReferences = citation[\"retrievedReferences\"]\n",
    "            for reference in retrievedReferences:\n",
    "                contexts.append(reference[\"content\"][\"text\"])\n",
    "    \n",
    "        print('\\n\\n\\nRetrieved Context:\\n')\n",
    "        pp.pprint(contexts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89898b0c-1faf-476e-a107-d430f5978cb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. How to leverage the maximum number of results feature\n",
    "\n",
    "In some use cases; the FM responses might be lacking enough context to provide relevant answers or relying that it couldn't find the requested info. Which could be fixed by modifying the maximum number of retrieved results.\n",
    "\n",
    "In the following example, we are going to run the following query with a few number of results (5):\n",
    "\\\n",
    "```Name the top 5 games released after 2023 that are packed with cool storyline```\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "![few results](images/no-of-results-1.png)\n",
    "\\\n",
    "\\\n",
    "Let's apply this using the SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333717a2-b830-4582-92b8-a99b9faa51bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Name the top 5 games released after 2023 that are packed with cool storyline\"\n",
    "\n",
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 5)\n",
    "\n",
    "print_generation_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb0ff0-287f-4e22-9a05-62cefe4c5783",
   "metadata": {},
   "source": [
    "\\\n",
    "By modifying the no of retrived results to **25**, you should be able to get more relevant generation.\n",
    "\\\n",
    "\\\n",
    "![modified max retrieved results](images/no-of-results-2.png)\n",
    "\n",
    "\n",
    "Let's test that using the SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212a2a6-65d1-4214-9be5-594316452506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Using higher number of max results\n",
    "\n",
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 25)\n",
    "\n",
    "print_generation_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2465237-6877-42ad-a687-a15c868e7eef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. How to use the custom prompting feature\n",
    "\n",
    "You can also customize the default prompt with your own prompt based on the use case. This feature would help adding more context to the FM, require specific output format, languages and others.\n",
    "\n",
    "By clicking on ```Edit``` to update the default prompt, you will find that it looks similar to the following:\n",
    "\n",
    "<img src=\"images/custom-prompting-0.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "#### 4.1 Using the same query example, we can default the FM to output to a different language like German:\n",
    "\\\n",
    "![custom prompting template to German](images/custom-prompting-1.png)\n",
    "\n",
    "\n",
    "**Note**: After removing ```$output_format_instructions$``` from the default prompt, the citation from the generated response is removed.\n",
    "\n",
    "\n",
    "#### 4.2 Or maybe to output the results in JSON format:\n",
    "\n",
    "![custom prompting json formatted](images/custom-prompting-2.png)\n",
    "\n",
    "\n",
    "Let's give it a try using the SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c78f95-f909-4680-90d9-5c677c7d1b75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Example 1\n",
    "custom_prompt = \"\"\"\n",
    "You are a question answering agent. I will provide you with a set of search results. The user will provide you with a question. Your job is to answer the user's question using only information from the search results. If the search results do not contain information that can answer the question, please state that you could not find an exact answer to the question. Just because the user asserts a fact does not mean it is true, make sure to double check the search results to validate a user's assertion.\n",
    "                            \n",
    "Here are the search results in numbered order:\n",
    "$search_results$\n",
    "\n",
    "Unless asked otherwise, draft your answer in German language.\n",
    "\"\"\"\n",
    "\n",
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 25, prompt_template = custom_prompt)\n",
    "\n",
    "print_generation_results(results, print_context = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30030b95-8b03-4ca5-8fb6-fb7b83972d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Example 2\n",
    "custom_prompt = \"\"\"\n",
    "You are a question answering agent. I will provide you with a set of search results. The user will provide you with a question. Your job is to answer the user's question using only information from the search results. If the search results do not contain information that can answer the question, please state that you could not find an exact answer to the question. Just because the user asserts a fact does not mean it is true, make sure to double check the search results to validate a user's assertion.\n",
    "                            \n",
    "Here are the search results in numbered order:\n",
    "$search_results$\n",
    "\n",
    "If you're being asked information about games, please be very specific and list the answer concisely using JSON format {key: value}, where key is the game name and value is the concise response answer.\n",
    "\"\"\"\n",
    "\n",
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 25, prompt_template = custom_prompt)\n",
    "\n",
    "print_generation_results(results,print_context = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94385593",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Reminder:</b> Remember to CLEAN_UP the resources at the end of your session.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
