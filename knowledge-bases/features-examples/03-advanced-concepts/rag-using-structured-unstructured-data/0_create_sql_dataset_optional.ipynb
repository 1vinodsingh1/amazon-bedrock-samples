{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create structured SQL dataset [Optional]\n",
    "\n",
    "This is a optional notebook to create dummy structured dataset and create a table in Amazon Athena for Text-2-SQL Retrieval.\n",
    "\n",
    "**Pre-requisite:**\n",
    "Download the provided dummy data and path to data in the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Define the path to files\n",
    "directory = '<path to data folder name with / at the end>'\n",
    "\n",
    "# define a project name:\n",
    "aws_account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "project_name = 'advanced-rag-text2sql-{}'\n",
    "\n",
    "# S3 bucket for Firehose destination\n",
    "bucket_name = project_name.format('s3-bucket')\n",
    "\n",
    "# Define the Glue role name\n",
    "glue_role_name = project_name.format('glue-role')\n",
    "\n",
    "# Glue database name\n",
    "glue_database_name = project_name.format('glue-database')\n",
    "\n",
    "# Glue crawler name\n",
    "glue_crawler_name = project_name.format('glue-crawler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AWS clients\n",
    "s3_client = boto3.client('s3')\n",
    "glue_client = boto3.client('glue')\n",
    "iam_client = boto3.client('iam')\n",
    "boto3_session = boto3.session.Session()\n",
    "region = boto3_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create S3 Bucket and upload data to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 bucket\n",
    "s3_client.create_bucket(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function uploads all files to their respective folders in an Amazon S3 bucket.\n",
    "def upload_to_s3(path, bucket_name, bucket_subfolder=None):\n",
    "    \"\"\"\n",
    "    Upload a file or directory to an AWS S3 bucket.\n",
    "\n",
    "    :param path: Path to the file or directory to be uploaded\n",
    "    :param bucket_name: Name of the S3 bucket\n",
    "    :param bucket_subfolder: Name of the subfolder within the S3 bucket (optional)\n",
    "    :return: True if the file(s) were uploaded successfully, False otherwise\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        # If the path is a file, create a folder for the file and upload it\n",
    "        folder_name = os.path.basename(path).split('.')[0]  # Get the file name without extension\"\n",
    "        object_name = f\"{folder_name}/{os.path.basename(path)}\" if bucket_subfolder is None else f\"{bucket_subfolder}/{folder_name}/{os.path.basename(path)}\"\n",
    "        try:\n",
    "            s3.upload_file(path, bucket_name, object_name)\n",
    "            print(f\"Successfully uploaded {path} to {bucket_name}/{object_name}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {path} to S3: {e}\")\n",
    "            return None\n",
    "    elif os.path.isdir(path):\n",
    "        # If the path is a directory, recursively upload all files within it and create a folder for each file\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(file_path, path)\n",
    "                folder_name = relative_path.split('.')[0]  # Get the folder name for the current file\n",
    "                object_name = f\"{folder_name}/{relative_path}\" if bucket_subfolder is None else f\"{bucket_subfolder}/{folder_name}/{relative_path}\"\n",
    "                try:\n",
    "                    s3.upload_file(file_path, bucket_name, object_name)\n",
    "                    print(f\"Successfully uploaded {file_path} to {bucket_name}/{object_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error uploading {file_path} to S3: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"{path} is not a file or directory.\")\n",
    "        return None\n",
    "\n",
    "# Upload the files:\n",
    "upload_to_s3(directory, bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Glue database and crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_role_assume_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"glue.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "glue_role_response = iam_client.create_role(\n",
    "    RoleName=glue_role_name,\n",
    "    AssumeRolePolicyDocument=json.dumps(glue_role_assume_policy_document)\n",
    ")\n",
    "\n",
    "# Attach managed policies to the Glue role\n",
    "iam_client.attach_role_policy(\n",
    "    RoleName=glue_role_name,\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonS3FullAccess'\n",
    ")\n",
    "\n",
    "iam_client.attach_role_policy(\n",
    "    RoleName=glue_role_name,\n",
    "    PolicyArn='arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole'\n",
    ")\n",
    "\n",
    "glue_role_arn = glue_role_response['Role']['Arn']\n",
    "\n",
    "# Create Glue database\n",
    "glue_response = glue_client.create_database(DatabaseInput={'Name': glue_database_name})\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Glue crawler\n",
    "glue_client.create_crawler(\n",
    "    Name=glue_crawler_name,\n",
    "    Role=glue_role_arn,\n",
    "    DatabaseName=glue_database_name,\n",
    "    Description='Crawl Firehose S3 data to create a table in Athena',\n",
    "    Targets={\n",
    "        'S3Targets': [\n",
    "            {\n",
    "                'Path': f's3://{bucket_name}/'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Lets trigger the Glue Crawler so that we can query the data using SQL and create a dashboard in Quicksight:\n",
    "try:\n",
    "    response = glue_client.start_crawler(\n",
    "        Name=glue_crawler_name\n",
    "    )\n",
    "    print(f\"Crawler {glue_crawler_name} started successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error starting crawler {glue_crawler_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the crawler to complete\n",
    "crawler_state = 'RUNNING'\n",
    "while crawler_state == 'RUNNING':\n",
    "    time.sleep(15)  # Wait for 15 seconds before checking the status again\n",
    "    crawler_response = glue_client.get_crawler(\n",
    "        Name=glue_crawler_name\n",
    "    )\n",
    "    crawler_state = crawler_response['Crawler']['State']\n",
    "\n",
    "# Print the final status of the crawler\n",
    "if crawler_state in ['SUCCEEDED', 'STOPPING']:\n",
    "    print(f\"Crawler {glue_crawler_name} completed successfully.\")\n",
    "else:\n",
    "    print(f\"Crawler {glue_crawler_name} failed with state: {crawler_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save the database name in local variables such that its available directly in the `MultiRetreiverQAChain` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store glue_database_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "\n",
    "Once the cralwer has run successfully, you should now see 4 tables created in Athena with the same names as your files. Now, you should be able to use the `MultiRetrievalQAChain` using this dummy dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
