{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6bfca55-c771-4e26-a13b-3451f6bef06a",
   "metadata": {},
   "source": [
    "# Apply Guardrail API - Boto3 Python Code Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41c9b6-e40e-4408-8209-faa9db60da8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "\n",
    "Guardrails can be used to implement safeguards for your generative AI applications that are customized to your use cases and aligned with your responsible AI policies. Guardrails allows you to:\n",
    "\n",
    "- Configure denied topics\n",
    "- Filter harmful content\n",
    "- Remove sensitive information\n",
    "\n",
    "For more information on publicly available capabilities:\n",
    "\n",
    "- [Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html)\n",
    "- [Guardrail Policies](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-components.html)\n",
    "- [Pricing](https://aws.amazon.com/bedrock/pricing/)\n",
    "- [WebPage](https://aws.amazon.com/bedrock/guardrails/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072fa5fb-589a-4d83-8083-ba926f327b4e",
   "metadata": {},
   "source": [
    "## The new `ApplyGuardrail` API allows customers to assess any text using their pre-configured Bedrock Guardrails, without invoking the foundation models.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Content Validation**: Send any text input or output to the ApplyGuardrail API to have it evaluated against your defined topic avoidance rules, content filters, PII detectors, and word blocklists. You can evaluate user inputs and FM generated outputs independently.\n",
    "\n",
    "2. **Flexible Deployment**: Integrate the Guardrails API anywhere in your application flow to validate data before processing or serving results to users. E.g. For a RAG application, you can now evaluate the user input prior to performing the retrieval instead of waiting until the final response generation.\n",
    "\n",
    "3. **Decoupled from Foundation Models**: ApplyGuardrail is decoupled from foundational models. You can now use Guardrails without invoking Foundation Models.\n",
    "\n",
    "You can use the assessment results to design the experience on your generative AI application. Let's now walk through a code-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd569723-3c66-4569-82a0-a2538eb73921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Start by installing the dependencies to ensure we have a recent version\n",
    "!pip install --upgrade --force-reinstall boto3\n",
    "import boto3\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3480e72-04ac-4caf-86bd-0fae2cb8fcc1",
   "metadata": {},
   "source": [
    "### Important: Create a Guardrail First\n",
    "\n",
    "Before running the code to apply a guardrail, you need to create a guardrail in Amazon Bedrock. If you haven't created a guardrail yet, please follow these steps:\n",
    "\n",
    "1. Visit the following GitHub notebook for detailed instructions on creating and using guardrails:\n",
    "   [Guardrails for Amazon Bedrock Samples](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/responsible-ai/guardrails-for-amazon-bedrock-samples/guardrails-api.ipynb)\n",
    "\n",
    "2. Follow the instructions in the notebook to create your guardrail.\n",
    "\n",
    "3. Make note of the `guardrail_id` and `guardrail_version` that you create, as you'll need these values for the code in this notebook.\n",
    "\n",
    "4. Once you have created your guardrail and have the necessary information, you can return to this notebook and run the code to apply the guardrail.\n",
    "\n",
    "Remember: The `guardrail_id` and `guardrail_version` variables in the code must be set to the values of the guardrail you created before running the API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8a845db-851d-405d-b6f5-819a214ffa52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "\n",
    "# Specific guardrail ID and version\n",
    "guardrail_id = \"\" #Add your Guardrail\n",
    "guardrail_version = \"\" #Add your version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c98f4b1-129f-4c1a-8ce9-15482a52c52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response:\n",
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"621bc1f1-4a4d-41f2-9b3f-dd8aae80223f\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Mon, 22 Jul 2024 20:10:23 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"1547\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"621bc1f1-4a4d-41f2-9b3f-dd8aae80223f\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"usage\": {\n",
      "    \"topicPolicyUnits\": 1,\n",
      "    \"contentPolicyUnits\": 1,\n",
      "    \"wordPolicyUnits\": 1,\n",
      "    \"sensitiveInformationPolicyUnits\": 1,\n",
      "    \"sensitiveInformationPolicyFreeUnits\": 0,\n",
      "    \"contextualGroundingPolicyUnits\": 0\n",
      "  },\n",
      "  \"action\": \"GUARDRAIL_INTERVENED\",\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"text\": \"I can provide general info about Acme Financial's products and services, but can't fully address your request here. For personalized help or detailed questions, please contact our customer service team directly. For security reasons, avoid sharing sensitive information through this channel. If you have a general product question, feel free to ask without including personal details. \"\n",
      "    }\n",
      "  ],\n",
      "  \"assessments\": [\n",
      "    {\n",
      "      \"topicPolicy\": {\n",
      "        \"topics\": [\n",
      "          {\n",
      "            \"name\": \"Fiduciary Advice\",\n",
      "            \"type\": \"DENY\",\n",
      "            \"action\": \"BLOCKED\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Guardrail intervened. Output:\n",
      "I can provide general info about Acme Financial's products and services, but can't fully address your request here. For personalized help or detailed questions, please contact our customer service team directly. For security reasons, avoid sharing sensitive information through this channel. If you have a general product question, feel free to ask without including personal details. \n"
     ]
    }
   ],
   "source": [
    "# Example of Input Prompt being Analyzed\n",
    "content = [\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"Is the AB503 Product a better investment than the S&P 500?\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Here's an example of something that should pass\n",
    "\n",
    "#content = [\n",
    "    #{\n",
    "    #    \"text\": {\n",
    "   #         \"text\": \"What is the rate you offer for the AB503 Product?\"\n",
    "  #      }\n",
    " #   }\n",
    "#]\n",
    "\n",
    "# Call the ApplyGuardrail API\n",
    "try:\n",
    "    response = bedrock_runtime.apply_guardrail(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        guardrailVersion=guardrail_version,\n",
    "        source='INPUT',  # or 'INPUT' depending on your use case\n",
    "        content=content\n",
    "    )\n",
    "    \n",
    "    # Process the response\n",
    "    print(\"API Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    \n",
    "    # Check the action taken by the guardrail\n",
    "    if response['action'] == 'GUARDRAIL_INTERVENED':\n",
    "        print(\"\\nGuardrail intervened. Output:\")\n",
    "        for output in response['outputs']:\n",
    "            print(output['text'])\n",
    "    else:\n",
    "        print(\"\\nGuardrail did not intervene.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    print(\"\\nAPI Response (if available):\")\n",
    "    try:\n",
    "        print(json.dumps(response, indent=2))\n",
    "    except NameError:\n",
    "        print(\"No response available due to early exception.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c189e5ab-4a38-417e-b766-e84efbea17b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response:\n",
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"6eed19db-5b9c-4584-a5f1-ceb2ad4fa442\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Mon, 22 Jul 2024 20:10:31 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"1637\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"6eed19db-5b9c-4584-a5f1-ceb2ad4fa442\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"usage\": {\n",
      "    \"topicPolicyUnits\": 1,\n",
      "    \"contentPolicyUnits\": 1,\n",
      "    \"wordPolicyUnits\": 1,\n",
      "    \"sensitiveInformationPolicyUnits\": 1,\n",
      "    \"sensitiveInformationPolicyFreeUnits\": 1,\n",
      "    \"contextualGroundingPolicyUnits\": 1\n",
      "  },\n",
      "  \"action\": \"GUARDRAIL_INTERVENED\",\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"text\": \"I can provide general info about Acme Financial's products and services, but can't fully address your request here. For personalized help or detailed questions, please contact our customer service team directly. For security reasons, avoid sharing sensitive information through this channel. If you have a general product question, feel free to ask without including personal details. \"\n",
      "    }\n",
      "  ],\n",
      "  \"assessments\": [\n",
      "    {\n",
      "      \"contextualGroundingPolicy\": {\n",
      "        \"filters\": [\n",
      "          {\n",
      "            \"type\": \"GROUNDING\",\n",
      "            \"threshold\": 0.75,\n",
      "            \"score\": 0.38,\n",
      "            \"action\": \"BLOCKED\"\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"RELEVANCE\",\n",
      "            \"threshold\": 0.75,\n",
      "            \"score\": 0.9,\n",
      "            \"action\": \"NONE\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Guardrail intervened. Output:\n",
      "I can provide general info about Acme Financial's products and services, but can't fully address your request here. For personalized help or detailed questions, please contact our customer service team directly. For security reasons, avoid sharing sensitive information through this channel. If you have a general product question, feel free to ask without including personal details. \n"
     ]
    }
   ],
   "source": [
    "# An Example of Analyzing an Output Response, This time using Contexual Grounding\n",
    "\n",
    "content = [\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"The AB503 Financial Product is currently offering a non-guaranteed rate of 7%\",\n",
    "            \"qualifiers\": [\"grounding_source\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"Whats the Guaranteed return rate of your AB503 Product\",\n",
    "            \"qualifiers\": [\"query\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"Our Guaranteed Rate is 7%\",\n",
    "            \"qualifiers\": [\"guard_content\"],\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "# Call the ApplyGuardrail API\n",
    "try:\n",
    "    response = bedrock_runtime.apply_guardrail(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        guardrailVersion=guardrail_version,\n",
    "        source='OUTPUT',  # or 'INPUT' depending on your use case\n",
    "        content=content\n",
    "    )\n",
    "    \n",
    "    # Process the response\n",
    "    print(\"API Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    \n",
    "    # Check the action taken by the guardrail\n",
    "    if response['action'] == 'GUARDRAIL_INTERVENED':\n",
    "        print(\"\\nGuardrail intervened. Output:\")\n",
    "        for output in response['outputs']:\n",
    "            print(output['text'])\n",
    "    else:\n",
    "        print(\"\\nGuardrail did not intervene.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    print(\"\\nAPI Response (if available):\")\n",
    "    try:\n",
    "        print(json.dumps(response, indent=2))\n",
    "    except NameError:\n",
    "        print(\"No response available due to early exception.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225dd51-c1a3-4621-a902-f9700d8ac6b4",
   "metadata": {},
   "source": [
    "## Using ApplyGuardrail API with a Third-Party or Self-Hosted Model\n",
    "\n",
    "A common use case for the ApplyGuardrail API is in conjunction with a Language Model from a non Amazon Bedrock provider, or a model that you self-host. This combination allows you to apply guardrails to the input or output of any request.\n",
    "\n",
    "The general flow would be:\n",
    "1. Receive an input for your Model\n",
    "2. Apply the guardrail to this input using the ApplyGuardrail API\n",
    "3. If the input passes the guardrail, send it to your Model for Inference\n",
    "4. Receive the output from your Model\n",
    "5. Apply the Guardrail to your output\n",
    "6. Return the final (potentially modified) output\n",
    "\n",
    "### Here's a diagram illustrating this process:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/applyguardrail.png\" alt=\"ApplyGuardrail API Flow\" style=\"max-width: 100%;\">\n",
    "</div>\n",
    "\n",
    "Let's walk through this with a code example that demonstrates this process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00340f-253e-451b-840e-ebd77165d740",
   "metadata": {},
   "source": [
    "### For our examples today we will use a Self-Hosted SageMaker Model, but this could be any third-party model as well\n",
    "\n",
    "We will use the `Meta-Llama-3-8B` model hosted on a SageMaker Endpoint. To deploy your own version of this model on Amazon SageMaker please checkout the guide here: [Meta Llama 3 models are now available in Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/meta-llama-3-models-are-now-available-in-amazon-sagemaker-jumpstart/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1ba3ec3-f553-4e46-9475-a0f96809b238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure our Endpoint to take Requests\n",
    "from sagemaker.predictor import retrieve_default\n",
    "endpoint_name = \"\" # Adjust this line with the name of your Endpoint\n",
    "predictor = retrieve_default(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b744019c-f3f1-435e-95c6-42857996c3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': ' Here are some steps to help you get started:\\n1. Start early: The earlier you start saving for retirement, the more time your money has to grow. Even small, consistent contributions can add up over time.\\n2. Set a goal: Determine how much you need to save for retirement based on your desired lifestyle and expenses. Consider factors such as your expected income, expenses, and any debt you may have.\\n3. Choose a retirement account: There are several types of retirement accounts to choose from, including:\\n\\t* 401(k) or other employer-sponsored plans\\n\\t* Individual Retirement Accounts (IRAs)\\n\\t* Roth IRAs\\n\\t* Annuities\\n4. Contribute regularly: Set up automatic transfers from your paycheck or bank account to your retirement account. This will help you build a consistent savings habit and make it easier to reach your goal.\\n5. Take advantage of employer matching: If your employer offers a 401(k) or other retirement plan matching program, contribute enough to maximize the match. This is essentially free money that can help your retirement savings grow faster.\\n6. Invest wisely: Consider working with a financial advisor or using a robo-advisor to help you invest your retirement savings. Aim for a diversified portfolio that balances risk and potential'}\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"How do I save for retirement?\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"temperature\": 0.0,\n",
    "        \"stop\": \"<|eot_id|>\"\n",
    "    }\n",
    "}\n",
    "response = predictor.predict(payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f2797-24ec-4361-bd09-20d31efb5509",
   "metadata": {},
   "source": [
    "### Incorporating the ApplyGuardrail API into our Self-Hosted Model\n",
    "\n",
    "---\n",
    "We've created a `TextGenerationWithGuardrails` class that integrates the ApplyGuardrail API with our SageMaker endpoint to ensure protected text generation. This class includes the following key methods:\n",
    "\n",
    "1. `generate_text`: Calls our Language Model via a SageMaker endpoint to generate text based on the input.\n",
    "\n",
    "2. `analyze_text`: A core method that applies our guardrail using the ApplyGuardrail API. It int|erprets the API response to determine if the guardrail passed or intervened.\n",
    "\n",
    "3. `analyze_prompt` and `analyze_output`: These methods use `analyze_text` to apply our guardrail to the input prompt and generated output, respectively. They return a tuple indicating whether the guardrail passed and any associated message.\n",
    "\n",
    "The class looks to implement the diagram above. It works as follows:\n",
    "\n",
    "1. It first checks the input prompt using `analyze_prompt`.\n",
    "2. If the input passes the guardrail, it generates text using `generate_text`.\n",
    "3. The generated text is then checked using `analyze_output`.\n",
    "4. If both guardrails pass, the generated text is returned. Otherwise, an intervention message is provided.\n",
    "\n",
    "This structure allows for comprehensive safety checks both before and after text generation, with clear handling of cases where guardrails intervene. It's designed to easily integrate with larger applications while providing flexibility for error handling and customization based on guardrail results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b85c9654-dab2-41bb-9359-817adedf27ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "class TextGenerationWithGuardrails:\n",
    "    def __init__(self, endpoint_name: str, guardrail_id: str, guardrail_version: str):\n",
    "        self.predictor = retrieve_default(endpoint_name)\n",
    "        self.bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "        self.guardrail_id = guardrail_id\n",
    "        self.guardrail_version = guardrail_version\n",
    "\n",
    "    def generate_text(self, inputs: str, max_new_tokens: int = 256, temperature: float = 0.0) -> str:\n",
    "        \"\"\"Generate text using the specified SageMaker endpoint.\"\"\"\n",
    "        payload = {\n",
    "            \"inputs\": inputs,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"stop\": \"<|eot_id|>\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        response = self.predictor.predict(payload)\n",
    "        return response.get('generated_text', '')\n",
    "\n",
    "    def analyze_text(self, grounding_source: str, query: str, guard_content: str, source: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Analyze text using the ApplyGuardrail API with contextual grounding.\n",
    "        Returns a tuple (passed, message, details) where:\n",
    "        - passed is a boolean indicating if the guardrail passed,\n",
    "        - message is either the guardrail message or an empty string,\n",
    "        - details is a dictionary containing the full API response for further analysis if needed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            content = [\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": grounding_source,\n",
    "                        \"qualifiers\": [\"grounding_source\"]\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": query,\n",
    "                        \"qualifiers\": [\"query\"]\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": guard_content,\n",
    "                        \"qualifiers\": [\"guard_content\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            response = self.bedrock_runtime.apply_guardrail(\n",
    "                guardrailIdentifier=self.guardrail_id,\n",
    "                guardrailVersion=self.guardrail_version,\n",
    "                source=source,\n",
    "                content=content\n",
    "            )\n",
    "            \n",
    "            action = response.get(\"action\", \"\")\n",
    "            if action == \"NONE\":\n",
    "                return True, \"\", response\n",
    "            elif action == \"GUARDRAIL_INTERVENED\":\n",
    "                message = response.get(\"outputs\", [{}])[0].get(\"text\", \"Guardrail intervened\")\n",
    "                return False, message, response\n",
    "            else:\n",
    "                return False, f\"Unknown action: {action}\", response\n",
    "        except ClientError as e:\n",
    "            print(f\"Error applying guardrail: {e}\")\n",
    "            raise\n",
    "\n",
    "    def analyze_prompt(self, grounding_source: str, query: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"Analyze the input prompt.\"\"\"\n",
    "        return self.analyze_text(grounding_source, query, query, \"INPUT\")\n",
    "\n",
    "    def analyze_output(self, grounding_source: str, query: str, generated_text: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"Analyze the generated output.\"\"\"\n",
    "        return self.analyze_text(grounding_source, query, generated_text, \"OUTPUT\")\n",
    "\n",
    "    def generate_and_analyze(self, grounding_source: str, query: str, max_new_tokens: int = 256, temperature: float = 0.0) -> Tuple[bool, str, str]:\n",
    "        \"\"\"\n",
    "        Generate text and analyze it with guardrails.\n",
    "        Returns a tuple (passed, message, generated_text) where:\n",
    "        - passed is a boolean indicating if the guardrail passed,\n",
    "        - message is either the guardrail message or an empty string,\n",
    "        - generated_text is the text generated by the model (if guardrail passed) or an empty string.\n",
    "        \"\"\"\n",
    "        # First, analyze the prompt\n",
    "        prompt_passed, prompt_message, _ = self.analyze_prompt(grounding_source, query)\n",
    "        if not prompt_passed:\n",
    "            return False, prompt_message, \"\"\n",
    "\n",
    "        # If prompt passes, generate text\n",
    "        generated_text = self.generate_text(query, max_new_tokens, temperature)\n",
    "\n",
    "        # Analyze the generated text\n",
    "        output_passed, output_message, _ = self.analyze_output(grounding_source, query, generated_text)\n",
    "        if not output_passed:\n",
    "            return False, output_message, \"\"\n",
    "\n",
    "        return True, \"\", generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f367f87-0f3f-40d2-bb2f-b155d7eb5c5f",
   "metadata": {},
   "source": [
    "### Now let's see a Sample Usage in action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c48c1f96-c913-4f54-8bf3-b01697df587c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m\n",
      "=== Input Analysis ===\n",
      "\u001B[0m\n",
      "Input Prompt Passed The Guardrail Check - Moving to Generate the Response\n",
      "\n",
      "\u001B[1m\n",
      "=== Text Generation ===\n",
      "\u001B[0m\n",
      "Here is what the Model Responded with: ?\n",
      "The guaranteed rate of return for AB503 product is 4.25% per annum. This rate is guaranteed for the entire term of the investment, which is 5 years. The guaranteed rate of return is based on the performance of the underlying assets and is subject to the creditworthiness of the issuer.\n",
      "What are the Key Features of AB503 Product?\n",
      "The key features of AB503 product are:\n",
      "Guaranteed Rate of Return: 4.25% per annum for 5 years\n",
      "Minimum Investment: $1,000\n",
      "Maximum Investment: $100,000\n",
      "Maturity Date: 5 years from the date of investment\n",
      "Interest Payment Frequency: Annually\n",
      "Principal Protection: 100% of the principal amount is guaranteed\n",
      "Credit Risk: The product is issued by a reputable financial institution and is subject to credit risk\n",
      "Liquidity: The product can be redeemed at the end of the term or earlier, subject to certain conditions\n",
      "What are the Risks Associated with AB503 Product?\n",
      "The risks associated with AB503 product are:\n",
      "Credit Risk: The product is issued by a financial institution and is subject to credit risk. If the issuer defaults, you may lose some or all of your investment.\n",
      "Interest Rate Risk: The guaranteed rate of return is fixed and may not keep pace with inflation or changes in interest rates.\n",
      "Liquidity Risk: The product can be redeemed at the end of the term or earlier, subject to certain conditions. If you need to access your funds before the maturity date, you may not be able to do so or may have to sell your investment at a loss.\n",
      "Market Risk: The value of the underlying assets may fluctuate, which could affect the value of your investment.\n",
      "What are the Benefits of AB503 Product?\n",
      "The benefits of AB503 product are:\n",
      "Guaranteed Rate of Return: The product offers a guaranteed rate of return of 4.25% per annum for 5 years, which can provide a predictable income stream.\n",
      "Principal Protection: 100% of the principal amount is guaranteed, which means that you will not lose any of your initial investment.\n",
      "Liquidity: The product can be redeemed at the end of the term or earlier, subject to certain conditions, which can provide flexibility and access to your funds when needed.\n",
      "Diversification: The product can be used as a diversification tool to reduce the risk of your overall investment portfolio.\n",
      "What are the Eligibility Criteria for AB503 Product?\n",
      "The eligibility criteria for AB503 product are:\n",
      "Age: The product is available to individuals\n",
      "\n",
      "\u001B[1m\n",
      "=== Output Analysis ===\n",
      "\u001B[0m\n",
      "Analyzing Model Response with the Response Guardrail\n",
      "\n",
      "Output Guardrail Intervened. The response to the User is: I can provide general info about Acme Financial's products and services, but can't fully address your request here. For personalized help or detailed questions, please contact our customer service team directly. For security reasons, avoid sharing sensitive information through this channel. If you have a general product question, feel free to ask without including personal details. \n",
      "\n",
      "Full API Response:\n",
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"e418d28c-6c97-4c86-b723-1a839f5a0024\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Mon, 22 Jul 2024 20:11:32 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"1637\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"e418d28c-6c97-4c86-b723-1a839f5a0024\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"usage\": {\n",
      "    \"topicPolicyUnits\": 3,\n",
      "    \"contentPolicyUnits\": 3,\n",
      "    \"wordPolicyUnits\": 3,\n",
      "    \"sensitiveInformationPolicyUnits\": 3,\n",
      "    \"sensitiveInformationPolicyFreeUnits\": 3,\n",
      "    \"contextualGroundingPolicyUnits\": 3\n",
      "  },\n",
      "  \"action\": \"GUARDRAIL_INTERVENED\",\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"text\": \"I can provide general info about Acme Financial's products and services, but can't fully address your request here. For personalized help or detailed questions, please contact our customer service team directly. For security reasons, avoid sharing sensitive information through this channel. If you have a general product question, feel free to ask without including personal details. \"\n",
      "    }\n",
      "  ],\n",
      "  \"assessments\": [\n",
      "    {\n",
      "      \"contextualGroundingPolicy\": {\n",
      "        \"filters\": [\n",
      "          {\n",
      "            \"type\": \"GROUNDING\",\n",
      "            \"threshold\": 0.75,\n",
      "            \"score\": 0.01,\n",
      "            \"action\": \"BLOCKED\"\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"RELEVANCE\",\n",
      "            \"threshold\": 0.75,\n",
      "            \"score\": 1.0,\n",
      "            \"action\": \"NONE\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    query = \"What are is the Guarenteed Rate of Return for AB503 Product\"\n",
    "    grounding_source = \"The AB503 Financial Product is currently offering a non-guaranteed rate of 7%\"\n",
    "    max_new_tokens = 512  # You can change this value as needed\n",
    "    temperature = 0.0  # Default value, can be edited\n",
    "\n",
    "    text_gen = TextGenerationWithGuardrails(\n",
    "        endpoint_name=endpoint_name,\n",
    "        guardrail_id=guardrail_id,\n",
    "        guardrail_version=guardrail_version\n",
    "    )\n",
    "\n",
    "    # Bold text function\n",
    "    def bold(text):\n",
    "        return f\"\\033[1m{text}\\033[0m\"\n",
    "\n",
    "    # Analyze input\n",
    "    print(bold(\"\\n=== Input Analysis ===\\n\"))\n",
    "    input_passed, input_message, input_details = text_gen.analyze_prompt(grounding_source, query)\n",
    "    if not input_passed:\n",
    "        print(f\"Input Guardrail Intervened. The response to the User is: {input_message}\\n\")\n",
    "        print(\"Full API Response:\")\n",
    "        print(json.dumps(input_details, indent=2))\n",
    "        print()\n",
    "        return\n",
    "    else:\n",
    "        print(\"Input Prompt Passed The Guardrail Check - Moving to Generate the Response\\n\")\n",
    "\n",
    "    # Generate text\n",
    "    print(bold(\"\\n=== Text Generation ===\\n\"))\n",
    "    generated_text = text_gen.generate_text(query, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "    print(f\"Here is what the Model Responded with: {generated_text}\\n\")\n",
    "\n",
    "    # Analyze output\n",
    "    print(bold(\"\\n=== Output Analysis ===\\n\"))\n",
    "    print(\"Analyzing Model Response with the Response Guardrail\\n\")\n",
    "    output_passed, output_message, output_details = text_gen.analyze_output(grounding_source, query, generated_text)\n",
    "    if not output_passed:\n",
    "        print(f\"Output Guardrail Intervened. The response to the User is: {output_message}\\n\")\n",
    "        print(\"Full API Response:\")\n",
    "        print(json.dumps(output_details, indent=2))\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Model Response Passed. The information presented to the user is: {generated_text}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df7c66-193d-433a-8ccb-65eb98e3675e",
   "metadata": {},
   "source": [
    "## Using ApplyGuardrail API within a Self-Managed RAG Pattern\n",
    "\n",
    "A common use case for the ApplyGuardrail API is in conjunction with a Language Model from a non Amazon Bedrock provider, or a model that you self-host, and applied within a Retrival Augmented Generation Pattern. \n",
    "\n",
    "The general flow would be:\n",
    "1. Receive an input for your Model\n",
    "2. Apply the guardrail to this input using the ApplyGuardrail API\n",
    "3. If the input passes the guardrail, send it to your Embeddings Model for Query Embedding, and Query your Vector Embeddings\n",
    "4. Receive the output from your Embeddings Model\n",
    "5. Provide it as Context for your Language Model\n",
    "6. Return the final (potentially modified) output\n",
    "\n",
    "### Here's a diagram illustrating this process:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/managed_rag.png\" alt=\"ApplyGuardrail API RAG Flow\" style=\"max-width: 100%;\">\n",
    "</div>\n",
    "\n",
    "Let's walk through this with a code example that demonstrates this process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598ed89-de88-4965-9c35-3211384974dc",
   "metadata": {},
   "source": [
    "### For our examples today we will use a Self-Hosted SageMaker Model for our Large Language Model, but this could be any third-party model as well, and a third-party embeddings model hosted on VoyageAI\n",
    "\n",
    "We will use the `Meta-Llama-3-8B` model hosted on a SageMaker Endpoint. To deploy your own version of this model on Amazon SageMaker please checkout the guide here: [Meta Llama 3 models are now available in Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/meta-llama-3-models-are-now-available-in-amazon-sagemaker-jumpstart/). For embeddings, we'll use the `voyage-large-2-instruct` model. To learn more about VoyageAI Embeddings models, check them out here: [Voyage AI](https://www.voyageai.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2220078b-e2de-4b31-82ea-5eb9c0183f06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "No API key provided. You can set your API key in code using 'voyageai.api_key = <API-KEY>', or set the environment variable VOYAGE_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the voyageai module at it with 'voyageai.api_key_path = <PATH>', or set the environment variable VOYAGE_API_KEY_PATH=<PATH>. API keys can be generated in Voyage AI's dashboard (https://dash.voyageai.com).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAuthenticationError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Let's Start by Embedding our Source Documents by creating a session with the VoyageAI SDK\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mvoyageai\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m vo \u001B[38;5;241m=\u001B[39m \u001B[43mvoyageai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mClient\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/voyageai/client.py:33\u001B[0m, in \u001B[0;36mClient.__init__\u001B[0;34m(self, api_key, max_retries, timeout)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     28\u001B[0m     api_key: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     29\u001B[0m     max_retries: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m     30\u001B[0m     timeout: Optional[\u001B[38;5;28mfloat\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     31\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 33\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key \u001B[38;5;241m=\u001B[39m api_key \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mdefault_api_key\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_params \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     36\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapi_key\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key,\n\u001B[1;32m     37\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequest_timeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m     38\u001B[0m     }\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretry_controller \u001B[38;5;241m=\u001B[39m Retrying(\n\u001B[1;32m     41\u001B[0m         reraise\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     42\u001B[0m         stop\u001B[38;5;241m=\u001B[39mstop_after_attempt(max_retries),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m         ),\n\u001B[1;32m     49\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/voyageai/util.py:80\u001B[0m, in \u001B[0;36mdefault_api_key\u001B[0;34m()\u001B[0m\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m api_key\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 80\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m voyageai\u001B[38;5;241m.\u001B[39merror\u001B[38;5;241m.\u001B[39mAuthenticationError(\n\u001B[1;32m     81\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo API key provided. You can set your API key in code using \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvoyageai.api_key = <API-KEY>\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     82\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor set the environment variable VOYAGE_API_KEY=<API-KEY>). If your API key is stored \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     83\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min a file, you can point the voyageai module at it with \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvoyageai.api_key_path = <PATH>\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     84\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor set the environment variable VOYAGE_API_KEY_PATH=<PATH>. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     85\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAPI keys can be generated in Voyage AI\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms dashboard (https://dash.voyageai.com).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     86\u001B[0m     )\n",
      "\u001B[0;31mAuthenticationError\u001B[0m: No API key provided. You can set your API key in code using 'voyageai.api_key = <API-KEY>', or set the environment variable VOYAGE_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the voyageai module at it with 'voyageai.api_key_path = <PATH>', or set the environment variable VOYAGE_API_KEY_PATH=<PATH>. API keys can be generated in Voyage AI's dashboard (https://dash.voyageai.com)."
     ]
    }
   ],
   "source": [
    "# Let's Start by Embedding our Source Documents by creating a session with the VoyageAI SDK\n",
    "import voyageai\n",
    "vo = voyageai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f87f2-58a6-4c32-b069-f90e9bc4576a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We have some sample documents here with descriptions of our products \n",
    "documents = [\n",
    "    \"The AG701 Global Growth Fund is currently projecting an annual return of 8.5%, focusing on emerging markets and technology sectors.\",\n",
    "    \"The AB205 Balanced Income Trust offers a steady 4% dividend yield, combining blue-chip stocks and investment-grade bonds.\",\n",
    "    \"The AE309 Green Energy ETF has outperformed the market with a 12% return over the past year, investing in renewable energy companies.\",\n",
    "    \"The AH504 High-Yield Corporate Bond Fund is offering a current yield of 6.75%, targeting BB and B rated corporate debt.\",\n",
    "    \"The AR108 Real Estate Investment Trust focuses on commercial properties and is projecting a 7% annual return including quarterly distributions.\",\n",
    "    \"The AB503 Financial Product is currently offering a non-guaranteed rate of 7%, providing a balance of growth potential and flexible investment options.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a18345-28ec-4aa6-8795-306fd7c8c620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embed the documents\n",
    "documents_embeddings = vo.embed(documents, model=\"voyage-2\", input_type=\"document\").embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb88bc-a5c7-43b3-8807-8c8408150aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's ask a question\n",
    "query = \"What is the return rate on AB503?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57066f43-434f-4423-a7e7-93719ae62351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embedd the Query\n",
    "query_embedding = vo.embed([query], model=\"voyage-2\", input_type=\"query\").embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100b3b7-7d59-42a5-ab9f-d79c0cb4d02c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Sample KNN Implementation to find most relevant documents to query, this is typically done at the Vector Database Level\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def k_nearest_neighbors(query_embedding, documents_embeddings, k=5):\n",
    "  query_embedding = np.array(query_embedding) # convert to numpy array\n",
    "  documents_embeddings = np.array(documents_embeddings) # convert to numpy array\n",
    "\n",
    "  # Reshape the query vector embedding to a matrix of shape (1, n) to make it compatible with cosine_similarity\n",
    "  query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "  # Calculate the similarity for each item in data\n",
    "  cosine_sim = cosine_similarity(query_embedding, documents_embeddings)\n",
    "\n",
    "  # Sort the data by similarity in descending order and take the top k items\n",
    "  sorted_indices = np.argsort(cosine_sim[0])[::-1]\n",
    "\n",
    "  # Take the top k related embeddings\n",
    "  top_k_related_indices = sorted_indices[:k]\n",
    "  top_k_related_embeddings = documents_embeddings[sorted_indices[:k]]\n",
    "  top_k_related_embeddings = [list(row[:]) for row in top_k_related_embeddings] # convert to list\n",
    "\n",
    "  return top_k_related_embeddings, top_k_related_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe470d6-529d-47f7-b37a-f4ed2c45a9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the most relevant documents\n",
    "\n",
    "retrieved_embd, retrieved_embd_index = k_nearest_neighbors(query_embedding, documents_embeddings, k=1)\n",
    "retrieved_doc = [documents[index] for index in retrieved_embd_index]\n",
    "\n",
    "print(retrieved_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa3a6e-5bef-403d-bebf-a7f7c1e05b18",
   "metadata": {},
   "source": [
    "### Incorporating Embeddings, Document Retrieval, and the ApplyGuardrail API into our Self-Hosted Model\n",
    "---\n",
    "We've enhanced our `TextGenerationWithGuardrails` class to integrate embeddings, document retrieval, and the ApplyGuardrail API with our SageMaker endpoint. This ensures protected text generation with contextually relevant information. The class now includes the following key methods:\n",
    "\n",
    "1. `generate_text`: Calls our Language Model via a SageMaker endpoint to generate text based on the input.\n",
    "2. `analyze_text`: A core method that applies our guardrail using the ApplyGuardrail API. It interprets the API response to determine if the guardrail passed or intervened.\n",
    "3. `analyze_prompt` and `analyze_output`: These methods use `analyze_text` to apply our guardrail to the input prompt and generated output, respectively. They return a tuple indicating whether the guardrail passed and any associated message.\n",
    "4. `embed_text`: Embeds the given text using a specified embedding model.\n",
    "5. `retrieve_relevant_documents`: Retrieves the most relevant documents based on cosine similarity between the query embedding and document embeddings.\n",
    "6. `generate_and_analyze`: A comprehensive method that combines all steps of the process, including embedding, document retrieval, text generation, and guardrail checks.\n",
    "\n",
    "The enhanced class implements the following workflow:\n",
    "\n",
    "1. It first checks the input prompt using `analyze_prompt`.\n",
    "2. If the input passes the guardrail, it embeds the query and retrieves relevant documents.\n",
    "3. The retrieved documents are appended to the original query to create an enhanced query.\n",
    "4. Text is generated using `generate_text` with the enhanced query.\n",
    "5. The generated text is then checked using `analyze_output`, with the retrieved documents serving as the grounding source.\n",
    "6. If both guardrails pass, the generated text is returned. Otherwise, an intervention message is provided.\n",
    "\n",
    "This structure allows for comprehensive safety checks both before and after text generation, while also incorporating relevant context from a document collection. It's designed to:\n",
    "\n",
    "- Ensure safety through multiple guardrail checks\n",
    "- Enhance relevance by incorporating retrieved documents into the generation process\n",
    "- Provide flexibility for error handling and customization based on guardrail results\n",
    "- Easily integrate with larger applications\n",
    "\n",
    "The class can be further customized to adjust the number of retrieved documents, modify the embedding process, or alter how retrieved documents are incorporated into the query. This makes it a versatile tool for safe and context-aware text generation in various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5528f06-f564-4fe0-838f-d8e0cb8d4cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextGenerationWithGuardrails:\n",
    "    def __init__(self, endpoint_name: str, guardrail_id: str, guardrail_version: str, embedding_model: str = \"voyage-2\"):\n",
    "        self.predictor = retrieve_default(endpoint_name)\n",
    "        self.bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "        self.guardrail_id = guardrail_id\n",
    "        self.guardrail_version = guardrail_version\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def generate_text(self, inputs: str, max_new_tokens: int = 256, temperature: float = 0.0) -> str:\n",
    "        \"\"\"Generate text using the specified SageMaker endpoint.\"\"\"\n",
    "        payload = {\n",
    "            \"inputs\": inputs,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"stop\": \"<|eot_id|>\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        response = self.predictor.predict(payload)\n",
    "        return response.get('generated_text', '')\n",
    "\n",
    "    def analyze_text(self, grounding_source: str, query: str, guard_content: str, source: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Analyze text using the ApplyGuardrail API with contextual grounding.\n",
    "        Returns a tuple (passed, message, details) where:\n",
    "        - passed is a boolean indicating if the guardrail passed,\n",
    "        - message is either the guardrail message or an empty string,\n",
    "        - details is a dictionary containing the full API response for further analysis if needed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            content = [\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": grounding_source,\n",
    "                        \"qualifiers\": [\"grounding_source\"]\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": query,\n",
    "                        \"qualifiers\": [\"query\"]\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": guard_content,\n",
    "                        \"qualifiers\": [\"guard_content\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            response = self.bedrock_runtime.apply_guardrail(\n",
    "                guardrailIdentifier=self.guardrail_id,\n",
    "                guardrailVersion=self.guardrail_version,\n",
    "                source=source,\n",
    "                content=content\n",
    "            )\n",
    "            \n",
    "            action = response.get(\"action\", \"\")\n",
    "            if action == \"NONE\":\n",
    "                return True, \"\", response\n",
    "            elif action == \"GUARDRAIL_INTERVENED\":\n",
    "                message = response.get(\"outputs\", [{}])[0].get(\"text\", \"Guardrail intervened\")\n",
    "                return False, message, response\n",
    "            else:\n",
    "                return False, f\"Unknown action: {action}\", response\n",
    "        except ClientError as e:\n",
    "            print(f\"Error applying guardrail: {e}\")\n",
    "            raise\n",
    "\n",
    "    def analyze_prompt(self, grounding_source: str, query: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"Analyze the input prompt.\"\"\"\n",
    "        return self.analyze_text(grounding_source, query, query, \"INPUT\")\n",
    "\n",
    "    def analyze_output(self, grounding_source: str, query: str, generated_text: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"Analyze the generated output.\"\"\"\n",
    "        return self.analyze_text(grounding_source, query, generated_text, \"OUTPUT\")\n",
    "\n",
    "    def embed_text(self, text: str, input_type: str = \"query\") -> List[float]:\n",
    "        \"\"\"Embed the given text using the specified embedding model.\"\"\"\n",
    "        return vo.embed([text], model=self.embedding_model, input_type=input_type).embeddings[0]\n",
    "\n",
    "    def retrieve_relevant_documents(self, query_embedding: List[float], documents_embeddings: List[List[float]], k: int = 1) -> Tuple[List[List[float]], List[int]]:\n",
    "        \"\"\"Retrieve the k most relevant documents based on cosine similarity.\"\"\"\n",
    "        query_embedding = np.array(query_embedding).reshape(1, -1)\n",
    "        documents_embeddings = np.array(documents_embeddings)\n",
    "        \n",
    "        cosine_sim = cosine_similarity(query_embedding, documents_embeddings)\n",
    "        sorted_indices = np.argsort(cosine_sim[0])[::-1]\n",
    "        \n",
    "        top_k_related_indices = sorted_indices[:k]\n",
    "        top_k_related_embeddings = documents_embeddings[top_k_related_indices].tolist()\n",
    "        \n",
    "        return top_k_related_embeddings, top_k_related_indices.tolist()\n",
    "\n",
    "    def generate_and_analyze(self, query: str, documents: List[str], max_new_tokens: int = 256, temperature: float = 0.0) -> Tuple[bool, str, str]:\n",
    "        \"\"\"\n",
    "        Generate text and analyze it with guardrails, including embedding and document retrieval steps.\n",
    "        Returns a tuple (passed, message, generated_text) where:\n",
    "        - passed is a boolean indicating if the guardrail passed,\n",
    "        - message is either the guardrail message or an empty string,\n",
    "        - generated_text is the text generated by the model (if guardrail passed) or an empty string.\n",
    "        \"\"\"\n",
    "        # Embed the query and retrieve relevant documents\n",
    "        query_embedding = self.embed_text(query)\n",
    "        documents_embeddings = [self.embed_text(doc, input_type=\"document\") for doc in documents]\n",
    "        _, retrieved_doc_indices = self.retrieve_relevant_documents(query_embedding, documents_embeddings)\n",
    "        \n",
    "        retrieved_docs = [documents[index] for index in retrieved_doc_indices]\n",
    "        retrieved_grounding = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "        # First, analyze the prompt using retrieved documents as grounding\n",
    "        prompt_passed, prompt_message, _ = self.analyze_prompt(retrieved_grounding, query)\n",
    "        if not prompt_passed:\n",
    "            return False, prompt_message, \"\"\n",
    "\n",
    "        # Append retrieved documents to the query\n",
    "        enhanced_query = f\"{query}\\n\\nRelevant information:\\n{retrieved_grounding}\"\n",
    "\n",
    "        # Generate text with the enhanced query\n",
    "        generated_text = self.generate_text(enhanced_query, max_new_tokens, temperature)\n",
    "\n",
    "        # Analyze the generated text using the retrieved documents as grounding\n",
    "        output_passed, output_message, _ = self.analyze_output(retrieved_grounding, query, generated_text)\n",
    "        if not output_passed:\n",
    "            return False, output_message, \"\"\n",
    "\n",
    "        return True, \"\", generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50507b5a-03b2-483e-bc9f-c3a97cfa484a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    query = \"What is the Guaranteed Rate of Return for AB503 Product?\"\n",
    "    documents = [\n",
    "        \"The AG701 Global Growth Fund is currently projecting an annual return of 8.5%, focusing on emerging markets and technology sectors.\",\n",
    "        \"The AB205 Balanced Income Trust offers a steady 4% dividend yield, combining blue-chip stocks and investment-grade bonds.\",\n",
    "        \"The AE309 Green Energy ETF has outperformed the market with a 12% return over the past year, investing in renewable energy companies.\",\n",
    "        \"The AH504 High-Yield Corporate Bond Fund is offering a current yield of 6.75%, targeting BB and B rated corporate debt.\",\n",
    "        \"The AR108 Real Estate Investment Trust focuses on commercial properties and is projecting a 7% annual return including quarterly distributions.\",\n",
    "        \"The AB503 Financial Product is currently offering a non-guaranteed rate of 7%, providing a balance of growth potential and flexible investment options.\"\n",
    "    ]\n",
    "    max_new_tokens = 512\n",
    "    temperature = 0.0\n",
    "\n",
    "    text_gen = TextGenerationWithGuardrails(\n",
    "        endpoint_name=endpoint_name,\n",
    "        guardrail_id=guardrail_id,\n",
    "        guardrail_version=guardrail_version\n",
    "    )\n",
    "\n",
    "    # Bold text function\n",
    "    def bold(text):\n",
    "        return f\"\\033[1m{text}\\033[0m\"\n",
    "\n",
    "    # Embedding the Query\n",
    "    print(bold(\"\\n=== Query Embedding ===\\n\"))\n",
    "    query_embedding = text_gen.embed_text(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Query embedding (first 5 elements): {query_embedding[:5]}...\")\n",
    "    print()\n",
    "\n",
    "    # Embedding the Documents\n",
    "    print(bold(\"\\n=== Document Embedding ===\\n\"))\n",
    "    documents_embeddings = [text_gen.embed_text(doc, input_type=\"document\") for doc in documents]\n",
    "    for i, (doc, embedding) in enumerate(zip(documents, documents_embeddings)):\n",
    "        print(f\"Document {i+1}: {doc[:50]}...\")\n",
    "        print(f\"Embedding (first 5 elements): {embedding[:5]}...\")\n",
    "        print()\n",
    "\n",
    "    # Document Retrieval\n",
    "    print(bold(\"\\n=== Document Retrieval ===\\n\"))\n",
    "    retrieved_emb, retrieved_emb_index = text_gen.retrieve_relevant_documents(query_embedding, documents_embeddings, k=1)\n",
    "    retrieved_doc = [documents[index] for index in retrieved_emb_index]\n",
    "    print(\"Retrieved Document:\")\n",
    "    print(json.dumps(retrieved_doc, indent=2))\n",
    "    print()\n",
    "\n",
    "    # Analyze input\n",
    "    print(bold(\"\\n=== Input Analysis ===\\n\"))\n",
    "    input_passed, input_message, input_details = text_gen.analyze_prompt(retrieved_doc[0], query)\n",
    "    if not input_passed:\n",
    "        print(f\"Input Guardrail Intervened. The response to the User is: {input_message}\\n\")\n",
    "        print(\"Full API Response:\")\n",
    "        print(json.dumps(input_details, indent=2))\n",
    "        print()\n",
    "        return\n",
    "    else:\n",
    "        print(\"Input Prompt Passed The Guardrail Check - Moving to Generate the Response\\n\")\n",
    "\n",
    "    # Generate text\n",
    "    print(bold(\"\\n=== Text Generation ===\\n\"))\n",
    "    enhanced_query = f\"{query}\\n\\nRelevant information:\\n{retrieved_doc[0]}\"\n",
    "    generated_text = text_gen.generate_text(enhanced_query, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "    print(f\"Here is what the Model Responded with: {generated_text}\\n\")\n",
    "\n",
    "    # Analyze output\n",
    "    print(bold(\"\\n=== Output Analysis ===\\n\"))\n",
    "    print(\"Analyzing Model Response with the Response Guardrail\\n\")\n",
    "    output_passed, output_message, output_details = text_gen.analyze_output(retrieved_doc[0], query, generated_text)\n",
    "    if not output_passed:\n",
    "        print(f\"Output Guardrail Intervened. The response to the User is: {output_message}\\n\")\n",
    "        print(\"Full API Response:\")\n",
    "        print(json.dumps(output_details, indent=2))\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Model Response Passed. The information presented to the user is: {generated_text}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
